{"componentChunkName":"component---src-templates-blog-post-tsx","path":"/2020-10-04-paper-review-pointnet/","result":{"data":{"site":{"siteMetadata":{"title":"Azmarie Wang","author":"Azmarie Wang"}},"markdownRemark":{"id":"d5fbc954-c6fc-51ab-8a61-dd205120ce8c","excerpt":"üìñ Link to the Paper: PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation Main Contribution The research problem in this paper is 3D‚Ä¶","html":"<p>üìñ Link to the Paper: <a href=\"https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a></p>\n<h2 id=\"main-contribution\" style=\"position:relative;\"><a href=\"#main-contribution\" aria-label=\"main contribution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Main Contribution</h2>\n<p>The research problem in this paper is 3D Classification and Segmentation. Among different 3D representations, the authors choose to use point cloud for its proximity to raw sensor data and its canonical form. Previously, the point cloud was often converted to other representation before being processed in deep neural networks, which causes artifacts. To solve this problem, the authors designed an effective feature learning directly on the point clouds. The main contributions include the proposal of PointNet, which performs end-to-end learning in 3D and achieves good results in various 3D tasks.</p>\n<h2 id=\"method\" style=\"position:relative;\"><a href=\"#method\" aria-label=\"method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Method</h2>\n<p>The proposed method is motivated to overcome the challenges of using point clouds as input. </p>\n<p>Firstly, point clouds are orderless, thus the model consuming N points needs to be invariant to N! permutations. Secondly, a rigid transformation should not alter the output, thus the model needs to preserve invariance under transformation. Thirdly, there are interactions among local points among the set of point clouds, which the model needs to capture. To tackle the first challenge, the authors observe that a permutation invariant neural network corresponds to a symmetric function, because in a symmetric function, for any ordering of the arguments, the function value stays the same. </p>\n<p>In this paper, they construct a family of symmetric functions by neural networks by multi-layer perceptron (MLP) and max pooling. Regarding the second challenge, the authors aligned input point clouds a canonical space, which is achieved by applying an affine transformation to input point coordinates, predicted by a mini PointNet T-net. This can be similarly applied to embedding space alignment in feature space with regularization to avoid bad local minimum. So far the network for 3D classification can be constructed, but the last challenge is important for the 3D segmentation network where class scores for each point need to be predicted. The method used here is to concatenate local embeddings and global feature vectors so that per point feature is aware of both local and global information for segmentation. </p>\n<h2 id=\"whats-good-and-whats-not-so-good\" style=\"position:relative;\"><a href=\"#whats-good-and-whats-not-so-good\" aria-label=\"whats good and whats not so good permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What‚Äôs good and what‚Äôs not so good?</h2>\n<p>One limitation of the paper is, in tackling the third challenge, they use the <b>concatenation of local and global embedding.</b> However, I think it seems extreme to put together embedding for one local point with the embedding of all points (global). Compared to 3D CNN where hierarchical feature learning is used, this method doesn‚Äôt consider multiple-level local context, and may not generalize well to a complex scene with multiple objects. </p>\n<p>Besides, in tackling the challenge of transformation invariance, they use affirm transformation. <b>Will that affect the coordinates in a way that‚Äôs spatially confusing to the network?</b> For example, would turning a chair upside down with a rotation transformation be a valid transformation? I think the author could have elaborated on T-net a bit more and what are the valid transformation matrices used to tackle this challenge.</p>\n<p>This paper exceeded the prior work with its direct usage of the point cloud in deep learning networks and great results. PointNet is also robust to data corruption including missing data points or outliers thanks to the ‚Äúcritical points‚Äù which can sparsely summarize the shape. </p>\n<h2 id=\"future-work\" style=\"position:relative;\"><a href=\"#future-work\" aria-label=\"future work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Future Work</h2>\n<p>Overall I think this is a great paper as the beginning for 3D deep learning with point clouds. One extension could be to exploit the benefit of hierarchical feature learning with PointNet or using a structure such as a skip connection to resample multi-layer local context. In hindsight, PointNet++ has proposed hierarchical feature learning precisely to extract information from the local neighbourhood.</p>","timeToRead":3,"fields":{"slug":"/2020-10-04-paper-review-pointnet/"},"frontmatter":{"title":"Paper Review: PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation","shortDate":"October 04, 2020","longDate":"October 04, 2020, 4:15:24 am","description":"Reviewing PointNet for 3D Classification and Segmentation","categories":["Computer Vision","Paper Review"],"cover":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAgADBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHu5IhlR//EABoQAAICAwAAAAAAAAAAAAAAAAABAgMiMUL/2gAIAQEAAQUCHjGNqOVo/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFxABAAMAAAAAAAAAAAAAAAAAAQARIP/aAAgBAQAGPwKWGf/EABwQAAICAgMAAAAAAAAAAAAAAAABESExURBBsf/aAAgBAQABPyGJTGtOdWNLutpoWZ68f//aAAwDAQACAAMAAAAQOM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAWEQEBAQAAAAAAAAAAAAAAAAABAEH/2gAIAQIBAT8QkHW//8QAHRABAAIBBQEAAAAAAAAAAAAAAQAhEUFRgZGxof/aAAgBAQABPxCibzzFqpuCQb4iEbLFp0z7Z4PI6T//2Q==","aspectRatio":2.3376623376623376,"src":"/static/0181bb8463c71729bb152133da4b5b9a/7f945/teaser.jpg","srcSet":"/static/0181bb8463c71729bb152133da4b5b9a/e2788/teaser.jpg 360w,\n/static/0181bb8463c71729bb152133da4b5b9a/7d509/teaser.jpg 720w,\n/static/0181bb8463c71729bb152133da4b5b9a/7f945/teaser.jpg 1440w,\n/static/0181bb8463c71729bb152133da4b5b9a/dc07a/teaser.jpg 1636w","sizes":"(max-width: 1440px) 100vw, 1440px"}}},"coverAuthor":null,"coverOriginalUrl":null,"keywords":["3D Classification","3D Segmentation","Computer Vision"],"published":true}}},"pageContext":{"slug":"/2020-10-04-paper-review-pointnet/","previous":{"fields":{"slug":"/2020-09-27-paper-review-mask-rcnn/"},"frontmatter":{"title":"Paper Review: Mask R-CNN"}},"next":{"fields":{"slug":"/2020-10-04-paper-review-mesh-rcnn/"},"frontmatter":{"title":"Paper Review: Mesh R-CNN"}}}},"staticQueryHashes":["63159454"]}