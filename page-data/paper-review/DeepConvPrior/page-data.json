{"componentChunkName":"component---src-templates-paper-review-tsx","path":"/paper-review/DeepConvPrior/","result":{"data":{"site":{"siteMetadata":{"title":"Azmarie Wang","author":"Azmarie Wang"}},"markdownRemark":{"id":"9a46f7ad-d66d-5810-84a0-40832c08b848","excerpt":"Main Contribution The research problem in this paper is room-scale indoor scene synthesis. Generating virtual indoor environments and furniture layout is a…","html":"<h2 id=\"main-contribution\" style=\"position:relative;\"><a href=\"#main-contribution\" aria-label=\"main contribution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Main Contribution</h2>\n<p>The research problem in this paper is room-scale indoor scene synthesis. Generating virtual indoor environments and furniture layout is a practical problem in many applications, such as gaming, AR/VR, furniture retail, and robotics navigation. Inspired by CNN’s ability to learn to recognize and develop internal representations of 2D images and the recent availability of large-scale 3D scene dataset, the authors propose to train a CNN-based model for scene synthesis problems. </p>\n<p>The main contributions of this paper include the proposal of the first CNN-based system for synthesizing indoor scenes taking only the geometry of the room as input, and iteratively generates the room by adding one object at a time. Besides, this system was made possible with the proposal of a novel orthographic top-down view of scenes that are semantically meaningful and 2D-based.</p>\n<h2 id=\"method\" style=\"position:relative;\"><a href=\"#method\" aria-label=\"method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Method</h2>\n<p>Before getting into the details of the generative model, the authors introduce the top-down view representation as a key piece in this approach. The rule of thumb of indoor scene synthesis is that a 3D indoor scene can often be characterized by the 2D object layout as floor plans. Taking this idea further, the authors convert the 3D scene to image-based representation compatible with deep convolutional networks. This consists of an orthographic top-down depth render of the room and semantic features as additional channels including several masks, orientation, and category information. The conversion is done on the selected SUNCG dataset. Now, the top-down views can be learnt and properly analyzed in CNN networks. </p>\n<p>The generative model generates a scene iteratively by placing one object at a time; each of the iterations consists of 3 steps: deciding if adding an object, deciding the category and location of the object, and placing an object instance in the scene. In the first step, a multilayer feed-forward neural network is used to process the current counts of objects and the high-level features from the top-down view representation extracted by CNN. </p>\n<p>As the authors point out, the first two steps are strongly correlated, thus they learn a conditional distribution for the second step. This is calculated with an additional attention mask channel to obtain the softmax distribution of an object category at a location through a second CNN-based model. Intuitively, a finished room layout should have sufficient unoccupied space permitting movement, the authors hence augment the set of object categories with several auxiliary categories which make up the majority of the training sets in this step. </p>\n<p>Lastly, with the location and object category, the model will then generate a 3D model and place it in the room with an orientation. The instance orientation is obtained from another CNN model taking the top-down view and a mask for the geometry of the instance as inputs. </p>\n<h2 id=\"whats-good-and-not-so-good\" style=\"position:relative;\"><a href=\"#whats-good-and-not-so-good\" aria-label=\"whats good and not so good permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What’s good and not so good?</h2>\n<p>In the training of the above three steps, the training dataset is filtered from SUNCG datasets and are manipulated in a way that makes sense intuitively, for example, an even split of positive and negative examples for step 1 and 3, and use 95% of auxiliary categories in training examples for step 2. </p>\n<p>However, I think the breakdown of <b>component-wise training introduces more uncertainty and time complexity compared to an end-to-end trainable network</b>, and it could be difficult to optimize for each component. Since this model is interactive with local scene plausibility, the inference time could be relatively long (4 minutes as noted in the paper). </p>\n<p>This limitation has been addressed in the follow-up paper [1] which factorizes the step of <b>adding each object into a different sequence of decisions for global reasoning</b>. This also solves some failure cases where multiple nightstands are clustered around the bed, as the bed is a clue for nightstands. Noteworthy that this paper has a great detailed section for acknowledging limitations and mentioning possible extensions, including problem domain and speed limitation. </p>\n<p>Overall, I think the <b>top-down view representation is promising</b> in studying indoor room synthesis because it reaps benefits from deep convolutional networks and its powerful image reasoning capabilities.</p>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<p>[1] Ritchie, D., Wang, K., &#x26; Lin, Y. A. (2019). Fast and flexible indoor scene synthesis via deep convolutional generative models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 6182-6190).</p>","fields":{"slug":"/DeepConvPrior/"},"frontmatter":{"shortDate":"November 08, 2020","title":"Deep Convolutional Priors for Indoor Scene Synthesis","categories":["3D","Shape Generation"],"summaryType":"Paper Review","paperPDFLink":"https://kwang-ether.github.io/pdf/deepsynth.pdf","projectLink":null,"projectTitle":null,"reference":"Wang, Kai, Manolis Savva, Angel X. Chang, and Daniel Ritchie. \"Deep convolutional priors for indoor scene synthesis.\" ACM Transactions on Graphics (TOG) 37, no. 4 (2018): 1-14.","published":true,"cover":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIDAQX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB6i1WLGFf/8QAGBAAAwEBAAAAAAAAAAAAAAAAAAECEjH/2gAIAQEAAQUCrrI5lGJEkf/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABoQAAICAwAAAAAAAAAAAAAAAAAxARBRcaH/2gAIAQEABj8CfbQowbP/xAAdEAACAwACAwAAAAAAAAAAAAABEQAhMUFhUZGh/9oACAEBAAE/IWLgtPLruEVTyiz4PcKzI2W45+7KFUvSXigyZn//2gAMAwEAAgADAAAAELzP/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQMBAT8Qp//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAECAQE/EIf/xAAeEAEBAAICAgMAAAAAAAAAAAABEQAhQVFhwTFxkf/aAAgBAQABPxAJj6PA1J4ORRa1QNwXpfWSFyPl1o7XIaHFmvh9mVasMTg0PrByKg7YF/Az/9k=","aspectRatio":1.6666666666666667,"src":"/static/4846c4899bc37422e3783602d1146d02/47498/teaser-optimized.jpg","srcSet":"/static/4846c4899bc37422e3783602d1146d02/9dc27/teaser-optimized.jpg 300w,\n/static/4846c4899bc37422e3783602d1146d02/4fe8c/teaser-optimized.jpg 600w,\n/static/4846c4899bc37422e3783602d1146d02/47498/teaser-optimized.jpg 1200w,\n/static/4846c4899bc37422e3783602d1146d02/e2fb1/teaser-optimized.jpg 1450w","sizes":"(max-width: 1200px) 100vw, 1200px"}}}}}},"pageContext":{"slug":"/DeepConvPrior/"}},"staticQueryHashes":["63159454"]}