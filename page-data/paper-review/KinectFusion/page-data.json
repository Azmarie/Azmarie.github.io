{"componentChunkName":"component---src-templates-paper-review-tsx","path":"/paper-review/KinectFusion/","result":{"data":{"site":{"siteMetadata":{"title":"Azmarie Wang","author":"Azmarie Wang"}},"markdownRemark":{"id":"9ba6ff31-bc40-5203-a546-040691c10cff","excerpt":"Main Contribution The research problem in this paper is 3D surface reconstruction and interaction. Inspired by the wide availability of the low-cost dense…","html":"<h2 id=\"main-contribution\" style=\"position:relative;\"><a href=\"#main-contribution\" aria-label=\"main contribution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Main Contribution</h2>\n<p>The research problem in this paper is 3D surface reconstruction and interaction. Inspired by the wide availability of the low-cost dense cameras, the authors aim to use only the data collected from a Kinect sensor to perform a global 3D dense reconstructions of the scene as they move through space. </p>\n<p>This research setup is meaningful because real-time tracking with infrastructure-free handheld device mimics scenarios in augmented reality (AR) applications, including 3D scanning and gaming. The main contributions include the proposal of KinectFusion, as the first system supporting the real-time 3D surface reconstruction of complex scenes using a depth sensor, and the tracking is always relative to the fully updated fused dense model.</p>\n<h2 id=\"method\" style=\"position:relative;\"><a href=\"#method\" aria-label=\"method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Method</h2>\n<p>The overall system proposed has several components, including surface measurement, pose estimation, update reconstruction, and surface prediction. In surface measurement, the live depth data is roughly converted to 3D points (referred to as point clouds in the paper) and normals in the camera space. Secondly, the authors use ICP alignment with every 3D points to align overlapping 3D point clouds. An important assumption here is that the points clouds between frames are roughly aligned due to small motion from one frame to the next. To calculate the offset transformation, an energy function is defined to minimize the global point-plane movement. This process is iterative until the points are aligned for each frame. </p>\n<p>Later, in surface reconstruction update, the surface is modelled implicitly by a truncated signed distance function (TSDF) representation. This prepares the continuous estimates as discretized values to be feed into the voxel grid. In this component, one implicit design tradeoff is the choice of using the voxel model as the representation. Compared to a polygon mesh, <b>voxel representation is efficient but it restricts the fixed geometry shape and limits model deformations</b>. </p>\n<p>Lastly, the surface prediction is produced by raycasting the grid to extract the position of the implicit surface by following zero crossings (a change of sign in the TSDF values). Putting things together, this real-time reconstruction system utilizes highly parallel GPU implementation to make sure its real-time speed. </p>\n<h2 id=\"whats-good-and-whats-not-so-good\" style=\"position:relative;\"><a href=\"#whats-good-and-whats-not-so-good\" aria-label=\"whats good and whats not so good permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What’s good and what’s not so good?</h2>\n<p>I am intrigued by the application of this paper and by the idea of bringing the physical to the digital world. In the section about experiments, I think the authors did a good job in locating ways to optimize the frame-to-frame results from the baseline model, such as using partial/full loops to combat the accumulating errors between frames, increasing viewpoints, and identifying keyframes. While evaluating a surface reconstruction method, it’s important to <b>consider properties such as scale and speed in addition to quality</b>. I found the <b>regular grid could be a bottleneck for the scalability</b> of this model. The authors mentioned each frame from the depth sensor is a surface measurement on a regular grid, which I understand as a fixed resolution model. </p>\n<p>It’s intuitive to see how a regular grid could make computation easy to manage, but the massive information collected needs to be fit into this <b>fixed grid to be stored in the GPU memory</b>. </p>\n<h2 id=\"future-work\" style=\"position:relative;\"><a href=\"#future-work\" aria-label=\"future work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Future Work</h2>\n<p>I think this leads to the limiting capability of this model that it only works properly indoors. If this were to be expanded to work outdoors, they might want to <b>incorporate more data streams</b> or <b>multiple videos of the scene</b> to calculate the depth as pre-processing information. From the memory space availability perspective, they could use <b>hierarchical techniques</b> such as octrees. The hierarchical layout could stay shallow to balance the tradeoff between time and space while keeping the computation relatively simple.</p>","fields":{"slug":"/KinectFusion/"},"frontmatter":{"shortDate":"October 18, 2020","title":"KinectFusion: Real-Time Dense Surface Mapping and Tracking","categories":["Rendering"],"summaryType":"Paper Review","paperPDFLink":"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf","projectLink":null,"projectTitle":null,"reference":"Izadi, Shahram, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie Shotton et al. \"KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera.\" In Proceedings of the 24th annual ACM symposium on User interface software and technology, pp. 559-568. 2011.","published":true,"cover":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAGABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAIF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAdSgsJ//xAAZEAABBQAAAAAAAAAAAAAAAAABAAIQESH/2gAIAQEAAQUCGQ2wv//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAABITH/2gAIAQEABj8CVwVNP//EABoQAAIDAQEAAAAAAAAAAAAAAAERACFRQWH/2gAIAQEAAT8h0zbmwEtaDyIFteT/2gAMAwEAAgADAAAAEPgP/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQMBAT8QR//EABYRAQEBAAAAAAAAAAAAAAAAAAEAMf/aAAgBAgEBPxBV2W//xAAaEAEBAAMBAQAAAAAAAAAAAAABEQBBkTHh/9oACAEBAAE/ELSQJ63rWQzml9PclrVNfrP/2Q==","aspectRatio":3.4482758620689653,"src":"/static/e2f1eb04068dfbc89daa536a2ffa0104/47498/teaser-optimized.jpg","srcSet":"/static/e2f1eb04068dfbc89daa536a2ffa0104/9dc27/teaser-optimized.jpg 300w,\n/static/e2f1eb04068dfbc89daa536a2ffa0104/4fe8c/teaser-optimized.jpg 600w,\n/static/e2f1eb04068dfbc89daa536a2ffa0104/47498/teaser-optimized.jpg 1200w,\n/static/e2f1eb04068dfbc89daa536a2ffa0104/52258/teaser-optimized.jpg 1800w,\n/static/e2f1eb04068dfbc89daa536a2ffa0104/50587/teaser-optimized.jpg 2400w,\n/static/e2f1eb04068dfbc89daa536a2ffa0104/1fec9/teaser-optimized.jpg 2410w","sizes":"(max-width: 1200px) 100vw, 1200px"}}}}}},"pageContext":{"slug":"/KinectFusion/"}},"staticQueryHashes":["63159454"]}