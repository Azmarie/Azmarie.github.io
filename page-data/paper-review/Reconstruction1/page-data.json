{"componentChunkName":"component---src-templates-paper-review-tsx","path":"/paper-review/Reconstruction1/","result":{"data":{"site":{"siteMetadata":{"title":"Azmarie Wang","author":"Azmarie Wang"}},"markdownRemark":{"id":"b42b5dfa-d9cd-53db-8c2f-60e038d21ac4","excerpt":"Main Contribution The research problem in this paper is image matching and 3D reconstruction. The goal the authors set out was to reconstruct the city of Rome…","html":"<h2 id=\"main-contribution\" style=\"position:relative;\"><a href=\"#main-contribution\" aria-label=\"main contribution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Main Contribution</h2>\n<p>The research problem in this paper is image matching and 3D reconstruction. The goal the authors set out was to reconstruct the city of Rome in 3D from the online 2D photo collection in 24h, hence building Rome in a day. This was inspired by the massive image collections available on the internet, as it is an unprecedented opportunity for computer vision algorithms to take advantage of the rich user-generated content to study the hidden 3D information.</p>\n<p>The main contribution of this work is a novel and parallel distributed matching system that matches vast collections of 2D images and solves large non-linear least squares problems in the 3D reconstruction stage effectively and efficiently.</p>\n<h2 id=\"method\" style=\"position:relative;\"><a href=\"#method\" aria-label=\"method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Method</h2>\n<p>The pipeline the authors proposed has several distinct stages, including feature extraction (pre-processing), imaging matching, track generation, and geometric estimation. This method targets two major problems posed by the massive and unorganized collections of 2D images from different cameras and viewpoints - the correspondence problem and the structure from motion (SFM) problem. For each stage of the pipeline, the authors experiment with different algorithms to explore their performance and scalability. </p>\n<p>First, for the correspondence problem, the author proposed to use SIFT features with ANN, and clean up with RANSAC by imposing the rigid scene constraint. For large scale matching, this problem is better framed as a graph estimation problem - given vertices matches, we want to propose and then verify the set of edges connecting correspondence. This graph is called the match graph. In this stage, the proposals are generated by whole image similarity and query expansion. I find the idea of looking to find the similarity between images close to the concept of word embedding in natural language processing. Applying this idea in computer vision, the authors take the SIFT features and cluster them into “visual words”, and the images become documents that contain such visual words. With this vocabulary tree in the TF-IDF scheme, we have a sparsely connected match graph as the initial proposal, after which, query expansion is used to increase the density of the component connections for the graph. </p>\n<p>Later, in track generation, tracks can be viewed as the connected components in the match graph described above. The second problem to be solved is the SFM problem - given corresponding points, we look for the 3D coordinates of the points of interest, camera parameters, and focal lengths. </p>\n<p>In this paper, this is done by first constructing a skeletal set and then incrementally improving by bundle adjustment. This design decision is intuitively justified by a large amount of redundancy demonstrated in Internet collections. But it seems unclear <b>how many of the images can be bundled for such improvement</b>. Lastly, the final reconstruction of the scene geometry within each cluster is done by a multiview stereo algorithm.</p>\n<h2 id=\"what-do-i-think\" style=\"position:relative;\"><a href=\"#what-do-i-think\" aria-label=\"what do i think permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What do I think?</h2>\n<p>Overall, I think the authors came up with a neat pipeline of a distributed implementation for this task. One concern I have regarding this pipeline is that it seems to be <b>relying heavily on the vectors embedding which are connected components</b> in the match graph. I wonder if this means that the most prominent components will take charge of this process, and the reconstructed 3D model is performant in these areas only, where the less prominent part of the model will be neglected. This leads to my question about the metrics used in evaluating the accuracy of the results. Upon reading the paper, it isn’t clear to me <b>what metric they used to measure the quality</b> of the results. It seems that because the goal of this paper is particularly specific and custom-defined (building a 3D model of a city from internet images under 24 hours), it is less obvious to compare this to previous work. However, I wonder if the authors have compared their work with a <b>ground truth or a highly accurate 3D model</b> of the Colosseum, which would be a helpful addition to make this paper more sound and complete. </p>\n<p>Another limitation I found is the <b>lack of analysis of the problem space</b>. The authors certainly addressed some problems in the downloaded images from the Internet such as diverse angles and different levels of zooming. I am curious to know <b>if the pipeline proposed can be used in a variety of scenes</b>, if it performs on the complex scenes and simple scenes equally well, and the role of the skeletal sets in 3D construction. I think these are all interesting things to explore and to extend from this paper. </p>\n<p>I found the <b>critical skeletal sets idea similar to the critical point concept from the PointNet paper</b>. On this note, I think it would be useful to have an algorithm that identifies the skeletal sets from an extensive unstructured image collection to capture the most significant information to be able to navigate through such an assortment while avoiding redundancy as much as possible. Such an algorithm could help determine the crucial keyframe from a collection, and can also point a direction to what is missing in the collection to <b>foster guided data image collection</b>. </p>","fields":{"slug":"/Reconstruction1/"},"frontmatter":{"shortDate":"October 18, 2020","title":"Building Rome in a Day","categories":["Reconstruction"],"summaryType":"Paper Review","paperPDFLink":"https://www2.cs.duke.edu/courses/spring19/compsci527/papers/Agarwal.pdf","projectLink":null,"projectTitle":null,"reference":"Agarwal, Sameer, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M. Seitz, and Richard Szeliski. \"Building rome in a day.\" Communications of the ACM 54, no. 10 (2011): 105-112.","published":true,"cover":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIDBAX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHrUzqOWD//xAAaEAEAAQUAAAAAAAAAAAAAAAABEQACEiEx/9oACAEBAAEFAlcm7RLSsygc/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhABAQEAAAAAAAAAAAAAAAAAADEg/9oACAEBAAY/AkXH/8QAGxABAAICAwAAAAAAAAAAAAAAAQARECFBUfD/2gAIAQEAAT8hAHrFUbjlPXFhGDSf/9oADAMBAAIAAwAAABAQz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAQEBAQEBAQAAAAAAAAAAAAERIQBRgTH/2gAIAQEAAT8QEOmHc5bar4jOISJ95H6yOfR0WTfB4rSKCzN7/9k=","aspectRatio":1.7751479289940828,"src":"/static/eaf4659f945287b3efec3455735a6961/47498/teaser-optimized.jpg","srcSet":"/static/eaf4659f945287b3efec3455735a6961/9dc27/teaser-optimized.jpg 300w,\n/static/eaf4659f945287b3efec3455735a6961/4fe8c/teaser-optimized.jpg 600w,\n/static/eaf4659f945287b3efec3455735a6961/47498/teaser-optimized.jpg 1200w,\n/static/eaf4659f945287b3efec3455735a6961/ec6c5/teaser-optimized.jpg 1280w","sizes":"(max-width: 1200px) 100vw, 1200px"}}}}}},"pageContext":{"slug":"/Reconstruction1/"}},"staticQueryHashes":["63159454"]}