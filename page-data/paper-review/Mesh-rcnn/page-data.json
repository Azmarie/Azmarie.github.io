{"componentChunkName":"component---src-templates-paper-review-tsx","path":"/paper-review/Mesh-rcnn/","result":{"data":{"site":{"siteMetadata":{"title":"Azmarie Wang","author":"Azmarie Wang"}},"markdownRemark":{"id":"3cd0bd8f-2d7a-562e-8fc5-a7edfe8b6b82","excerpt":"Main Contribution Drawing inspiration from the state-of-art networks in 2D perception and 3D shape prediction, this paper is tackling the problem of 3D object…","html":"<h2 id=\"main-contribution\" style=\"position:relative;\"><a href=\"#main-contribution\" aria-label=\"main contribution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Main Contribution</h2>\n<p>Drawing inspiration from the state-of-art networks in 2D perception and 3D shape prediction, this paper is tackling the problem of 3D object reconstruction. Given a real-world input image, the network aims to output 2D instance segmentation results and 3D triangle mesh for each detected object. The main contribution of this paper includes the proposal of Mesh R-CNN, which extends Mask R-CNN with a novel mesh predictor branch designed for 3D shapes and achieves state-of-art results. </p>\n<p>Additionally, Mesh R-CNN is a pioneer in 3D shape prediction in real-world cluttered scenes with a variety of objects. This research problem is important for real-world applications such as autonomous driving, virtual reality, and other emerging domains. </p>\n<p>The main novel part of the method is to find the right way to predict meshes in the neural network, where they use an iterative mesh refinement with hybrid shape representation to overcome the fixed topology in previous mesh deformation works. </p>\n<h2 id=\"method\" style=\"position:relative;\"><a href=\"#method\" aria-label=\"method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Method</h2>\n<p>Mesh R-CNN expands the 2D instance segmentation system (Mask R-CNN) by adding a mesh prediction branch, which is composed of a hybrid approach of voxel prediction followed by mesh refinement. This two-step approach, according to the authors, helps predicting fine-grained 3D structures. Starting from a RGB input image, the network will first perform 2D instance segmentation, same as Mask R-CNN, to get features per region. Then it will predict a coarse 3D structure as a voxelized prediction for each ROI in the voxel branch, which is later cubified to mesh for the mesh branch, including vertex alignment, graph convolution and refinement stages. </p>\n<p>In vertex alignment, the vertices in the current mesh are projected onto the image plane, and then an aligned feature from the image plane is sampled. Graph convolution merges information along neighbouring vertices in the mesh structure. The mesh refinement stage predicts offsets for all the vertices in the mesh to update the positions. </p>\n<p>In defining the loss of mesh, they convert the mesh into point clouds by sampling the points from the surface and calculate the chamfer distance against the ground truth with regularization to enforce smoothness. Chamfer distance seems to be the L2 distance in 3D shapes, and it could be sensitive to outliers. One limitation of this paper I identified is <b>not experimenting with other metrics</b>, such as F1 score, and/or justify why they have chosen chamfer distance. </p>\n<p>Finally, the voxel and mesh losses alongside the box and mask losses for 2D instance segmentation are used to train the system end-to-end. The results of Mesh R-CNN outperforms its prior work, and it has reasonable completion of the objects even if an object is occluded. </p>\n<h2 id=\"whats-good-and-whats-not-so-good\" style=\"position:relative;\"><a href=\"#whats-good-and-whats-not-so-good\" aria-label=\"whats good and whats not so good permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What’s good and what’s not so good?</h2>\n<p>One thing I found interesting is that compared to the high performing 2D network Mask R-CNN, this network is also trained using fully supervised learning with pairs of images and meshes, however with a much smaller dataset. The Pix3D dataset used for training and testing only contains 10,069 pairs of images and meshes, much smaller than a standard 2D training dataset, such as COCO. </p>\n<p>Imaginable, <b>a 3D dataset is a lot harder to collect and obtain</b>, since it requires expertise to create and annotate the ground truth. I think this work has chosen an intelligent way to deal with this problem, which is “standing on the shoulder of” high-performing 2D fully supervised network and ripe the benefits from the 2D supervision (using a pre-trained model for instance segmentation on COCO). </p>\n<h2 id=\"future-work\" style=\"position:relative;\"><a href=\"#future-work\" aria-label=\"future work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Future Work</h2>\n<p>Relying on 3D supervision doesn’t seem to be the optimal solution, if <b>unsupervised or semi-supervised</b> methods could be developed to relax this constraint. Since there are already 2D examples in unsupervised networks, such as GAN or conditional GAN, this seems like a feasible possible extension.</p>","fields":{"slug":"/Mesh-rcnn/"},"frontmatter":{"shortDate":"October 04, 2020","title":"Mesh R-CNN","categories":["Segmentation","3D"],"summaryType":"Paper Review","paperPDFLink":"https://openaccess.thecvf.com/content_ICCV_2019/papers/Gkioxari_Mesh_R-CNN_ICCV_2019_paper.pdf","projectLink":null,"projectTitle":null,"reference":"Gkioxari, Georgia, Jitendra Malik, and Justin Johnson. \"Mesh r-cnn.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.","published":true,"cover":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB2gUH/8QAFxABAQEBAAAAAAAAAAAAAAAAAQARIf/aAAgBAQABBQIEE26X/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFxAAAwEAAAAAAAAAAAAAAAAAAAEhEP/aAAgBAQAGPwK1jz//xAAZEAEAAwEBAAAAAAAAAAAAAAABABEhUYH/2gAIAQEAAT8hO30diUbp2FABfs//2gAMAwEAAgADAAAAEAPP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGBABAAMBAAAAAAAAAAAAAAAAAQARITH/2gAIAQEAAT8QMN4bctwhFoALIQ3AdY//2Q==","aspectRatio":2.9411764705882355,"src":"/static/186d66cf27d0d758fdea745fa6be3777/47498/teaser-optimized.jpg","srcSet":"/static/186d66cf27d0d758fdea745fa6be3777/9dc27/teaser-optimized.jpg 300w,\n/static/186d66cf27d0d758fdea745fa6be3777/4fe8c/teaser-optimized.jpg 600w,\n/static/186d66cf27d0d758fdea745fa6be3777/47498/teaser-optimized.jpg 1200w,\n/static/186d66cf27d0d758fdea745fa6be3777/52258/teaser-optimized.jpg 1800w,\n/static/186d66cf27d0d758fdea745fa6be3777/50587/teaser-optimized.jpg 2400w,\n/static/186d66cf27d0d758fdea745fa6be3777/249e8/teaser-optimized.jpg 3561w","sizes":"(max-width: 1200px) 100vw, 1200px"}}}}}},"pageContext":{"slug":"/Mesh-rcnn/"}},"staticQueryHashes":["63159454"]}