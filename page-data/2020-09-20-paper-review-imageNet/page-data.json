{"componentChunkName":"component---src-templates-blog-post-tsx","path":"/2020-09-20-paper-review-imageNet/","result":{"data":{"site":{"siteMetadata":{"title":"Azmarie Wang","author":"Azmarie Wang"}},"markdownRemark":{"id":"6c5c9bb7-827e-5d70-91d2-f10ea17e4b0a","excerpt":"ðŸ“– Link to the Paper: ImageNet: a Large-Scale Hierarchical Image Database ðŸ’¡ Link to Project Webstie: ImageNet Whatâ€™s ImageNet? Inspired by the explosion ofâ€¦","html":"<p>ðŸ“– Link to the Paper: <a href=\"https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ImageNet: a Large-Scale Hierarchical Image Database</a></p>\n<p>ðŸ’¡ Link to Project Webstie: <a href=\"http://www.image-net.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ImageNet</a></p>\n<h2 id=\"whats-imagenet\" style=\"position:relative;\"><a href=\"#whats-imagenet\" aria-label=\"whats imagenet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Whatâ€™s ImageNet?</h2>\n<p>Inspired by the explosion of data, ImageNet proposed to target an ambitious research problem - how to harness the power of vast quantities of image data and organize them in such a way thatâ€™s beneficial to a variety of research problems. The main contribution in the paper is the introduction of a large-scale, highly-diverse, and highly-accurate database built on the hierarchical structure of WordNet called <code class=\"language-text\">ImageNet</code>.</p>\n<h2 id=\"method\" style=\"position:relative;\"><a href=\"#method\" aria-label=\"method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Method</h2>\n<p>Constructing an accurate large-scale database is no easy task. In ImageNet, the data was collected by querying several image search engines per synset and then verified by global users leveraging the services of <a href=\"https://www.mturk.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Amazon Mechanical Turk</a> to reach a predetermined confidence score threshold. The paper advanced the state-of-art dataset by its large scale, high accuracy, and large diversity, and also its semantic hierarchy based on WordNet. One limitation of the ImageNet could be its choice of assigning only a single label to each image, itâ€™s not optimal when there are more than one clear objects in the image.</p>\n<h2 id=\"whats-good\" style=\"position:relative;\"><a href=\"#whats-good\" aria-label=\"whats good permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Whatâ€™s good?</h2>\n<p>What I found as inherently novel about ImageNet is its <b>focus and belief in data</b> - a fair representation of the problem space with data is important and can be beneficial to computer vision tasks regardless of the algorithm and models. In hindsight, ImageNet has been proven to be a supreme source of training data and benchmark datasets. </p>\n<h2 id=\"future-work\" style=\"position:relative;\"><a href=\"#future-work\" aria-label=\"future work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Future Work</h2>\n<p>ImageNet and ImageNet Challenge inspired a stream of work in the neural networks which generated groundbreaking results, to the point where transfer learning via pre training on ImageNet is widely used as a standard procedure before fine-tuning on another dataset. One possible extension into the computer graphic tasks will be to extend the ImageNet dataset into 3D, including depth information for a 2D scene. Even though a 3D dataset is even more costly to obtain, it could benefit 3D scene understanding and robotic tasks greatly. </p>","timeToRead":2,"fields":{"slug":"/2020-09-20-paper-review-imageNet/"},"frontmatter":{"title":"Paper Review: ImageNet: a Large-Scale Hierarchical Image Database","shortDate":"September 20, 2020","longDate":"September 20, 2020, 4:15:24 am","description":"Reviewing the iconic ImageNet paper","categories":["Computer Vision","Paper Review"],"cover":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEAv/EABYBAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAABlRVrNSPD/8QAGhAAAgIDAAAAAAAAAAAAAAAAAAIBAxAREv/aAAgBAQABBQJ2aDqwlNi5/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGxAAAgEFAAAAAAAAAAAAAAAAARAAESIycaH/2gAIAQEABj8CsFZjxDT/AP/EABsQAAICAwEAAAAAAAAAAAAAAAARATEQQXEh/9oACAEBAAE/IVfRKZvBhr2bRXmbx//aAAwDAQACAAMAAAAQ8D//xAAVEQEBAAAAAAAAAAAAAAAAAAABAP/aAAgBAwEBPxBYC//EABYRAQEBAAAAAAAAAAAAAAAAAAEQIf/aAAgBAgEBPxAcn//EABwQAQADAAIDAAAAAAAAAAAAAAEAESEQUTGh8P/aAAgBAQABPxBYiG9PSapVmY6fc3d9LKL4geH3UZ//2Q==","aspectRatio":2.3376623376623376,"src":"/static/658ae66c436109e6214d4d7d127b9257/f2e3f/teaser.jpg","srcSet":"/static/658ae66c436109e6214d4d7d127b9257/e2788/teaser.jpg 360w,\n/static/658ae66c436109e6214d4d7d127b9257/7d509/teaser.jpg 720w,\n/static/658ae66c436109e6214d4d7d127b9257/f2e3f/teaser.jpg 1400w","sizes":"(max-width: 1400px) 100vw, 1400px"}}},"coverAuthor":null,"coverOriginalUrl":null,"keywords":["ImageNet","Computer Vision","Image Dataset"],"published":true}}},"pageContext":{"slug":"/2020-09-20-paper-review-imageNet/","previous":{"fields":{"slug":"/2020-08-08-face-morphing/"},"frontmatter":{"title":"Face Morphing: A Step-by-Step Tutorial"}},"next":{"fields":{"slug":"/2020-09-20-paper-review-dilated-convolutions/"},"frontmatter":{"title":"Paper Review: Multi-Scale Context Aggregation by Dilated Convolutions"}}}},"staticQueryHashes":["63159454"]}