<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Azmarie Wang]]></title><description><![CDATA[Azmarie Wang's personal blog]]></description><link>https://azmarie.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Sun, 17 Jan 2021 23:56:01 GMT</lastBuildDate><item><title><![CDATA[Geometry-Aware Style Transfer: Implementation and Analysis]]></title><description><![CDATA[üìñ For report and analysis, check out my article on Medium üåü Want to play with it yourself? Check out my github repo for implementation and‚Ä¶]]></description><link>https://azmarie.github.io/2020-12-27-geometry-aware-style-transfer/</link><guid isPermaLink="false">https://azmarie.github.io/2020-12-27-geometry-aware-style-transfer/</guid><pubDate>Sun, 27 Dec 2020 20:29:57 GMT</pubDate><content:encoded>&lt;!-- 
**tl;dr** This is to document my proje

--- --&gt;
&lt;p&gt;üìñ For report and analysis, check out &lt;a href=&quot;https://azmariewang.medium.com/geometry-aware-style-transfer-implementation-and-analysis-3a9034dfca2d&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;my article on Medium&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üåü Want to play with it yourself? Check out my &lt;a href=&quot;https://github.com/Azmarie/Caricature-Your-Face&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;github repo&lt;/a&gt; for implementation and demo.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Review: Building Rome in a Day]]></title><description><![CDATA[üìñ Link to the Paper: Building Rome in a Day Main Contribution The research problem in this paper is image matching and 3D reconstruction‚Ä¶]]></description><link>https://azmarie.github.io/2020-10-11-paper-review-reconstruction1/</link><guid isPermaLink="false">https://azmarie.github.io/2020-10-11-paper-review-reconstruction1/</guid><pubDate>Sun, 18 Oct 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ Link to the Paper: &lt;a href=&quot;https://www2.cs.duke.edu/courses/spring19/compsci527/papers/Agarwal.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Building Rome in a Day&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is image matching and 3D reconstruction. The goal the authors set out was to reconstruct the city of Rome in 3D from the online 2D photo collection in 24h, hence building Rome in a day. This was inspired by the massive image collections available on the internet, as it is an unprecedented opportunity for computer vision algorithms to take advantage of the rich user-generated content to study the hidden 3D information.&lt;/p&gt;
&lt;p&gt;The main contribution of this work is a novel and parallel distributed matching system that matches vast collections of 2D images and solves large non-linear least squares problems in the 3D reconstruction stage effectively and efficiently.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The pipeline the authors proposed has several distinct stages, including feature extraction (pre-processing), imaging matching, track generation, and geometric estimation. This method targets two major problems posed by the massive and unorganized collections of 2D images from different cameras and viewpoints - the correspondence problem and the structure from motion (SFM) problem. For each stage of the pipeline, the authors experiment with different algorithms to explore their performance and scalability. &lt;/p&gt;
&lt;p&gt;First, for the correspondence problem, the author proposed to use SIFT features with ANN, and clean up with RANSAC by imposing the rigid scene constraint. For large scale matching, this problem is better framed as a graph estimation problem - given vertices matches, we want to propose and then verify the set of edges connecting correspondence. This graph is called the match graph. In this stage, the proposals are generated by whole image similarity and query expansion. I find the idea of looking to find the similarity between images close to the concept of word embedding in natural language processing. Applying this idea in computer vision, the authors take the SIFT features and cluster them into ‚Äúvisual words‚Äù, and the images become documents that contain such visual words. With this vocabulary tree in the TF-IDF scheme, we have a sparsely connected match graph as the initial proposal, after which, query expansion is used to increase the density of the component connections for the graph. &lt;/p&gt;
&lt;p&gt;Later, in track generation, tracks can be viewed as the connected components in the match graph described above. The second problem to be solved is the SFM problem - given corresponding points, we look for the 3D coordinates of the points of interest, camera parameters, and focal lengths. &lt;/p&gt;
&lt;p&gt;In this paper, this is done by first constructing a skeletal set and then incrementally improving by bundle adjustment. This design decision is intuitively justified by a large amount of redundancy demonstrated in Internet collections. But it seems unclear &lt;b&gt;how many of the images can be bundled for such improvement&lt;/b&gt;. Lastly, the final reconstruction of the scene geometry within each cluster is done by a multiview stereo algorithm.&lt;/p&gt;
&lt;h2 id=&quot;whats-good-and-whats-not-so-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good-and-whats-not-so-good&quot; aria-label=&quot;whats good and whats not so good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs good and what‚Äôs not so good?&lt;/h2&gt;
&lt;p&gt;Overall, I think the authors came up with a neat pipeline of a distributed implementation for this task. One concern I have regarding this pipeline is that it seems to be &lt;b&gt;relying heavily on the vectors embedding which are connected components&lt;/b&gt; in the match graph. I wonder if this means that the most prominent components will take charge of this process, and the reconstructed 3D model is performant in these areas only, where the less prominent part of the model will be neglected. This leads to my question about the metrics used in evaluating the accuracy of the results. Upon reading the paper, it isn‚Äôt clear to me &lt;b&gt;what metric they used to measure the quality&lt;/b&gt; of the results. It seems that because the goal of this paper is particularly specific and custom-defined (building a 3D model of a city from internet images under 24 hours), it is less obvious to compare this to previous work. However, I wonder if the authors have compared their work with a &lt;b&gt;ground truth or a highly accurate 3D model&lt;/b&gt; of the Colosseum, which would be a helpful addition to make this paper more sound and complete. &lt;/p&gt;
&lt;p&gt;Another limitation I found is the &lt;b&gt;lack of analysis of the problem space&lt;/b&gt;. The authors certainly addressed some problems in the downloaded images from the Internet such as diverse angles and different levels of zooming. I am curious to know &lt;b&gt;if the pipeline proposed can be used in a variety of scenes&lt;/b&gt;, if it performs on the complex scenes and simple scenes equally well, and the role of the skeletal sets in 3D construction. I think these are all interesting things to explore and to extend from this paper. &lt;/p&gt;
&lt;p&gt;I found the &lt;b&gt;critical skeletal sets idea similar to the critical point concept from the PointNet paper&lt;/b&gt;. On this note, I think it would be useful to have an algorithm that identifies the skeletal sets from an extensive unstructured image collection to capture the most significant information to be able to navigate through such an assortment while avoiding redundancy as much as possible. Such an algorithm could help determine the crucial keyframe from a collection, and can also point a direction to what is missing in the collection to &lt;b&gt;foster guided data image collection&lt;/b&gt;. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Review: KinectFusion: Real-Time Dense Surface Mapping and Tracking]]></title><description><![CDATA[üìñ Link to the Paper: KinectFusion: Real-Time Dense Surface Mapping and Tracking Main Contribution The research problem in this paper is 3D‚Ä¶]]></description><link>https://azmarie.github.io/2020-10-11-paper-review-reconstruction2/</link><guid isPermaLink="false">https://azmarie.github.io/2020-10-11-paper-review-reconstruction2/</guid><pubDate>Sun, 18 Oct 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ Link to the Paper: &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;KinectFusion: Real-Time Dense Surface Mapping and Tracking&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is 3D surface reconstruction and interaction. Inspired by the wide availability of the low-cost dense cameras, the authors aim to use only the data collected from a Kinect sensor to perform a global 3D dense reconstructions of the scene as they move through space. &lt;/p&gt;
&lt;p&gt;This research setup is meaningful because real-time tracking with infrastructure-free handheld device mimics scenarios in augmented reality (AR) applications, including 3D scanning and gaming. The main contributions include the proposal of KinectFusion, as the first system supporting the real-time 3D surface reconstruction of complex scenes using a depth sensor, and the tracking is always relative to the fully updated fused dense model.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The overall system proposed has several components, including surface measurement, pose estimation, update reconstruction, and surface prediction. In surface measurement, the live depth data is roughly converted to 3D points (referred to as point clouds in the paper) and normals in the camera space. Secondly, the authors use ICP alignment with every 3D points to align overlapping 3D point clouds. An important assumption here is that the points clouds between frames are roughly aligned due to small motion from one frame to the next. To calculate the offset transformation, an energy function is defined to minimize the global point-plane movement. This process is iterative until the points are aligned for each frame. &lt;/p&gt;
&lt;p&gt;Later, in surface reconstruction update, the surface is modelled implicitly by a truncated signed distance function (TSDF) representation. This prepares the continuous estimates as discretized values to be feed into the voxel grid. In this component, one implicit design tradeoff is the choice of using the voxel model as the representation. Compared to a polygon mesh, &lt;b&gt;voxel representation is efficient but it restricts the fixed geometry shape and limits model deformations&lt;/b&gt;. &lt;/p&gt;
&lt;p&gt;Lastly, the surface prediction is produced by raycasting the grid to extract the position of the implicit surface by following zero crossings (a change of sign in the TSDF values). Putting things together, this real-time reconstruction system utilizes highly parallel GPU implementation to make sure its real-time speed. &lt;/p&gt;
&lt;h2 id=&quot;whats-good-and-whats-not-so-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good-and-whats-not-so-good&quot; aria-label=&quot;whats good and whats not so good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs good and what‚Äôs not so good?&lt;/h2&gt;
&lt;p&gt;I am intrigued by the application of this paper and by the idea of bringing the physical to the digital world. In the section about experiments, I think the authors did a good job in locating ways to optimize the frame-to-frame results from the baseline model, such as using partial/full loops to combat the accumulating errors between frames, increasing viewpoints, and identifying keyframes. While evaluating a surface reconstruction method, it‚Äôs important to &lt;b&gt;consider properties such as scale and speed in addition to quality&lt;/b&gt;. I found the &lt;b&gt;regular grid could be a bottleneck for the scalability&lt;/b&gt; of this model. The authors mentioned each frame from the depth sensor is a surface measurement on a regular grid, which I understand as a fixed resolution model. &lt;/p&gt;
&lt;p&gt;It‚Äôs intuitive to see how a regular grid could make computation easy to manage, but the massive information collected needs to be fit into this &lt;b&gt;fixed grid to be stored in the GPU memory&lt;/b&gt;. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;I think this leads to the limiting capability of this model that it only works properly indoors. If this were to be expanded to work outdoors, they might want to &lt;b&gt;incorporate more data streams&lt;/b&gt; or &lt;b&gt;multiple videos of the scene&lt;/b&gt; to calculate the depth as pre-processing information. From the memory space availability perspective, they could use &lt;b&gt;hierarchical techniques&lt;/b&gt; such as octrees. The hierarchical layout could stay shallow to balance the tradeoff between time and space while keeping the computation relatively simple.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Review: Interactive Reconstruction of Monte Carlo Image Sequences using a Recurrent Denoising Autoencoder]]></title><description><![CDATA[üìñ Link to the Paper: Interactive Reconstruction of Monte Carlo Image Sequences using a Recurrent Denoising Autoencoder Main Contribution‚Ä¶]]></description><link>https://azmarie.github.io/2020-10-11-paper-review-rendering1/</link><guid isPermaLink="false">https://azmarie.github.io/2020-10-11-paper-review-rendering1/</guid><pubDate>Sun, 11 Oct 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ Link to the Paper: &lt;a href=&quot;https://research.nvidia.com/sites/default/files/publications/dnn_denoise_author.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Interactive Reconstruction of Monte Carlo Image Sequences using a Recurrent Denoising Autoencoder&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is interactive image reconstruction/ rendering. This problem is relevant in the gaming industry and especially in AR/ VR applications enabling real-time experiences. Motivated by the success of image restoration using neural networks in recent years, the authors aimed to target the ray tracing denoising problem as reconstructing image sequences with the help of deep neural networks. &lt;/p&gt;
&lt;p&gt;The main contributions include the proposal of using novel recurrent connections in deep autoencoder networks for temporal stability, which is end-to-end trainable thus can automatically learn relationships based on auxiliary per-pixel input channels. Noteworthy, the network achieved a good execution speed with high-quality results from extremely low sampling frames.   &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The proposed architecture is a recurrent autoencoder with skip connection every second layer, trained end to end in a supervised manner with paired data. In the paper, the authors refer to the autoencoder as denoising autoencoders. Autoencoders are known to be effective in learning to reconstruct the inputs from a minimal latent representation obtained in the bottleneck. The noise is often random and can be averaged out, so it is regarded as non-essential information and won‚Äôt be included in the latent representation. &lt;/p&gt;
&lt;p&gt;Moreover, I think the usage of RNN is also easily justifiable intuitively. The past frames can provide useful information in constructing the current frame through the feedback loop, thus the recurrent connections are a good structure for temporal coherence. The authors made it clear that they are focusing on solving the reconstruction task with an extremely low amount of sampling frames. Thus, using the recurrent structure can obtain illumination information from its sparse sampling point. The temporal features are learnt at multiple scales through recurrent blocks. The architecture is fully convolutional, so it is trainable in small fixed resolution and much faster than training on the full resolution (such as a CNN with skip connection network). &lt;/p&gt;
&lt;p&gt;Further, the network is end-to-end trainable and put auxiliary inputs automatically into use to disambiguate the colour data. The tradeoff of using an RNN is between the &lt;b&gt;temporal coherency and the expensive hierarchy of recurrent blocks&lt;/b&gt;. Since this paper focuses on rendering at an interactive rate with low sample counts, it is not production-quality ready and the performance suffers from low sampling. Besides, neural networks tend to produce what is seen frequently more prominently, thus it may produce poor quality for a complex scene and rare objects. The authors mentioned that they choose to not consider the depth of field and motion blur, which limited the application to gaming and other virtual experiences. &lt;/p&gt;
&lt;h2 id=&quot;whats-good-and-whats-not-so-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good-and-whats-not-so-good&quot; aria-label=&quot;whats good and whats not so good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs good and what‚Äôs not so good?&lt;/h2&gt;
&lt;p&gt;In the experiments, the authors showcased examples where the inputs were highly noisy, and the network generated good quality outputs. However, it would be good to know &lt;b&gt;if the trained network can fit inputs with multiple intensity of noise&lt;/b&gt;, such as mildly noisy and extremely noisy. I think the authors have yet to explore the spatial property of the objects in the video, adding spatial information to be jointly optimized with the temporal information could be a possible extension while keeping the low sample count. &lt;/p&gt;
&lt;p&gt;Overall, this paper is a successful example of using deep learning techniques in image rendering. It seems that deep learning is not only great for image restoration tasks but also is readily employable for image sequences for light transport reconstruction. It set good examples for future deep learning based ray tracing denoisers, where different types of kernels are learnt to substitute for the hand crafted kernels. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Review: A Practical Model for Subsurface Light Transport]]></title><description><![CDATA[üìñ Link to the Paper: A Practical Model for Subsurface Light Transport Main Contribution The research problem in this paper is image‚Ä¶]]></description><link>https://azmarie.github.io/2020-10-11-paper-review-rendering2/</link><guid isPermaLink="false">https://azmarie.github.io/2020-10-11-paper-review-rendering2/</guid><pubDate>Sun, 11 Oct 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ Link to the Paper: &lt;a href=&quot;https://graphics.stanford.edu/papers/bssrdf/bssrdf.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;A Practical Model for Subsurface Light Transport&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is image rendering, designed to accommodate for subsurface scattering in translucent materials such as liquids and human skin. Subsurface scattering refers to the phenomena where light enters the object, it scatters and propagates inside the material, and then either gets absorbed or leaves the object at a separate location of lower intensity. &lt;/p&gt;
&lt;p&gt;The main contribution includes a practical model of BSSRDF, which efficiently renders a realistic simulation of the object capturing the translucency effect, and its mathematical foundation. This research problem is important because most of the material is translucent to some degree in reality. This is especially useful for the development of realistic CGI and medical physics research.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;Compared to the simplified BRDF model where light enters and leaves at the same surface point, the authors utilize the BSSRDF model accommodating phenomena like subsurface scattering. BSSRDF stands for the bidirectional scattering-surface reflectance distribution function, which describes how light enters an object, scatters around it, and then leaves the surface at a different location. In technical terms, it illustrates the relationship between outgoing radiance and the incident flux in light propagation with reduced intensity. Building on the theory of BSSRDF, the authors constructed this model with two parts - the diffusion approximation term and the single scattering term. &lt;/p&gt;
&lt;p&gt;First, the diffusion approximation is based on the isotropic light distribution in highly scattering media, which in simple terms, means roughly the same light distribution measured in different directions. &lt;/p&gt;
&lt;p&gt;As mentioned earlier, the surface scattering transports light from one point to another. Intuitively, the diffusion term is approximated by the distance of these two points and the Fresnel terms of the incoming and the outgoing light vectors. The dipole is the method of estimating the reflectance from two point sources, which is often a solution in diffusion problems. Here, the authors constructed these two point sources on the opposite side of the surface, with positive and negative contributions individually. In this step, the authors &lt;b&gt;assume a locally flat and semi-infinite homogeneous media&lt;/b&gt;. However, in reality, this could hardly be the case. Hence, I think this could be one of the limitations of this method. &lt;/p&gt;
&lt;p&gt;Secondly, the single scattering term extends from a previous model for BRDF. The outgoing radiance after a single scattering is calculated by integrating the incident radiance along the refracted outgoing ray, which accounts for each reflection on the surface. Taking two parts together, the complete model is a sum of the diffusion approximation and the single scattering term.&lt;/p&gt;
&lt;h2 id=&quot;whats-good-and-whats-not-so-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good-and-whats-not-so-good&quot; aria-label=&quot;whats good and whats not so good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs good and what‚Äôs not so good?&lt;/h2&gt;
&lt;p&gt;This model exceeded the state-of-art method by its great results and efficiency. The authors argued that this outperformed the BRDF models since it produces a soft-looking surface that is more realistic and closer to human eye perception for the translucent materials. It also exceeds the brute force path tracing method in speed and efficiency, though could be less performant than a BRDF due to its computational complexity. In my opinion, this paper is novel in proposing a practical model to formulate the problem of subsurface scattering, which paved the foundation for future work and spike in CGI in the movie industry. One limitation of the paper, as mentioned earlier, is the idealistic assumptions made about the scattering media, in reality, which may not be the case. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;In my research around this paper, I found an extension by the author in 2005 [4] on multipole diffusion approximation for better results. Some other extensions were various efforts made in handcrafting solutions for a specific texture such as hair shading [1], rainbows in nature[2], and fur[3]. The results from the above papers are stunning and many of them are mature enough to be applied in the industry. &lt;/p&gt;
&lt;p&gt;This, on the other hand, makes me think about the &lt;b&gt;trade-off between using a traditional handcrafted method like this one and a deep learning method in computer vision and graphic tasks&lt;/b&gt;. Many deep learning networks that we consider as standard these days, such as CNN or GAN, can often be constructed as a general-purpose or multi-tasking architecture, which achieve generally good results on images from different domains. &lt;/p&gt;
&lt;p&gt;However, the &lt;b&gt;generalizability of neural networks&lt;/b&gt; is not guaranteed on edge or real-world complex cases, for example, in the areas the extensions of this work have addressed - complex texture like fur, hair, rainbows. Another potential problem with neural networks in image rendering is the flexibility to deal with an artifact, as we couldn‚Äôt fiddle with the networks directly case by case.&lt;/p&gt;
&lt;h2 id=&quot;reference&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#reference&quot; aria-label=&quot;reference permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Sadeghi, I., Pritchett, H., Jensen, H. W., &amp;#x26; Tamstorf, R. (2010). An artist friendly hair shading system. ACM Transactions on Graphics (TOG), 29(4), 1-10.&lt;/p&gt;
&lt;p&gt;[2] Sadeghi, I., Munoz, A., Laven, P., Jarosz, W., Seron, F., Gutierrez, D., &amp;#x26; Jensen, H. W. (2012). Physically-based simulation of rainbows. ACM Transactions on Graphics (TOG), 31(1), 1-12.&lt;/p&gt;
&lt;p&gt;[3] Yan, L. Q., Sun, W., Jensen, H. W., &amp;#x26; Ramamoorthi, R. (2017). A BSSRDF model for efficient rendering of fur with global illumination. ACM Transactions on Graphics (TOG), 36(6), 1-13.&lt;/p&gt;
&lt;p&gt;[4] Donner, C., &amp;#x26; Jensen, H. W. (2005). Light diffusion in multi-layered translucent materials. ACM Transactions on Graphics (ToG), 24(3), 1032-1039.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Review: Mesh R-CNN]]></title><description><![CDATA[üìñ Link to the Paper: Mesh R-CNN Main Contribution Drawing inspiration from the state-of-art networks in 2D perception and 3D shape‚Ä¶]]></description><link>https://azmarie.github.io/2020-10-04-paper-review-mesh-rcnn/</link><guid isPermaLink="false">https://azmarie.github.io/2020-10-04-paper-review-mesh-rcnn/</guid><pubDate>Sun, 04 Oct 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ Link to the Paper: &lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/papers/Gkioxari_Mesh_R-CNN_ICCV_2019_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Mesh R-CNN&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;Drawing inspiration from the state-of-art networks in 2D perception and 3D shape prediction, this paper is tackling the problem of 3D object reconstruction. Given a real-world input image, the network aims to output 2D instance segmentation results and 3D triangle mesh for each detected object. The main contribution of this paper includes the proposal of Mesh R-CNN, which extends Mask R-CNN with a novel mesh predictor branch designed for 3D shapes and achieves state-of-art results. &lt;/p&gt;
&lt;p&gt;Additionally, Mesh R-CNN is a pioneer in 3D shape prediction in real-world cluttered scenes with a variety of objects. This research problem is important for real-world applications such as autonomous driving, virtual reality, and other emerging domains. &lt;/p&gt;
&lt;p&gt;The main novel part of the method is to find the right way to predict meshes in the neural network, where they use an iterative mesh refinement with hybrid shape representation to overcome the fixed topology in previous mesh deformation works. &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;Mesh R-CNN expands the 2D instance segmentation system (Mask R-CNN) by adding a mesh prediction branch, which is composed of a hybrid approach of voxel prediction followed by mesh refinement. This two-step approach, according to the authors, helps predicting fine-grained 3D structures. Starting from a RGB input image, the network will first perform 2D instance segmentation, same as Mask R-CNN, to get features per region. Then it will predict a coarse 3D structure as a voxelized prediction for each ROI in the voxel branch, which is later cubified to mesh for the mesh branch, including vertex alignment, graph convolution and refinement stages. &lt;/p&gt;
&lt;p&gt;In vertex alignment, the vertices in the current mesh are projected onto the image plane, and then an aligned feature from the image plane is sampled. Graph convolution merges information along neighbouring vertices in the mesh structure. The mesh refinement stage predicts offsets for all the vertices in the mesh to update the positions. &lt;/p&gt;
&lt;p&gt;In defining the loss of mesh, they convert the mesh into point clouds by sampling the points from the surface and calculate the chamfer distance against the ground truth with regularization to enforce smoothness. Chamfer distance seems to be the L2 distance in 3D shapes, and it could be sensitive to outliers. One limitation of this paper I identified is &lt;b&gt;not experimenting with other metrics&lt;/b&gt;, such as F1 score, and/or justify why they have chosen chamfer distance. &lt;/p&gt;
&lt;p&gt;Finally, the voxel and mesh losses alongside the box and mask losses for 2D instance segmentation are used to train the system end-to-end. The results of Mesh R-CNN outperforms its prior work, and it has reasonable completion of the objects even if an object is occluded. &lt;/p&gt;
&lt;h2 id=&quot;whats-good-and-whats-not-so-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good-and-whats-not-so-good&quot; aria-label=&quot;whats good and whats not so good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs good and what‚Äôs not so good?&lt;/h2&gt;
&lt;p&gt;One thing I found interesting is that compared to the high performing 2D network Mask R-CNN, this network is also trained using fully supervised learning with pairs of images and meshes, however with a much smaller dataset. The Pix3D dataset used for training and testing only contains 10,069 pairs of images and meshes, much smaller than a standard 2D training dataset, such as COCO. &lt;/p&gt;
&lt;p&gt;Imaginable, &lt;b&gt;a 3D dataset is a lot harder to collect and obtain&lt;/b&gt;, since it requires expertise to create and annotate the ground truth. I think this work has chosen an intelligent way to deal with this problem, which is ‚Äústanding on the shoulder of‚Äù high-performing 2D fully supervised network and ripe the benefits from the 2D supervision (using a pre-trained model for instance segmentation on COCO). &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;Relying on 3D supervision doesn‚Äôt seem to be the optimal solution, if &lt;b&gt;unsupervised or semi-supervised&lt;/b&gt; methods could be developed to relax this constraint. Since there are already 2D examples in unsupervised networks, such as GAN or conditional GAN, this seems like a feasible possible extension.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Review: PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation]]></title><description><![CDATA[üìñ Link to the Paper: PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation Main Contribution The research problem in‚Ä¶]]></description><link>https://azmarie.github.io/2020-10-04-paper-review-pointnet/</link><guid isPermaLink="false">https://azmarie.github.io/2020-10-04-paper-review-pointnet/</guid><pubDate>Sun, 04 Oct 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ Link to the Paper: &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is 3D Classification and Segmentation. Among different 3D representations, the authors choose to use point cloud for its proximity to raw sensor data and its canonical form. Previously, the point cloud was often converted to other representation before being processed in deep neural networks, which causes artifacts. To solve this problem, the authors designed an effective feature learning directly on the point clouds. The main contributions include the proposal of PointNet, which performs end-to-end learning in 3D and achieves good results in various 3D tasks.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The proposed method is motivated to overcome the challenges of using point clouds as input. &lt;/p&gt;
&lt;p&gt;Firstly, point clouds are orderless, thus the model consuming N points needs to be invariant to N! permutations. Secondly, a rigid transformation should not alter the output, thus the model needs to preserve invariance under transformation. Thirdly, there are interactions among local points among the set of point clouds, which the model needs to capture. To tackle the first challenge, the authors observe that a permutation invariant neural network corresponds to a symmetric function, because in a symmetric function, for any ordering of the arguments, the function value stays the same. &lt;/p&gt;
&lt;p&gt;In this paper, they construct a family of symmetric functions by neural networks by multi-layer perceptron (MLP) and max pooling. Regarding the second challenge, the authors aligned input point clouds a canonical space, which is achieved by applying an affine transformation to input point coordinates, predicted by a mini PointNet T-net. This can be similarly applied to embedding space alignment in feature space with regularization to avoid bad local minimum. So far the network for 3D classification can be constructed, but the last challenge is important for the 3D segmentation network where class scores for each point need to be predicted. The method used here is to concatenate local embeddings and global feature vectors so that per point feature is aware of both local and global information for segmentation. &lt;/p&gt;
&lt;h2 id=&quot;whats-good-and-whats-not-so-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good-and-whats-not-so-good&quot; aria-label=&quot;whats good and whats not so good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs good and what‚Äôs not so good?&lt;/h2&gt;
&lt;p&gt;One limitation of the paper is, in tackling the third challenge, they use the &lt;b&gt;concatenation of local and global embedding.&lt;/b&gt; However, I think it seems extreme to put together embedding for one local point with the embedding of all points (global). Compared to 3D CNN where hierarchical feature learning is used, this method doesn‚Äôt consider multiple-level local context, and may not generalize well to a complex scene with multiple objects. &lt;/p&gt;
&lt;p&gt;Besides, in tackling the challenge of transformation invariance, they use affirm transformation. &lt;b&gt;Will that affect the coordinates in a way that‚Äôs spatially confusing to the network?&lt;/b&gt; For example, would turning a chair upside down with a rotation transformation be a valid transformation? I think the author could have elaborated on T-net a bit more and what are the valid transformation matrices used to tackle this challenge.&lt;/p&gt;
&lt;p&gt;This paper exceeded the prior work with its direct usage of the point cloud in deep learning networks and great results. PointNet is also robust to data corruption including missing data points or outliers thanks to the ‚Äúcritical points‚Äù which can sparsely summarize the shape. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;Overall I think this is a great paper as the beginning for 3D deep learning with point clouds. One extension could be to exploit the benefit of hierarchical feature learning with PointNet or using a structure such as a skip connection to resample multi-layer local context. In hindsight, PointNet++ has proposed hierarchical feature learning precisely to extract information from the local neighbourhood.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Review: Mask R-CNN]]></title><description><![CDATA[üìñ Link to the Paper: Mask R-CNN The R-CNN Family 


 Problem Statement Input: Images with objects Output: Correct masks of all objects in‚Ä¶]]></description><link>https://azmarie.github.io/2020-09-27-paper-review-mask-rcnn/</link><guid isPermaLink="false">https://azmarie.github.io/2020-09-27-paper-review-mask-rcnn/</guid><pubDate>Sun, 27 Sep 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ Link to the Paper: &lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Mask R-CNN&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;the-r-cnn-family&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#the-r-cnn-family&quot; aria-label=&quot;the r cnn family permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The R-CNN Family&lt;/h2&gt;
&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/19af9/b-1.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 33.78378378378378%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAAA2ElEQVQoz61RXQ/CIAzk//83E33Q6NT5rRkMN7exCWM7RxUzNSY+2OQCXI+jLaxtW/yKpmk+ud7eBcMfw5myOEkQxTFUVRFhjIGQZ/COk0lKVTlc8hzTYE68tXeuKCsswhD74xG1tfcKN/sDJrMA5zSF1hp5UWCxWiNYhlhtd8SpssSJCwyGIzi91oYeTi4ZxtMZ6Y039OU6gUPZXc6yDEVnrJQiQ5/rh+0MtL6irmvKVY8OWX+g3phzDiklYmrPvszIa52BEIIQRRGtzpy9/9q3YT/xOH+LG+x9IrukLtoeAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/59245/b-1.webp 148w,
/static/07104a4d1a8b07998ab755f1e76c17c3/4a139/b-1.webp 295w,
/static/07104a4d1a8b07998ab755f1e76c17c3/3b0a6/b-1.webp 590w,
/static/07104a4d1a8b07998ab755f1e76c17c3/fe731/b-1.webp 885w,
/static/07104a4d1a8b07998ab755f1e76c17c3/1d4bd/b-1.webp 1180w,
/static/07104a4d1a8b07998ab755f1e76c17c3/e00f2/b-1.webp 1378w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/f227d/b-1.png 148w,
/static/07104a4d1a8b07998ab755f1e76c17c3/78893/b-1.png 295w,
/static/07104a4d1a8b07998ab755f1e76c17c3/05d41/b-1.png 590w,
/static/07104a4d1a8b07998ab755f1e76c17c3/06dbc/b-1.png 885w,
/static/07104a4d1a8b07998ab755f1e76c17c3/0bd9b/b-1.png 1180w,
/static/07104a4d1a8b07998ab755f1e76c17c3/19af9/b-1.png 1378w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/05d41/b-1.png&quot;
          alt=&quot;RCNN Family&quot;
          title=&quot;RCNN Family&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;RCNN Family&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/c91f3/b-2.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABfklEQVQoz4WSvUvDQBjG8x8J/gUO4qqboIOjIDjYxUEERwu6uNRRcHHTRegkFaSibbVEKoW0IVSr+f5o2jQflzzmTlNL1XrwcLkc/N7nufflBEEAz/Mol8tsr1WraLVa+GslSYJpiyOEgJAIvh/A7fdhmBY03UAQhqB3QRAwhek5iqIRNNNkES77ICSGYRjQVQWKLDNwPy2gaRp0XYdpmvA874fbSTg3/sMPCUqVJpqSwtxQDYdDBqKijuM4huM4sCzr1+dgQBJ/0q8bKlYOSsif3qS3Md5Tp3IqCuj1ehgMBgxIHVO5rsvA1D1NM3IYRjE7XN2pmF89w0VRTIFeGvczKgXatv31NATtdps1TpIkttPGiqLIinHfhgkqlVvUH3m8vgiQlQS2YyPwfeaMKos9bQK4KApwXxMxt7CP5bUcNrbWMTO7g8WlAiz7DaqqodPpsOjdbpc1jjqZbMioKQ9PzyicFJHbLeHw+Bx7+SNsbl+iVm/8O4vj45JBPwDWfaSP6lBdBAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/59245/b-2.webp 148w,
/static/0566b1aa58106dbfbcfe2da01063e141/4a139/b-2.webp 295w,
/static/0566b1aa58106dbfbcfe2da01063e141/3b0a6/b-2.webp 590w,
/static/0566b1aa58106dbfbcfe2da01063e141/fe731/b-2.webp 885w,
/static/0566b1aa58106dbfbcfe2da01063e141/1d4bd/b-2.webp 1180w,
/static/0566b1aa58106dbfbcfe2da01063e141/31901/b-2.webp 1398w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/f227d/b-2.png 148w,
/static/0566b1aa58106dbfbcfe2da01063e141/78893/b-2.png 295w,
/static/0566b1aa58106dbfbcfe2da01063e141/05d41/b-2.png 590w,
/static/0566b1aa58106dbfbcfe2da01063e141/06dbc/b-2.png 885w,
/static/0566b1aa58106dbfbcfe2da01063e141/0bd9b/b-2.png 1180w,
/static/0566b1aa58106dbfbcfe2da01063e141/c91f3/b-2.png 1398w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/05d41/b-2.png&quot;
          alt=&quot;RCNN&quot;
          title=&quot;RCNN&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;RCNN&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/ab1a16397e76571184562efc47123e50/782ef/b-3.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABaElEQVQoz4VSTUvDQBDN3/PqLxDEkyfP3jwJetCT9CCCHz14UChCe/ASEBIVIhFjqZVKKGnSxNi0SdMm6ea5u5iwthUfPDLZzL6ZNxlJ13WoqgrDMKBpGo3vafwKRVHgOA4Y8jznFGORxTmDxILZbIbJNMFgMEA8HiMcjTCkJCRfECwgvouxJCb1PkPIWgvPbQvTnwJjWkC8KJIQsnAmCdn8YTR9uB4TIRjRLqMoQhzH/HIh6vs+LxaGIeYhOf0+TNPkVk17gqv6B+RHB2Y/wnAYIAgCJElS2mLCnU4H3W6Xk83ZdV14nsfzpCzNkJMM770AGzsaVtZUbO49wTADTOKIWo5/dcAELcuCbdtckHWapikd0ZR/Ky2/9SIc3LSwe/mC/Vob57cGLUR4UpZlpeX/UApWGi2sH8rYOn7A6nYVRxc1NjE+Q0Zm56+1WfpTFL2JauMOZ9d1VE5O8eV7pcVlOze/fwW+AcSyq8fCttyWAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/ab1a16397e76571184562efc47123e50/59245/b-3.webp 148w,
/static/ab1a16397e76571184562efc47123e50/4a139/b-3.webp 295w,
/static/ab1a16397e76571184562efc47123e50/3b0a6/b-3.webp 590w,
/static/ab1a16397e76571184562efc47123e50/fe731/b-3.webp 885w,
/static/ab1a16397e76571184562efc47123e50/1d4bd/b-3.webp 1180w,
/static/ab1a16397e76571184562efc47123e50/ef412/b-3.webp 1402w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/ab1a16397e76571184562efc47123e50/f227d/b-3.png 148w,
/static/ab1a16397e76571184562efc47123e50/78893/b-3.png 295w,
/static/ab1a16397e76571184562efc47123e50/05d41/b-3.png 590w,
/static/ab1a16397e76571184562efc47123e50/06dbc/b-3.png 885w,
/static/ab1a16397e76571184562efc47123e50/0bd9b/b-3.png 1180w,
/static/ab1a16397e76571184562efc47123e50/782ef/b-3.png 1402w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/ab1a16397e76571184562efc47123e50/05d41/b-3.png&quot;
          alt=&quot;Fast RCNN&quot;
          title=&quot;Fast RCNN&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Fast RCNN&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/772653a28863423d165820e8de688991/b85b2/b-4.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABbklEQVQoz4VSTUvDQBDNv/NfKPgTBI96FVEv9iSFHgTvKqIgSgWDSmNAW81Xa+ohxlpt03wnm6TP7GokrYU+GHZ3Zvbtm9nhTNOEKIrMZFmGJEngeR7NZhOCIEBVVRQYj8eYBy5NU4RhCMdxYVkWfN/L9zYGw2G+OixWkBWExX7axwjL7JbjQ5ReIHdN+CFBEATMygpnqSz7uOngk/YFRR/SNKbQdV3EcQxaSQHP81jMtm0kSTKpkCQZBm4M24uR5M5rwQB/9wmjH2A0sn/b4CPLsj9CeqaE1MoPUVLuUR+gevqM84aCjaqMhcVbLK7d4+LhHSSOcjX+v9J6vR77LE3ToOs6W6Mo+lEYkRRufun1rY/KUQvrNQHbhyqqZy3mTxKCME+mZRcwDINNBCXqdDpQFIW1YaKH9dYHlndvsFJrYGnnEqtb+0hIzIjo64SQuaPDSi4SlK6B2vEVDk7q2Kzsod3WmJ/2btaIzDKKb/MsqHtpsB6TAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/772653a28863423d165820e8de688991/59245/b-4.webp 148w,
/static/772653a28863423d165820e8de688991/4a139/b-4.webp 295w,
/static/772653a28863423d165820e8de688991/3b0a6/b-4.webp 590w,
/static/772653a28863423d165820e8de688991/fe731/b-4.webp 885w,
/static/772653a28863423d165820e8de688991/1d4bd/b-4.webp 1180w,
/static/772653a28863423d165820e8de688991/ed4c1/b-4.webp 1368w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/772653a28863423d165820e8de688991/f227d/b-4.png 148w,
/static/772653a28863423d165820e8de688991/78893/b-4.png 295w,
/static/772653a28863423d165820e8de688991/05d41/b-4.png 590w,
/static/772653a28863423d165820e8de688991/06dbc/b-4.png 885w,
/static/772653a28863423d165820e8de688991/0bd9b/b-4.png 1180w,
/static/772653a28863423d165820e8de688991/b85b2/b-4.png 1368w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/772653a28863423d165820e8de688991/05d41/b-4.png&quot;
          alt=&quot;FastER RCNN&quot;
          title=&quot;FastER RCNN&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;FastER RCNN&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&quot;problem-statement&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#problem-statement&quot; aria-label=&quot;problem statement permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;: Images with objects&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;: Correct masks of all objects in the image while also precisely segmenting each instance.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Application&lt;/b&gt;: Autonomous driving, medical imaging, human pose estimation, etc.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Goal of this Mask R-CNN&lt;/b&gt;: To create a meta-algorithm to support future research of instance segmentation, that has good speed/accuracy and is intuitive/easy-to-use.&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is instance segmentation. Compared to object detection where a bounding box is determined per instance, instance segmentation predicts pixel-level segmentation for each instance, in addition to the object class. After the authors examine the prior work in object detection and semantic segmentation, they decide to take the best of both worlds and solve the instance segmentation problem by augmenting the Faster R-CNN model with a mask branch that is a small FCN. &lt;/p&gt;
&lt;p&gt;The main contribution of this paper is the proposal of Mask R-CNN, as a fast and easy-to-use network with great accuracy, for it to serve as a meta-algorithm and a solid foundation for future instance-level research. &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/5a42c88461a6855175ed5f830da32812/cb752/1.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABz0lEQVQoz22Sz07bQBDG8xA8Qt+CA4ee+gY9V6pQEe1jwKFHJC5InLlUlVraAyciRQERJQFCnKQhUkpxiP/Gf9axvevk68xaRIAYaWV7duY3n7/dyng8RrVaRb9n4LLdRq1Wg2EY6Ha7qNfr6HQ6aLVaaDabaNO+UgrL5VIvjqfvi8UCFbwSUkoURYFiUegihnAuz/NndY+gx9DAB8vCl+0P+PzpIzY23uLn8a8V1AoFXJHqxizLMJlMMOj10CXVvueVdTQso0FSD1Wo3AwMbL1fx9HOFt6sVbC//xX9/h8Mh0Oksxk820YqC1jTKW4pd9looHl+hinBOWIxh+P6SJIEOQNt9y++HWyi8WMXezvvcHJyCNuekRoTKRUFQQDTszHs9zC4vsbdaASTYAzgmKcpPN9HHMelwjxLcHP1G6PBKRoX36n4duVJrDJEMsWdP0U8F5DULOn32F/2q7QiRxCGEFqhLA9FJDkerNITFUZIfU8rME0TNnmsSKXvuvh3fw8Ri2eHUhCYlfEQpYoSmCRkvusgDAMIIcgXgRn7R8a7BIriCAF9O+Qn7zOIFb6M1bXhE2SvbGpwHEc/LVLmkzcM5BzDefGgKIo0+LV7+B+4L53rXG0qJgAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/5a42c88461a6855175ed5f830da32812/59245/1.webp 148w,
/static/5a42c88461a6855175ed5f830da32812/4a139/1.webp 295w,
/static/5a42c88461a6855175ed5f830da32812/3b0a6/1.webp 590w,
/static/5a42c88461a6855175ed5f830da32812/fe731/1.webp 885w,
/static/5a42c88461a6855175ed5f830da32812/1d4bd/1.webp 1180w,
/static/5a42c88461a6855175ed5f830da32812/39706/1.webp 1434w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/5a42c88461a6855175ed5f830da32812/f227d/1.png 148w,
/static/5a42c88461a6855175ed5f830da32812/78893/1.png 295w,
/static/5a42c88461a6855175ed5f830da32812/05d41/1.png 590w,
/static/5a42c88461a6855175ed5f830da32812/06dbc/1.png 885w,
/static/5a42c88461a6855175ed5f830da32812/0bd9b/1.png 1180w,
/static/5a42c88461a6855175ed5f830da32812/cb752/1.png 1434w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/5a42c88461a6855175ed5f830da32812/05d41/1.png&quot;
          alt=&quot;RoI Align&quot;
          title=&quot;RoI Align&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;RoI Align&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/0f7a111d3dff9960260c847b79234ff3/5771b/2.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABUUlEQVQoz11Ri27CMAzs//8YCIGEBIIx6GOUVnSFtaU0pfRxy3lL6GbJin2xL2fHWa1WmE6n2Gw2WK/XWCwWILbb7TCfz7FcLgVnPJlMsN1uMZvNcDgcQBuGAcYYO8/nE/f7HbfbDapWEhdFgaqqUNe1bej7Xpx513UWZ0wOkwshGx/1Q5MoISIxsTEhTxKOjfn1ehUR5s7p+05eoapS5X+ajJqyLJGmqTwwJifuuq7glrCqfsY9J2fEyQnMS1UIEZ2K9/s9LpcLmqaxhJwsCALZ/WeSvHbIV7IsQxzHOIURPN/Du/umGxqrwvd9UaKUso1U5Xm6Vn9OrmuG18ivZcuS2w7pZ4q2bS1GNVEUya4MIb3RNYH+daXX0ZtPMQUkYCPP/zukE+dp77SIVueZHrck4S/uGAVhGIpTyfhnE91A7Hg8yvjGuHdiH7rnK89t/TcRHrSU97Rh5QAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/0f7a111d3dff9960260c847b79234ff3/59245/2.webp 148w,
/static/0f7a111d3dff9960260c847b79234ff3/4a139/2.webp 295w,
/static/0f7a111d3dff9960260c847b79234ff3/3b0a6/2.webp 590w,
/static/0f7a111d3dff9960260c847b79234ff3/fe731/2.webp 885w,
/static/0f7a111d3dff9960260c847b79234ff3/1d4bd/2.webp 1180w,
/static/0f7a111d3dff9960260c847b79234ff3/a078f/2.webp 1356w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/0f7a111d3dff9960260c847b79234ff3/f227d/2.png 148w,
/static/0f7a111d3dff9960260c847b79234ff3/78893/2.png 295w,
/static/0f7a111d3dff9960260c847b79234ff3/05d41/2.png 590w,
/static/0f7a111d3dff9960260c847b79234ff3/06dbc/2.png 885w,
/static/0f7a111d3dff9960260c847b79234ff3/0bd9b/2.png 1180w,
/static/0f7a111d3dff9960260c847b79234ff3/5771b/2.png 1356w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/0f7a111d3dff9960260c847b79234ff3/05d41/2.png&quot;
          alt=&quot;Parallel Head&quot;
          title=&quot;Parallel Head&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Parallel Head&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/bcfdc/3.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 44.5945945945946%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABkUlEQVQoz41SXXObMBD0//9TfegkmdoOoR4zBlxsMCC+JDDfxMJsdUrq9iEPvZmdPQlpuVvdSgiB7XYL27ZxOBxgGAYsy4Lv+zBNU++dz2c4joPdbof9fq/z9XqDrutAsSyLBsVqGAYkSYI8z1EUxSOvqgqcc5RlqfPr9Yq6rhXXaJpWfROQNwnSWe7/CNLhvu/Rtq062OicmNYkxnmhKqmV2BVfx/IAaWrBNE3heR6iKNKtEsIwhPlm4Ol5jVfjjIKXaGqJwBsQ+ZPiDnFwA0+AnEFV+1khVXE6nbRXx+Pxw0sFYtd14Nq/YLzYyHIOkbUwvlkwv9v4qWC9eHDWF7ibEHKa/7bMGNNVxXGsOQgCvUcILxfkKdP+YpmxyAH326B41Dy/94p7MlL7qAWllJjn+YFxHMGUOFlBrReighAc/xMrer2vgn5EEOpREuYrwRLzrcPURpg6hrGNNb8rTBox7rL5EKRZpMvk558xybJMj0jCLji5P1TLHFNfgMcbCGagiLaKX1Glb3pdsg3mMcFv7hal4RMNIw8AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/59245/3.webp 148w,
/static/89edc638680a0e8cefea2fe3e05b7b33/4a139/3.webp 295w,
/static/89edc638680a0e8cefea2fe3e05b7b33/3b0a6/3.webp 590w,
/static/89edc638680a0e8cefea2fe3e05b7b33/fe731/3.webp 885w,
/static/89edc638680a0e8cefea2fe3e05b7b33/1d4bd/3.webp 1180w,
/static/89edc638680a0e8cefea2fe3e05b7b33/667df/3.webp 1390w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/f227d/3.png 148w,
/static/89edc638680a0e8cefea2fe3e05b7b33/78893/3.png 295w,
/static/89edc638680a0e8cefea2fe3e05b7b33/05d41/3.png 590w,
/static/89edc638680a0e8cefea2fe3e05b7b33/06dbc/3.png 885w,
/static/89edc638680a0e8cefea2fe3e05b7b33/0bd9b/3.png 1180w,
/static/89edc638680a0e8cefea2fe3e05b7b33/bcfdc/3.png 1390w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/05d41/3.png&quot;
          alt=&quot;FCN Mask Head&quot;
          title=&quot;FCN Mask Head&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;FCN Mask Head&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/85932b3c8a94651d46f227321be5af34/6f34f/4.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABqklEQVQoz32SS2/TUBCF8wNZ8wMQEltYsQIJIbY8Be2GXSR2RBSkIiEhFthNKDROQkpC6qYhlRO7cePEj9jx4+PeK1zRqjDSyGdGc854Zm4lCAI07TOdb7uYPzrUdzRa7Ta9Xk/kNQzDoC1iXddpNpsqlrjT6dBoNBQ2TRNpRVFQKQTI/DmT8QH97nc8Z8piuSRdJ/jiq3CaKg/DkCzLiKKI9XqtYpmXuTPBNM8pgjnP3+jcfPSaaOFiHjvoxoDDsSPIIZ7nkQni/0yKSatkQpDIYzIa8UuMfNQdMjyecWQ5TE8D4mSNH0bkeSFZiniZnxvZWcSqOEhy3n0YYuwvcGdzvIMmrtklGPfJxQpK0t8CF0UrI9vnaW2Pj18GXL9jcOVanVtP2uwPTwjdKb7rUMShanhxxEv/ME5S3NM5X/sW96u73H25x8Otn7yt/7lc6aI4F+tZrVYkSfLvHZaJjfcDbrzY4Xa1xdV7NTZebSuppbiyfFpSSF7Wtm0sy8J1XWaz2ZnLZkqwVN7WWmzWPvGsusWDx5uc2JNznUssReM4Vg3k85Eu47LuN77LoEBj7jKNAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/85932b3c8a94651d46f227321be5af34/59245/4.webp 148w,
/static/85932b3c8a94651d46f227321be5af34/4a139/4.webp 295w,
/static/85932b3c8a94651d46f227321be5af34/3b0a6/4.webp 590w,
/static/85932b3c8a94651d46f227321be5af34/fe731/4.webp 885w,
/static/85932b3c8a94651d46f227321be5af34/1d4bd/4.webp 1180w,
/static/85932b3c8a94651d46f227321be5af34/94ef7/4.webp 1428w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/85932b3c8a94651d46f227321be5af34/f227d/4.png 148w,
/static/85932b3c8a94651d46f227321be5af34/78893/4.png 295w,
/static/85932b3c8a94651d46f227321be5af34/05d41/4.png 590w,
/static/85932b3c8a94651d46f227321be5af34/06dbc/4.png 885w,
/static/85932b3c8a94651d46f227321be5af34/0bd9b/4.png 1180w,
/static/85932b3c8a94651d46f227321be5af34/6f34f/4.png 1428w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/85932b3c8a94651d46f227321be5af34/05d41/4.png&quot;
          alt=&quot;Putting things together&quot;
          title=&quot;Putting things together&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Putting things together&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&quot;whats-good-and-whats-not-so-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good-and-whats-not-so-good&quot; aria-label=&quot;whats good and whats not so good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs good and what‚Äôs not so good?&lt;/h2&gt;
&lt;p&gt;Mask R-CNN is an intuitive extension from Faster R-CNN with a few unique corrections for instance segmentation task, including RoIAlign and a parallel FCN mask head. RoIAlign is proposed to combat quantization from RoIPool to protect the pixel-to-pixel alignment. The authors argued by experiment results that decoupling segmentation from classification and bounding-box regression is preferred than the coupling of multiple tasks. Thus, Mask R-CNN adds a separate mask head to the Faster R-CNN network to predict a binary mask for each class independently. &lt;/p&gt;
&lt;p&gt;However, the authors didn‚Äôt give a formal proof of &lt;b&gt;why decoupling is more desirable than making masks&lt;/b&gt; across classes compete against each other. I consider this a minor limitation of the paper. An implicit trade-off of the Mask R-CNN design is the &lt;b&gt;accuracy vs. speed balance&lt;/b&gt;, since Mask R-CNN uses the Faster R-CNN network, it also inherent the speed limitation from it as a two-stage network. Compared to a single-stage network YOLO, Mask R-CNN is slower in the object detection task, but more accurate thanks to its a localization step that preserves the spatial coherence for the segmentation. &lt;/p&gt;
&lt;p&gt;Overall, this extension is intuitive, and it generates non-trivial improvement on multiple tasks including object detection, instance segmentation, and human pose estimation with the same framework. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;Mask RCNN is an influential paper, which has been cited 9714 times (Jan 2021). In my opinion, this paper not only advanced the state-of-art networks by its accuracy but more importantly, the vision behind it - &lt;b&gt;you don‚Äôt necessarily need a heavily-engineered complex structure to achieve a fundamental improvement&lt;/b&gt;. &lt;/p&gt;
&lt;p&gt;As the authors did in this paper, observing how the achievement from a previous task (object detection and semantic segmentation in this case) can benefit a problem from an unsolved task (instance segmentation) and putting things together in a way that make sense (intuitive extension to include the mask and avoid quantization) can also produce great results. With the idea behind Mask R-CNN, we can extend even faster object detection networks to solve instance segmentation such as YOLO.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/843f9/5.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 31.756756756756754%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAiUlEQVQY06WPywoAIQhF+//fbFFQFL2JHs4oGNN6BFGveTQBr40xoJQCWmtwzlGeUoIY44noqIcQSMs5n36tFTGw9wbBQGxIKcFaS1Dv/Rn4Qhn0XYbaBZxz0naEGWNORI0dr+BruOYcQRdwrUWP8ctKKdrae7+GW2uXs4bxAnLx15gjuPjrzHkAipzUkeCkp/YAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/59245/5.webp 148w,
/static/5a84631efaec934c65cba2cb2f9d3727/4a139/5.webp 295w,
/static/5a84631efaec934c65cba2cb2f9d3727/3b0a6/5.webp 590w,
/static/5a84631efaec934c65cba2cb2f9d3727/fe731/5.webp 885w,
/static/5a84631efaec934c65cba2cb2f9d3727/1d4bd/5.webp 1180w,
/static/5a84631efaec934c65cba2cb2f9d3727/ff8d7/5.webp 1440w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/f227d/5.png 148w,
/static/5a84631efaec934c65cba2cb2f9d3727/78893/5.png 295w,
/static/5a84631efaec934c65cba2cb2f9d3727/05d41/5.png 590w,
/static/5a84631efaec934c65cba2cb2f9d3727/06dbc/5.png 885w,
/static/5a84631efaec934c65cba2cb2f9d3727/0bd9b/5.png 1180w,
/static/5a84631efaec934c65cba2cb2f9d3727/843f9/5.png 1440w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/05d41/5.png&quot;
          alt=&quot;Future Work&quot;
          title=&quot;Future Work&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Future Work&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Review: Multi-Scale Context Aggregation by Dilated Convolutions]]></title><description><![CDATA[üìñ Link to the Paper: Multi-Scale Context Aggregation by Dilated Convolutions Main Contribution The research problem in this paper is‚Ä¶]]></description><link>https://azmarie.github.io/2020-09-20-paper-review-dilated-convolutions/</link><guid isPermaLink="false">https://azmarie.github.io/2020-09-20-paper-review-dilated-convolutions/</guid><pubDate>Sun, 20 Sep 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ Link to the Paper: &lt;a href=&quot;http://vladlen.info/papers/dilated-convolutions.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Multi-Scale Context Aggregation by Dilated Convolutions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is semantic segmentation. The authors observed that the existing semantic segmentation methods were mostly repurposed convolutional networks from image classification tasks, and they want to challenge this adaptation by proposing a model specifically designed for dense prediction. The main contribution includes, firstly, proposing a module using dilated convolutions to aggregate multi-scale contextual information while preserving resolution; secondly, challenge the necessity of the vestigial components that had been developed for image classification in semantic segmentation networks.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The proposed method is motivated by the structure of the full-resolution dense prediction problem and the nice exponentially expanding property of dilated convolution that makes it a natural potential solution. The authors constructed the context module with multiple dilated convolution layers of an increasing dilation rate to effectively learn from a large receptive field without losing resolution. This paper also challenged the existing approaches in dealing with the conflicting demands of reducing resolution in imaging classification and the need for full-resolution output in dense prediction. The authors proposed to remove the last two pooling and striding layers entirely to increase accuracy, rather than applying post hoc measures.&lt;/p&gt;
&lt;h2 id=&quot;whats-good-and-whats-not-so-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good-and-whats-not-so-good&quot; aria-label=&quot;whats good and whats not so good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs good and what‚Äôs not so good?&lt;/h2&gt;
&lt;p&gt;Overall, I found this paper valuable as it proposed an alternative view on the adaptation of image classification networks on semantic segmentation tasks. The repurposed networks outperformed the status quo, however, are all parts of the network necessary for the task at hand? It might not be the case, as proven in this paper. &lt;/p&gt;
&lt;p&gt;One thing I find not abundantly clear in this paper is that it seems to assume that loss of resolution in the operation throughout the network is inherently bad, and &lt;b&gt;preserving the full resolution is inherently desirable&lt;/b&gt; for dense prediction problems. I don‚Äôt think the authors drive the point home by justifying this assumption. Besides, the authors didn‚Äôt mention &lt;b&gt;the benefits of transfer learning from pre-training&lt;/b&gt; on the image classification task, which I find very relevant in arguing the structural difference between these two tasks. &lt;/p&gt;
&lt;p&gt;From my point of view, this paper would be more convincing if the author explained the assumption, and tried to justify both points of view by presenting a formal comparison between the rectangular prisms this paper proposed to the standard pyramid-shaped architectures carried from image classification tasks.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Review: ImageNet: a Large-Scale Hierarchical Image Database]]></title><description><![CDATA[üìñ Link to the Paper: ImageNet: a Large-Scale Hierarchical Image Database üí° Link to Project Webstie: ImageNet What‚Äôs ImageNet? Inspired by‚Ä¶]]></description><link>https://azmarie.github.io/2020-09-20-paper-review-imageNet/</link><guid isPermaLink="false">https://azmarie.github.io/2020-09-20-paper-review-imageNet/</guid><pubDate>Sun, 20 Sep 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ Link to the Paper: &lt;a href=&quot;https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;ImageNet: a Large-Scale Hierarchical Image Database&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üí° Link to Project Webstie: &lt;a href=&quot;http://www.image-net.org/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;ImageNet&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;whats-imagenet&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-imagenet&quot; aria-label=&quot;whats imagenet permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs ImageNet?&lt;/h2&gt;
&lt;p&gt;Inspired by the explosion of data, ImageNet proposed to target an ambitious research problem - how to harness the power of vast quantities of image data and organize them in such a way that‚Äôs beneficial to a variety of research problems. The main contribution in the paper is the introduction of a large-scale, highly-diverse, and highly-accurate database built on the hierarchical structure of WordNet called &lt;code class=&quot;language-text&quot;&gt;ImageNet&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;Constructing an accurate large-scale database is no easy task. In ImageNet, the data was collected by querying several image search engines per synset and then verified by global users leveraging the services of &lt;a href=&quot;https://www.mturk.com/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Amazon Mechanical Turk&lt;/a&gt; to reach a predetermined confidence score threshold. The paper advanced the state-of-art dataset by its large scale, high accuracy, and large diversity, and also its semantic hierarchy based on WordNet. One limitation of the ImageNet could be its choice of assigning only a single label to each image, it‚Äôs not optimal when there are more than one clear objects in the image.&lt;/p&gt;
&lt;h2 id=&quot;whats-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good&quot; aria-label=&quot;whats good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs good?&lt;/h2&gt;
&lt;p&gt;What I found as inherently novel about ImageNet is its &lt;b&gt;focus and belief in data&lt;/b&gt; - a fair representation of the problem space with data is important and can be beneficial to computer vision tasks regardless of the algorithm and models. In hindsight, ImageNet has been proven to be a supreme source of training data and benchmark datasets. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;ImageNet and ImageNet Challenge inspired a stream of work in the neural networks which generated groundbreaking results, to the point where transfer learning via pre training on ImageNet is widely used as a standard procedure before fine-tuning on another dataset. One possible extension into the computer graphic tasks will be to extend the ImageNet dataset into 3D, including depth information for a 2D scene. Even though a 3D dataset is even more costly to obtain, it could benefit 3D scene understanding and robotic tasks greatly. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Face Morphing: A Step-by-Step Tutorial]]></title><description><![CDATA[üìñ For the step-by-step tutorial with intuitive reasoning, check out my article on Medium üåü Want to play with it yourself? Check out my‚Ä¶]]></description><link>https://azmarie.github.io/2020-08-08-face-morphing/</link><guid isPermaLink="false">https://azmarie.github.io/2020-08-08-face-morphing/</guid><pubDate>Sat, 08 Aug 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ For the step-by-step tutorial with intuitive reasoning, check out &lt;a href=&quot;https://azmariewang.medium.com/face-morphing-a-step-by-step-tutorial-with-code-75a663cdc666&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;my article on Medium&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üåü Want to play with it yourself? Check out my &lt;a href=&quot;https://github.com/Azmarie/Face-Morphing&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;github repo&lt;/a&gt; for implementation and demo.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Hairstyle Transfer ‚Äî Semantic Editing GAN Latent Code]]></title><description><![CDATA[üìñ For report and analysis, check out my article on Medium üåü Want to play with it yourself? Check out my github repo for implementation and‚Ä¶]]></description><link>https://azmarie.github.io/2020-03-24-Hairstyle-Transfer/</link><guid isPermaLink="false">https://azmarie.github.io/2020-03-24-Hairstyle-Transfer/</guid><pubDate>Tue, 24 Mar 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ For report and analysis, check out &lt;a href=&quot;https://medium.com/swlh/hairstyle-transfer-semantic-editing-gan-latent-code-b3a6ccf91e82&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;my article on Medium&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üåü Want to play with it yourself? Check out my &lt;a href=&quot;https://github.com/Azmarie/Hairstyle-Transfer&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;github repo&lt;/a&gt; for implementation and demo.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Just Use Any: js.la Edition]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2020/js-la/</link><guid isPermaLink="false">https://azmarie.github.io/2020/js-la/</guid><pubDate>Thu, 30 Jan 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Ambitious UIs for Pitch to Play Workflows]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2019/netflix/</link><guid isPermaLink="false">https://azmarie.github.io/2019/netflix/</guid><pubDate>Wed, 13 Nov 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Just Use Any: How to Win Colleagues & Influence Your Boss]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2019/tsconf/</link><guid isPermaLink="false">https://azmarie.github.io/2019/tsconf/</guid><pubDate>Fri, 11 Oct 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Learning to Love Type Systems]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2018/dotjs/</link><guid isPermaLink="false">https://azmarie.github.io/2018/dotjs/</guid><pubDate>Wed, 28 Nov 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Learning to Love Type Systems]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2018/reactathon/</link><guid isPermaLink="false">https://azmarie.github.io/2018/reactathon/</guid><pubDate>Sat, 08 Sep 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Learning to Love Type Systems]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2018/react-rally/</link><guid isPermaLink="false">https://azmarie.github.io/2018/react-rally/</guid><pubDate>Fri, 17 Aug 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Building the World's Largest Studio at Netflix]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2018/rubyconf/</link><guid isPermaLink="false">https://azmarie.github.io/2018/rubyconf/</guid><pubDate>Thu, 08 Mar 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Monoliths to Services with Elixir & Phoenix]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2017/code-europe/</link><guid isPermaLink="false">https://azmarie.github.io/2017/code-europe/</guid><pubDate>Thu, 25 May 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Confessions of an Ember Addon Author]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2017/emberconf/</link><guid isPermaLink="false">https://azmarie.github.io/2017/emberconf/</guid><pubDate>Wed, 29 Mar 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[From Front End to Full Stack]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2017/erlang-and-elixir-factory/</link><guid isPermaLink="false">https://azmarie.github.io/2017/erlang-and-elixir-factory/</guid><pubDate>Thu, 23 Mar 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[From Front End to Full Stack]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2016/elixirconf/</link><guid isPermaLink="false">https://azmarie.github.io/2016/elixirconf/</guid><pubDate>Fri, 02 Sep 2016 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[I Can Write My App with no Handlebars]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2016/embercamp/</link><guid isPermaLink="false">https://azmarie.github.io/2016/embercamp/</guid><pubDate>Tue, 12 Jul 2016 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Idiomatic Ember: Finding the Sweet Spot of Performance & Productivity]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2016/emberconf/</link><guid isPermaLink="false">https://azmarie.github.io/2016/emberconf/</guid><pubDate>Wed, 30 Mar 2016 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Declarative Templating in Ember.js]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2016/oredev-1/</link><guid isPermaLink="false">https://azmarie.github.io/2016/oredev-1/</guid><pubDate>Fri, 04 Mar 2016 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Idiomatic Ember: Finding the Sweet Spot of Performance & Productivity]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2016/oredev-2/</link><guid isPermaLink="false">https://azmarie.github.io/2016/oredev-2/</guid><pubDate>Thu, 03 Mar 2016 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Ambitious UX for Ambitious Apps]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2015/emberconf/</link><guid isPermaLink="false">https://azmarie.github.io/2015/emberconf/</guid><pubDate>Tue, 03 Mar 2015 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item></channel></rss>