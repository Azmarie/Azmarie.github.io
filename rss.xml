<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Azmarie Wang]]></title><description><![CDATA[Azmarie Wang's personal blog]]></description><link>https://azmarie.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Tue, 19 Jan 2021 23:07:45 GMT</lastBuildDate><item><title><![CDATA[Geometry-Aware Style Transfer: Implementation and Analysis]]></title><description><![CDATA[üìñ For report and analysis, check out my article on Medium üåü Want to play with it yourself? Check out my github repo for implementation and‚Ä¶]]></description><link>https://azmarie.github.io/2020-12-27-geometry-aware-style-transfer/</link><guid isPermaLink="false">https://azmarie.github.io/2020-12-27-geometry-aware-style-transfer/</guid><pubDate>Sun, 27 Dec 2020 20:29:57 GMT</pubDate><content:encoded>&lt;!-- 
**tl;dr** This is to document my proje

--- --&gt;
&lt;p&gt;üìñ For report and analysis, check out &lt;a href=&quot;https://azmariewang.medium.com/geometry-aware-style-transfer-implementation-and-analysis-3a9034dfca2d&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;my article on Medium&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üåü Want to play with it yourself? Check out my &lt;a href=&quot;https://github.com/Azmarie/Caricature-Your-Face&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;github repo&lt;/a&gt; for implementation and demo.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Acoustic Voxels: Computational Optimization of Modular Acoustic Filters]]></title><description><![CDATA[Main Contribution The research problem in this paper is computational design and fabrication in acoustic filters. Acoustic filters have many‚Ä¶]]></description><link>https://azmarie.github.io/Fabrication2/</link><guid isPermaLink="false">https://azmarie.github.io/Fabrication2/</guid><pubDate>Sun, 29 Nov 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is computational design and fabrication in acoustic filters. Acoustic filters have many industrial and medical applications, but they are difficult to customize with desired properties, requiring heavy trial-and-error iterations. Driven by the effectiveness and efficiency of additive manufacturing in manufacturing complex shapes, the authors are motivated to utilize computational methods to simulate and optimize the shape of the cavity for the acoustic filtering effects, which are to be physically realized via additive manufacturing. This paper proposed Acoustic Voxels, as a computational method to assemble shape primitives into a complex geometry that produces the desired acoustic filtering. &lt;/p&gt;
&lt;p&gt;The main contributions are that the proposed method largely automates the design of acoustic filters with custom properties; the range of acoustic filters is expanded and it opens up explorations into new possibilities such as mufflers design, wind instrument prototyping, and audiovisual UI via acoustic signatures.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The authors first introduce background theory for acoustic filters, including how to evaluate acoustic filters - input impedance and transmission loss. However, optimizing a complex structure targeting either quantity is computationally expensive, thus in this project, the authors leverage the concept of the transmission matrix, which is widely used in industrial muffler design. &lt;/p&gt;
&lt;p&gt;To use this technique in a filter structure, the authors assumed that acoustic pressure and velocity are both distributed uniformly over the cross-section, their relationship can be approximated linearly. Then, the authors go into details of the method design to construct the internal structure of an acoustic filter. &lt;/p&gt;
&lt;p&gt;The method has roughly 3 steps. Firstly is to utilize a modular primitive resonator, which is a hollow cube with extruded cylinders on its six faces, where all the cylindrical extrusions have the same radius and length. This is advantageous because the primitive is small enough to fill any shape, flexible for sampling, and it leads to efficient computation of the transmission matrix. The second step is to assemble the primitives by connecting an inlet and an outlet to form a complex structure. The third step features an efficient combinatorial and continuous optimization algorithm of the assembly connectivity and individual primitive parameters. Random samples for the connectivity of the resonators are evaluated while continuously changing the size of each hollow cube. The authors then discussed 3 key applications of the system. This includes muffler design at a finer granularity thanks to the system‚Äôs ability to construct complex structures, for example, an engine noise muffler demonstrated in the supplementary video. Then, to present the system‚Äôs capability for prototyping an acoustic resonator which serves as the key part of wind instruments. &lt;/p&gt;
&lt;p&gt;The authors present ways to design trumpets with customized sets of notes and shapes by maximizing the impedance values at the frequencies of those notes. The last application lies in acoustic signatures, such as acoustic tagging and encoding, where objects and devices (users) could interact through acoustics. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;This paper spent more paragraphs explaining the key applications made possible by this novel system. I find it helpful in assisting people who don‚Äôt have much prior knowledge about sound simulation and acoustic filters (such as myself) to appreciate the value the proposed system brings forward. &lt;/p&gt;
&lt;p&gt;In the &lt;a href=&quot;https://www.youtube.com/watch?v=gyMta5Eyo0A&amp;#x26;ab_channel=DisneyResearchHub&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;supplementary video&lt;/a&gt;, the authors demonstrated how each application would work with a customized setting to achieve the desired acoustic properties. I especially find the last application particularly interesting in expanding new possibilities of audiovisual HCI design. &lt;/p&gt;
&lt;p&gt;Considering acoustic encoding, the nature of encoding makes it a &lt;b&gt;more secure and private way to access and store information&lt;/b&gt;. This reminds me of the &lt;b&gt;privacy-preserving&lt;/b&gt; discussions in NLP - with the emergence of AI assistants (such as Alexa) and wearable technologies (smart watch etc.), the users could be vulnerable to data breaches and privacy infringements. I wonder if acoustic encoding could be used in NLP applications to preserve users‚Äô privacy as I assume speech recognition is broken down into acoustic in the hardware. This may make an interesting future work branching out of the topic of this paper as well as research around NLP and personal assistance technologies. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Guided Exploration of Physically Valid Shapes for Furniture Design]]></title><description><![CDATA[Main Contribution The research problem in this paper is computational design in combination with the physical validity of shapes. These two‚Ä¶]]></description><link>https://azmarie.github.io/Fabrication1/</link><guid isPermaLink="false">https://azmarie.github.io/Fabrication1/</guid><pubDate>Sun, 29 Nov 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is computational design in combination with the physical validity of shapes. These two research fields are often studied separately, and in furniture design practice, the workflow is often iterative, going back and forth between the designed and the physical simulator as a validator. The authors recognize that this is not ideal, and are motivated to streamline this process with an intuitive real-time exploratory modeling system. &lt;/p&gt;
&lt;p&gt;The main contribution includes the proposal of an interactive modeling framework to design shapes under geometric and physical constraints; while the users focus on creative modelling, the physical realizability is achieved by providing active suggestions to the user from novel force-space analysis.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The authors first introduced the modeling interface, consisting of a modeling panel and a suggestion panel. In the context of a nail-jointed furniture design system, the modelling user interface is designed for models consisting of multiple planks connected by nail joints. The system continuously checks for validity against its set of physical and geometric constraints. When the current shape is valid, the system will indicate the range of user manipulations to stay valid, and if the current shape becomes invalid due to connectivity, durability, or stability issues, red warnings will be visualized in the panel and suggestions will be provided in the suggestion panel to resolve the invalid state. &lt;/p&gt;
&lt;p&gt;In the proposed algorithm, the model first checks for the geometric constraints (connectivity) and physical constraints (durability and stability), and then performs analysis to compute valid ranges and make suggestions. In later sections, the authors talked in detail about how the system handles physical validity and makes suggestions to resolve the invalidity based on the analysis. &lt;/p&gt;
&lt;p&gt;In establishing physical validity, the authors first propose measurement for durability with a focus on the joint and the contact forces, as they assume the joints are the weakest links in nail jointed structure. With the definition of joints and the contact force, durability is evaluated by solving constrained rigid body dynamics of the bending forces on the joint. Secondly, sensitivity analysis is used to investigate the impact of design changes on the physical validity of a shape, this is done by locally computing a linear approximation to study how forces in equilibrium change concerning changes to the current design. &lt;/p&gt;
&lt;p&gt;Here, the authors acknowledge that this is a challenging task for frictional contacts, thus adding to the static setting, the authors further assumed that all contact points are exactly on the ground and the contact states do not change during interactions, which is not the case in reality, but a trade-off the authors made to overcome this challenge of obtaining an accurate sensitivity analysis with frictional contacts. &lt;/p&gt;
&lt;p&gt;Lastly, the authors discussed how the system supports the guided exploration of the valid subspace of the configuration space by making continuous and discrete suggestions. The authors simplify the problem of determining boundaries in high-dimensional space to looking for a plane that is geometrically prescribed by the corresponding inequality in the force space, where the contact force and the bending force are the two axes. This is feasible because each plank has 8 degrees of design freedom, individual force space on the axis is bounded by the configuration space dimension (8 times the number of planks), thus the full design space is bounded within a limit. However this is far from the real-time rate in practice, thus in continuous shape suggestion, the system renders at most 3 degrees of freedom limiting more complicated designs as a trade-off for efficiency. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;The authors acknowledged several limitations of the system, resulting from the extreme simplification of real-world objects. This includes plank material to be perfectly rigid and unbreakable, which particularly impacts the soundness of furniture with little supporting structure. &lt;/p&gt;
&lt;p&gt;Additionally, furniture involving dynamic physics wasn‚Äôt explored in the project, as well as larger-scale objects such as an indoor scene design modelling a couple of furniture components together. These will make good future work directions. &lt;/p&gt;
&lt;p&gt;Moreover, I found the technical writing of this paper is organized based on the interface and key algorithms, including handling physical validity and exploring valid space. However, the geometric constraints are discussed in the section for valid space, and sensitivity analysis is discussed in the section for physical validity. &lt;/p&gt;
&lt;p&gt;I find it could be more straightforward to organize the article by first measuring and establishing the validity for both of the constraints, and then investigate system analysis for suggestions including sensitivity analysis and valid space exploration.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Polaris: A System for Query, Analysis and Visualization of Multi-dimensional Relational Databases ]]></title><description><![CDATA[Main Contribution The research problem in this paper is data visualization for exploring large multidimensional relational databases. With‚Ä¶]]></description><link>https://azmarie.github.io/Visualization2/</link><guid isPermaLink="false">https://azmarie.github.io/Visualization2/</guid><pubDate>Sun, 22 Nov 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is data visualization for exploring large multidimensional relational databases. With emerging commercial data warehousing and massive data collections, extracting meaning from data has become an unavoidable and challenging task for many fields including business intelligence. Driven by the unpredictable nature of exploratory analysis, the proposed tool must be able to switch visualizations rapidly in the process of hypothesis, experimentation, and discovery. &lt;/p&gt;
&lt;p&gt;This paper introduced Polaris, an interactive system for exploring and analyzing large multidimensional databases. Polaris introduces a unified way to specify visualization, including a UI for generating graphical display and relational queries from the visual specifications, enabling fast and intuitive visual feedback to users. &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;In the overview, the authors talked about the key characteristic to support the exploratory analysis of large multidimensional databases: data-dense displays, multiple display types, and exploratory interface. These are addressed in Polaris by providing an interface that progressively generates table-based displays consisting of rows, columns, and layers, as tables are multivariate, naturally comparative, and familiar to most users. &lt;/p&gt;
&lt;p&gt;In Polaris, the users can drag-and-drop fields from the schema onto shelves, where the configuration of analysis and visualization operations is referred to as a visual specification. The interface is then interpreted as visual specifications, including table configurations, graphic configurations for each pane (table entry), and configurations for the visual encodings. &lt;/p&gt;
&lt;p&gt;Firstly, table algebra is formally used for table configuration, which is generated when a drag-and-drop is done to place a field on the shelf. X,y, and z axes are specified using columns, rows, and layers. A valid expression is an ordered sequence of operators (concatenation, cross, nest), and operands (ordinal and quantitative fields of the database). Using multiple data sources is supported in a single visualization through layer mapping. Secondly, the next step is to specify the type of graphic in each pane. Here, the system categorized graphics into 3 families by the dependency relationship between the axis variables, including ordinal-ordinal graphics (e.g. table), ordinal-quantitative graphic (e.g. bar chart), and quantitative- quantitative graphic (e.g. map). &lt;/p&gt;
&lt;p&gt;Lastly, the visual or retinal properties of the selected mark from the last step also needed to be specified, where the system will generate an effective mapping from the domain of the field to the range of the visual property. Building on this interface foundation, the authors then discussed additional features enabling a highly interactive process of data transformation and queries. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;This project banks on the idea of &lt;b&gt;automation&lt;/b&gt; (compared to Pivot Table) of human tasks, where it visually expresses data by translating drag-and-drop user actions into logic and queries. Some improvements could be made to further automate the process of visual specification generation and maybe extending formalism to 3D. This paper introduced two case studies where Polaris systems could be used in the exploratory analysis of data visualization. &lt;/p&gt;
&lt;p&gt;It‚Äôs convincing that this system has a promising outlook in real-world scenarios, though it is lacking in user and performance evaluation. As the authors point out in the discussion, effectiveness is valued over efficiency in designing the exploratory interface, and it could take even ‚Äúseveral tens of seconds‚Äù for a query to happen. &lt;/p&gt;
&lt;p&gt;However, this trade-off could be better balanced, as the datasets are getting exponentially large and the expectation for software interactivity is increasing. Nevertheless, this is clearly a research project that leads to huge commercial success in data visualization and pushes the industry forward. Seeing the development of Tableau from 2003 to now, it‚Äôs quite amazing to see how a research project has turned into a $15.7 billion business, evaluated at the time of the Salesforce acquisition in 2019. &lt;/p&gt;
&lt;p&gt;On the side: This is a few years old, but still really insightful: &lt;a href=&quot;https://www.youtube.com/watch?v=_GpWXtAt55c&amp;#x26;ab_channel=SFUGraduate%26PostdoctoralStudies&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;People, Data, and Analysis&lt;/a&gt; by Pat Hanrahan (author of Polaris, and later on co-founder at Tableau). It‚Äôs not directly related to the paper, but he talked about some relevant grand ideas around data visualization. Also, this was from a public lectures series hosted by SFU!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Hierarchical Edge Bundles: Visualization of Adjacency Relations in Hierarchical Data]]></title><description><![CDATA[Main Contribution The research problem in this paper is data visualization at scale for compound graphs. For visualizing a hierarchical‚Ä¶]]></description><link>https://azmarie.github.io/Visualization1/</link><guid isPermaLink="false">https://azmarie.github.io/Visualization1/</guid><pubDate>Sun, 22 Nov 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is data visualization at scale for compound graphs. For visualizing a hierarchical organization of large data sets, prior works often use tree visualization methods, which easily lead to visual clutter when displaying adjacency relations. This work proposes a generic technique for the visualization of compound (di)graphs by bundling adjacent edges together. &lt;/p&gt;
&lt;p&gt;The main contributions include the introduction of hierarchical edge bundling to reduce visual clutter in complex networks and visualize implicit adjacency edges between parent nodes with varying bundling strength. Additionally, it can be utilized in conjunction with existing tree visualization techniques for better flexibility and straightforward integration.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The author aims to have this method compatible with tree visualization techniques, thus in the problem setup, he uses a tree visualization layout as a guide for bundling the adjacency edges. Using the concept of parametric curves from computer graphics, the two adjacent nodes and the tree path are used to locate the control polygon of a spline curve, which visualizes this adjacency relation. Bundling ambiguity can be reduced by controlling a parameter adjusting the amount of bundling by straightening the spline curve. &lt;/p&gt;
&lt;p&gt;Then, the author discussed design decisions around the spline model including determining the spline representation and the straightening procedure. The author chooses a piecewise cubic B-spline representation for better local control and a relatively low degree for feasible computation. Then, to straighten a spline curve, the author straightens the control points of a polygon and uses the new points to find a new control polygon to generate the spline curve. This is preferable to straightening each spline point due to fewer straightening operations. &lt;/p&gt;
&lt;p&gt;Another step towards a clean visualization is the rendering of curves of varying lengths. Since short curves tend to be occluded by the long curves, the author proposes to draw the short curves on top of the long curves and it can be further emphasized by alpha blending where the long curves are drawn in a more transparent color. Lastly, the direction of an edge is signified using an RGB interpolated color gradient which is commutative (identical results drawing from the opposite order). &lt;/p&gt;
&lt;p&gt;From the figures given in the paper, the alpha blending and color encoding are good approaches to highlight any sparse case that possibly contains key information but was previously buried in the majority of other cases.&lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;In the user feedback section, the author added valuable observation from a user perspective. Compared to non-bundled visualizations where the data hotspots are clustered, this approach shows how bundling could reduce visual clutter and occlusion of implicit information so that the actual connections among the complex network could stand out. However, &lt;b&gt;bundling, as a way of compression&lt;/b&gt;, can lead to lossy information and bring about ambiguity into the node structure. &lt;/p&gt;
&lt;p&gt;To balance this &lt;b&gt;trade-off between readability and the richness of information&lt;/b&gt;, we can collect several visualization states by manipulating the bundling strength, as visualization moves from a low-level view towards a high-level view where the bundling strength is close to 1. Besides, this (moving along the bundle strength axis) can facilitate the human perception for hierarchical understanding. &lt;/p&gt;
&lt;p&gt;Overall, this paper shows that the edge bundling technique is great for visualizing datasets consisting of hierarchical structures and adjacency relations. But it‚Äôs actually more than that - &lt;b&gt;edge bundling has proven to be a general approach to reduce visual clusters in node-link diagrams&lt;/b&gt;, it has been used in graph visualization [1] to bring down the overall edge crossings, and parallel coordinates for multidimensional data [2]. &lt;/p&gt;
&lt;h2 id=&quot;reference&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#reference&quot; aria-label=&quot;reference permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Holten, D., &amp;#x26; Van Wijk, J. J. (2009, June). Force‚Äêdirected edge bundling for graph visualization. In Computer graphics forum (Vol. 28, No. 3, pp. 983-990). Oxford, UK: Blackwell Publishing Ltd.&lt;/p&gt;
&lt;p&gt;[2] Zhou, H., Yuan, X., Qu, H., Cui, W., &amp;#x26; Chen, B. (2008, May). Visual clustering in parallel coordinates. In Computer Graphics Forum (Vol. 27, No. 3, pp. 1047-1054). Oxford, UK: Blackwell Publishing Ltd.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[PolyGen: An Autoregressive Generative Model of 3D Meshes]]></title><description><![CDATA[Main Contribution The research problem in this paper is 3D object generation, specifically working with polygon meshes.  This research‚Ä¶]]></description><link>https://azmarie.github.io/PolyGen/</link><guid isPermaLink="false">https://azmarie.github.io/PolyGen/</guid><pubDate>Sun, 08 Nov 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is 3D object generation, specifically working with polygon meshes.  This research problem presents in creating a virtual world for AR/VR applications, games, and other virtual AI environments where objects are made out of 3D meshes. Mesh is known to be difficult to work with due to its unordered elements and discrete face structures, thus it‚Äôs often converted in a post-processing step from other representations. Inspired by successful neural autoregressive models on complex and high-dimensional data, the authors propose to use a neural generative model working directly with 3D meshes. &lt;/p&gt;
&lt;p&gt;The main contributions of this paper include the proposal of PolyGen, as a generative model of 3D objects which models the mesh directly, predicting vertices and faces sequentially using a Transformer-based architecture.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;As we know, a mesh consists of vertices and faces. In this model, two parts are proposed to tackle them individually, one is an autoregressive vertex model unconditionally modelling mesh vertices, and the other is an autoregressive face model modelling the mesh faces conditioned on input vertices. The model is trained on an augmented ShapeNet dataset and the renders of the processed ShapeNet meshes are created using Blender. &lt;/p&gt;
&lt;p&gt;Before the first model, there is a preprocessing step to convert the mesh to n-gon mesh from triangulated meshes in ShapeNet. N-gon meshes are meshes with variable length polygons. This is done to simplify the modelling task by reducing the size of meshes and removing the triangulation variability (polygons can be triangulated in different ways). Here, the authors address a limitation of n-gon that it doesn‚Äôt uniquely define a 3D surface with n‚â•3, which is minor here because most of the n-gons are or close to planars. &lt;/p&gt;
&lt;p&gt;Then for the two parts of models trained individually - to model vertices and then model the faces given the vertices. Firstly, the vertex model takes in a sequence of vertices, which are treated as a long sequence sorted by z,y,x axis values followed by a stopping token. Vertices are quantized into discrete variables and then modelled with categorical distribution, similar to PixelCNN and WaveNet. &lt;/p&gt;
&lt;p&gt;The authors mentioned a trade-off between &lt;b&gt;mesh fidelity and mesh size with the choice of bin numbers&lt;/b&gt;, where a larger bin number is good for mesh compression but results in lossy quality. They find 8-bit quantization as a good balance. With the current sequence of vertex coordinates as context input, the autoregressive model outputs a predictive distribution for the next vertex coordinate, maximizing the log-probability of the sequence. Secondly, the face model takes in a sequence of mesh faces, ordered by their lowest vertex index, and second-lowest and so on. This is treated as a long sequence again with a new face token and stopping token. To condition on the input vertex set, the input is first embedded using an encoder to obtain contextual vertex embeddings, and then each of the input face sequences is embedded using the corresponding vertex embedding, and then Transformer decoder is used to output pointer at each step.&lt;/p&gt;
&lt;p&gt;The authors used the idea from the Pointer Network (replacing LSTM with Transformers) to compare the pointer vector with the input embeddings via a dot-product, then later normalized through softmax to obtain a distribution over the input set. The evaluation is conducted mainly through log-likelihood and other sanctity checks such as chamfer-distance and comparing generated sample distribution against real data. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;The authors demonstrated that mesh generation with an autoregressive model working directly on mesh representation is doable. The novelty of this model is that it can directly model human-crafted mesh data, rather than alternative 3D representations in prior works. However, on the other hand, the mesh generation is conditioned on object representations such as images and voxels, which could still be limiting. &lt;/p&gt;
&lt;p&gt;Another limitation lies in the dataset available. As the authors marked in the paper, training only on the &lt;b&gt;ShapeNet dataset&lt;/b&gt; will result in overfitting due to its relatively small size. Besides, the &lt;b&gt;Transformer model has high memory requirements&lt;/b&gt;, which limits the training dataset to be within 800 vertices or 2800 face indices (exceeding meshes are filtered out), thus this model may &lt;b&gt;not be suited for large and complex meshes&lt;/b&gt;. &lt;/p&gt;
&lt;p&gt;We have seen examples of taking a successful model from one domain and re-propose it to solve a problem from a different domain, for example, Mask-RCNN extends a successful network for object detection to instance segmentation, and then Mesh-RCNN into computer graphics. Once again, in this paper, &lt;b&gt;PolyGen makes use of Transformer for its success in NLP tasks and the capability to model complex data and turns into a high-performing 3D generation model&lt;/b&gt;.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Deep Convolutional Priors for Indoor Scene Synthesis]]></title><description><![CDATA[Main Contribution The research problem in this paper is room-scale indoor scene synthesis. Generating virtual indoor environments and‚Ä¶]]></description><link>https://azmarie.github.io/DeepConvPrior/</link><guid isPermaLink="false">https://azmarie.github.io/DeepConvPrior/</guid><pubDate>Sun, 08 Nov 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is room-scale indoor scene synthesis. Generating virtual indoor environments and furniture layout is a practical problem in many applications, such as gaming, AR/VR, furniture retail, and robotics navigation. Inspired by CNN‚Äôs ability to learn to recognize and develop internal representations of 2D images and the recent availability of large-scale 3D scene dataset, the authors propose to train a CNN-based model for scene synthesis problems. &lt;/p&gt;
&lt;p&gt;The main contributions of this paper include the proposal of the first CNN-based system for synthesizing indoor scenes taking only the geometry of the room as input, and iteratively generates the room by adding one object at a time. Besides, this system was made possible with the proposal of a novel orthographic top-down view of scenes that are semantically meaningful and 2D-based.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;Before getting into the details of the generative model, the authors introduce the top-down view representation as a key piece in this approach. The rule of thumb of indoor scene synthesis is that a 3D indoor scene can often be characterized by the 2D object layout as floor plans. Taking this idea further, the authors convert the 3D scene to image-based representation compatible with deep convolutional networks. This consists of an orthographic top-down depth render of the room and semantic features as additional channels including several masks, orientation, and category information. The conversion is done on the selected SUNCG dataset. Now, the top-down views can be learnt and properly analyzed in CNN networks. &lt;/p&gt;
&lt;p&gt;The generative model generates a scene iteratively by placing one object at a time; each of the iterations consists of 3 steps: deciding if adding an object, deciding the category and location of the object, and placing an object instance in the scene. In the first step, a multilayer feed-forward neural network is used to process the current counts of objects and the high-level features from the top-down view representation extracted by CNN. &lt;/p&gt;
&lt;p&gt;As the authors point out, the first two steps are strongly correlated, thus they learn a conditional distribution for the second step. This is calculated with an additional attention mask channel to obtain the softmax distribution of an object category at a location through a second CNN-based model. Intuitively, a finished room layout should have sufficient unoccupied space permitting movement, the authors hence augment the set of object categories with several auxiliary categories which make up the majority of the training sets in this step. &lt;/p&gt;
&lt;p&gt;Lastly, with the location and object category, the model will then generate a 3D model and place it in the room with an orientation. The instance orientation is obtained from another CNN model taking the top-down view and a mask for the geometry of the instance as inputs. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;In the training of the above three steps, the training dataset is filtered from SUNCG datasets and are manipulated in a way that makes sense intuitively, for example, an even split of positive and negative examples for step 1 and 3, and use 95% of auxiliary categories in training examples for step 2. &lt;/p&gt;
&lt;p&gt;However, I think the breakdown of &lt;b&gt;component-wise training introduces more uncertainty and time complexity compared to an end-to-end trainable network&lt;/b&gt;, and it could be difficult to optimize for each component. Since this model is interactive with local scene plausibility, the inference time could be relatively long (4 minutes as noted in the paper). &lt;/p&gt;
&lt;p&gt;This limitation has been addressed in the follow-up paper [1] which factorizes the step of &lt;b&gt;adding each object into a different sequence of decisions for global reasoning&lt;/b&gt;. This also solves some failure cases where multiple nightstands are clustered around the bed, as the bed is a clue for nightstands. Noteworthy that this paper has a great detailed section for acknowledging limitations and mentioning possible extensions, including problem domain and speed limitation. &lt;/p&gt;
&lt;p&gt;Overall, I think the &lt;b&gt;top-down view representation is promising&lt;/b&gt; in studying indoor room synthesis because it reaps benefits from deep convolutional networks and its powerful image reasoning capabilities.&lt;/p&gt;
&lt;h2 id=&quot;reference&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#reference&quot; aria-label=&quot;reference permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Ritchie, D., Wang, K., &amp;#x26; Lin, Y. A. (2019). Fast and flexible indoor scene synthesis via deep convolutional generative models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 6182-6190).&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments]]></title><description><![CDATA[Main Contribution The research problem in this paper is embodied AI, specifically the task of Vision-and-Language Navigation (VLN). This is‚Ä¶]]></description><link>https://azmarie.github.io/Vision-and-Language Navigation/</link><guid isPermaLink="false">https://azmarie.github.io/Vision-and-Language Navigation/</guid><pubDate>Sun, 01 Nov 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is embodied AI, specifically the task of Vision-and-Language Navigation (VLN). This is a practical problem in robotics, where language-empowered intelligent agents adapt to the physical environment. Despite the recent successes in vision and language tasks individually, this combination has not been systematically studied due to the challenge of linking both tasks in an unstructured and unseen environment. &lt;/p&gt;
&lt;p&gt;This work pioneered the research of visually-grounded natural language navigation and inspired more recent work to push the boundary forward. The main contributions of this paper include the proposal of Matterport 3D Simulator as a large-scale interactive reinforcement learning environment, Room-to-Room (R2R) as the state-of-art benchmark dataset, and an attention-based sequence-to-sequence model designed to introduce a baseline for the VLN task. &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;In this work, the authors first introduced a novel Matterport3D Simulator and Room-to-Room task/dataset, and then further investigated the difficulty of this task by proposing several plausible models using this dataset. &lt;/p&gt;
&lt;p&gt;Firstly for Matterport3D Simulator, 10,800 densely-sampled panoramic RGBD images of real environments are sampled. The key point is the real-world images, other than the readily available synthesized datasets because no synthesized datasets can level the real image for its rich visual context. Then, based on this simulator, the R2R dataset is prepared to support the R2R task, where an embodied agent intake language instructions to navigate from a starting pose to a goal location.  &lt;/p&gt;
&lt;p&gt;In the simulator, an embodied agent taking advantage of the panoramic views to virtually ‚Äúmove‚Äù throughout the scene, thus R2R addressed the fact that the agent can move and control the camera in comparison to previous benchmarks. In obtaining R2R, the top 3 navigation instructions were collected using Amazon Mechanical Turk in a time-consuming process. The average length of instructions is 29 words (much longer than VQA), and the average trajectory length ~10m. Lastly, a sequence-to-sequence model was proposed, similar to models for VQA, but used ResNet-152, LSTM, and a bottom-up attention mechanism. The LSTM encoder encodes the language tokens, and the LSTM decoder decodes a sequence of actions to take in the environment while keeps track of the agent‚Äôs traversing history. At every timestamp, the model receives a new visual observation. &lt;/p&gt;
&lt;p&gt;In training, the model is to predict the action the shortest path would take from the current state. Besides, the authors experimented with ‚Äúteacher-forcing‚Äù, where the target word is passed as the next input to the decoder, and ‚Äústudent-forcing‚Äù, where the next action is sampled from the previous output probability distribution. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;Overall, I think the author did a great job presenting the VLN task and contributed greatly to lay the groundwork for future research, including the proposal of a novel 3D simulator, benchmark dataset, and baseline models. &lt;/p&gt;
&lt;p&gt;One limitation the paper mentioned is from its &lt;b&gt;choice of dataset Matterport3D dataset&lt;/b&gt; as it comprises &lt;b&gt;clean and tidy scenes of luxurious interiors with hardly any moving objects&lt;/b&gt;, such as human or animals.&lt;/p&gt;
&lt;p&gt;The simulator could be extended to incorporate depth information so that the agent can learn a semantic depth map of the environment. Though, It‚Äôs still very commendable because it‚Äôs real-world imagery with rich visual context, important in preventing overfitting. Another implicit limitation is &lt;b&gt;the language model currently only supports English instructions&lt;/b&gt;, which is inconvenient for non-English speakers. I expect future works incorporating more powerful language models into VLN task to expand on this. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;I think future works could be done on extending the model action space from the 6 actions for 30 degrees each (left, right, up, down, forward and stop) to a panoramic viewpoint (360 degrees) for better parametrization and a complete viewpoint. Overall, the combination of vision-language and the interaction in a dynamic environment is highly practical and has an exciting outlook from my point of view. &lt;/p&gt;
&lt;p&gt;From my research, this paper is the foundational work for VLN which inspired a stream of research addressing this task, in the efforts to better combat the ambiguity of the language instruction and the partial observability of the agent. This includes combining imitation learning and reinforcement learning [1] and exploring additional tasks as self-supervised signals with self-monitoring agents [2]. &lt;/p&gt;
&lt;h2 id=&quot;reference&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#reference&quot; aria-label=&quot;reference permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Wang, X., Huang, Q., Celikyilmaz, A., Gao, J., Shen, D., Wang, Y. F., ‚Ä¶ &amp;#x26; Zhang, L. (2019). Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 6629-6638).&lt;/p&gt;
&lt;p&gt;[2] Ma, C. Y., Lu, J., Wu, Z., AlRegib, G., Kira, Z., Socher, R., &amp;#x26; Xiong, C. (2019). Self-monitoring navigation agent via auxiliary progress estimation. arXiv preprint arXiv:1901.03035.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[CARLA: An Open Urban Driving Simulator]]></title><description><![CDATA[Main Contribution The research problem in this paper is simulation and sensorimotor control, specifically for autonomous driving. To break‚Ä¶]]></description><link>https://azmarie.github.io/CARLA/</link><guid isPermaLink="false">https://azmarie.github.io/CARLA/</guid><pubDate>Sun, 01 Nov 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is simulation and sensorimotor control, specifically for autonomous driving. To break the physical constraint for further advances in autonomous driving research and development, the authors proposed a digitalized simulation platform for autonomous driving. &lt;/p&gt;
&lt;p&gt;The main contributions of this paper include the introduction of CARLA as an open simulator for urban driving, which supports flexible configurations. Moreover, the authors also showcased CARLA‚Äôs ability to train models and validate the performance of different approaches for autonomous driving.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;In this paper, the authors first introduced the simulation engine and then utilizing CARLA for analysis of autonomous driving systems with three pipelines. &lt;/p&gt;
&lt;p&gt;First, the architecture of CARLA consists of a client-side Python client API and a server-side rendering from Unreal Engine. CARLA allows for a configurable sensor suite, including depth map and semantic segmentation map. The digital assets are custom made, and it supports diverse weather and lighting, making the rendering realistic for autonomous driving research. To show CARLA‚Äôs feasibility and usability in the research environment, the authors evaluate three popular approaches for studying autonomous driving, including modular pipeline, deep imitation learning, and deep reinforcement learning. A modular pipeline decomposes this task to modular components as perception, local planner, and PID controller. The perception component is fulfilled by semantic segmentation network and classification to tell if it is at an intersection or not. The local planner is a custom state machine of 5 discrete states to avoid collisions and coordinates low-level navigation. &lt;/p&gt;
&lt;p&gt;Secondly, for the end-to-end conditional imitation training, training data (observations and actions) is recorded from human drivers who also provide the corresponding commands. The goal is for the model to learn to predict the expert‚Äôs actions with a deep convolutional network, however, the drawback is that there‚Äôs no way to control the system, for example turning at an intersection. Thirdly, the authors explored end-to-end reinforcement learning, which takes 12 days to train the model while imitation learning takes roughly 12 hours. &lt;/p&gt;
&lt;p&gt;Reinforcement learning doesn‚Äôt need human driving, instead, it learns from the interaction with the environment by maximizing the sum of future rewards. Imitation learning maps sensory input to driving commands, while reinforcement learning maps the sensory input to actions with a deep network. &lt;/p&gt;
&lt;p&gt;Overall, CARLA targets urban driving scenes specifically and supports end-to-end learning for future research.&lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;CARLA has become a popular choice of simulator for a dynamic urban environment with traffic for its physics integration and visual realism. Due to its open-source nature, it has &lt;b&gt;wide-reaching community support&lt;/b&gt;, and it is actively adding new features to its ecosystem on the Github repo. Despite being a virtual simulation, it made possible the CARLA Autonomous Driving Challenge and lowered the entry barriers to conduct research projects for autonomous driving. &lt;/p&gt;
&lt;p&gt;In one of my previous projects, I worked on a draining problem for autonomous driving to remove the rain-streak and the fogginess of rainy imagery, so that the system can ‚Äúsee‚Äù clearly. Due to the lack of real-world paired images (same road condition with only the weather as a variable), we experimented with CARLA where we drove the same road in rainy and sunny weather with varying light conditions. In the end, we proposed a CycleGan-like network so that it can be trained unsurprised with real-world images, but I find CARLA very useful in the proof of concept for its precise variable controlling and also convenient to install and set up for research. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[As-Rigid-As-Possible Shape Manipulation]]></title><description><![CDATA[Main Contribution The research problem in this paper is interactive image editing and shape deformation. This research problem is important‚Ä¶]]></description><link>https://azmarie.github.io/As-Rigid-As-Possible/</link><guid isPermaLink="false">https://azmarie.github.io/As-Rigid-As-Possible/</guid><pubDate>Sun, 25 Oct 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is interactive image editing and shape deformation. This research problem is important in animation and image editing GUIs where real-time interaction is desired. The main contributions include the proposal of an interactive system allowing users to manipulate a shape without a predefined structure which is compatible with multiple-point input devices. &lt;/p&gt;
&lt;p&gt;As-rigid-as-possible refers to the modelling of the internal resistance to deformation of a shape, it is achieved in this work by minimizing the geometric distortion from each triangle in the mesh. Besides, the two-step closed-form algorithm is the key to achieve real-time interaction, as it uses quadratic error metrics to establish a system of simultaneous linear equations. &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The overall system proposed has several components, including firstly, triangulation to generate a mesh inside the shape (referred to as rest shape in the paper) and pre-computed registration to accelerate computation, secondly compilation when adding or removing new handles, and finally, manipulation when the handles are moved. One limitation of this system is that, in the compilation step, users can only place handles at existing mesh vertices which limits the flexibility of this model. For each interaction, this system updates the handle configuration by solving the quadratic error functions to minimize global distortion. Thus, the key problem is de-scoped to designing a quadratic error metric that represents the overall distortion. The input is the XY-coordinates of the given handles, and the output is the XY-coordinates of the free vertices. &lt;/p&gt;
&lt;p&gt;The authors propose to break this down into two steps, a rotation part and a scale part. Each part is tackled by a quadratic error function individually and these two least-score problems can be run sequentially. In the first step of scale-free construction, the goal is to mimic the orientation and translation distortion while retaining the original scale. This step is based on the 2D case in Laplacian editing where a triangle takes the place of a vertex. After summing all the errors for each triangle, the minimizing problem becomes finding the partial derivatives of the error function. Noteworthy here that most of the terms in the matrix manipulation can be pre-computed during registration and compilation, the makes this step as fast as one matrix multiplication per interaction. &lt;/p&gt;
&lt;p&gt;To complement the scale-independent distortion, the second step focuses on scale adjustment. Given the intermediate result from the first step and the original triangle in the rest shape, the authors consolidate the fitted triangle congruent to the original triangle minimizing the error. This is first approximated by uniform scaling and then adjusted by a scaling factor. Then, a final quadratic edge error function is used to minimize the distance between the fitted triangle with the corresponding original triangles for each edge. Putting things together, the authors triangulate the input image and produce deformations that are globally smooth by solving a linear system of equations. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;Overall, the paper proposes an interactive system that allows the user to deform a 2D shape by manipulating a few points as the constrained handles. This problem is made linear by a two-step closed-form algorithm for rotation and scale. One interesting property of this work is the integration of multiple-point input devices for easy and direct manipulation, such as a touchpad. This will make the tool &lt;b&gt;accessible for beginners and non-artists&lt;/b&gt;, which possibly lowers the barrier to entry for animation. &lt;/p&gt;
&lt;p&gt;However, this wouldn‚Äôt handle the presence of &lt;b&gt;occlusions and topology variation&lt;/b&gt; due to the lack of 3D information in these 2D triangulations, as this work is intended not to use physical simulations. I think one possible future work is to obtain depth information of a layered representation of the image to extend this work to the 3rd dimension.&lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;At the end of the paper, the authors mentioned a meaningful future work would be extending this to 3D shapes, which is non-trivial because the energy (error) function cannot be linearly parameterized. This is actually solved in a 2007 paper by Sorkine et al [1] with an iterative approach that enables a 3D mesh deformation also using As-Rigid-As-Possible technique. Upon some research, I find As-Rigid-As-Possible as a popular technique in geometry processing to preserve the geometric details. A more recent 2017 paper [2] proposed to enhance the consistency of adjacent rigid transformations by applying larger local neighbourhoods, compared to the 1-ring neighbourhood from this paper. &lt;/p&gt;
&lt;p&gt;Intuitively, this can make the deformation more natural and more tolerant of edge cases. However, I don‚Äôt think it can substitute for the skeleton or dynamic motion data in animation frames as As-Rigid-As-Possible is still a static method independent from physics.&lt;/p&gt;
&lt;h2 id=&quot;reference&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#reference&quot; aria-label=&quot;reference permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Sorkine, O., &amp;#x26; Alexa, M. (2007, July). As-rigid-as-possible surface modeling. In Symposium on Geometry processing (Vol. 4, pp. 109-116).&lt;/p&gt;
&lt;p&gt;[2] Chen, S. Y., Gao, L., Lai, Y. K., &amp;#x26; Xia, S. (2017). Rigidity controllable as-rigid-as-possible shape deformation. Graphical Models, 91, 13-21.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills]]></title><description><![CDATA[Main Contribution The research problem in this paper is physics-based character animation. In animation, it‚Äôs important to generate‚Ä¶]]></description><link>https://azmarie.github.io/DeepMimic/</link><guid isPermaLink="false">https://azmarie.github.io/DeepMimic/</guid><pubDate>Sun, 25 Oct 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is physics-based character animation. In animation, it‚Äôs important to generate physically realistic modelling of motions in dynamic actions involving humans and animals. Inspired by the success of Reinforcement Learning (RL) in motion control, the authors aim to future enhance the quality of learned controllers by incorporating reference animation data. &lt;/p&gt;
&lt;p&gt;The main contributions include the proposal of a physics-based character animation system allowing goal-directed reinforcement learning with data. The reference is supplied in the form of motion capture clips or keyframes handcrafted by a technical artist, making this system versatile and practical. &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The proposed system takes a character model, a reference motion (i.e jump, jog, run), and a task (i.e. striking/throwing a target, running towards a direction) defined by the reward function as input. It then outputs a policy that enables the character to reproduce the given reference motions while satisfying the task objective. Compared to prior works, I think the concept of completing ‚Äúa task‚Äù and combining it with motion imitation is a novel technique from this work. DeepMimic has presented this technique to create a controller that drives natural and realistic animation characters using reinforcement learning. &lt;/p&gt;
&lt;p&gt;The reward contains two terms, the first term is from the imitation objective encouraging the character to mimic the reference motion; the second term is from the task objective encouraging the completion of the task. The policy is modelled with a neural network with two standard fully-connected layers, and additionally convolutional layers along with a fully-connected layer if the task requires a heightmap. In training, two key techniques are applied to tackle some extremely challenging motion scenarios, such as backflip, which are Reference State Initialization (RSI) and Early Termination (ET). &lt;/p&gt;
&lt;p&gt;In discussing RSI, the authors give a vivid example of performing a backflip, which in training is very receptive to the initial conditions at takeoff. Thus, the authors use RS to initialize the state sampled randomly from the reference motion so that the system can find the high rewards state gradually. In ET, when the character falls into a bad state without the hope of recovering, this training episode will be terminated early to avoid collecting void data, which mitigates the class imbalance problem in this setting. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;The paper discusses multi-clip integration for even more complex tasks, multiple policies are learnt by training multiple neural networks individually and then linearly blended at runtime. This seems a little counter-intuitive. I wonder if &lt;b&gt;jointly training a single policy for the composition of motion clips&lt;/b&gt; could be done instead such that the transition between motions is smoothly learnt. Besides, this work only supports a small number of clips as admitted in the paper, scaling up to a large number of clips could be a possible extension of this paper. &lt;/p&gt;
&lt;p&gt;In this work, the authors proposed a data-driven RL method utilizing reference motion data. As we can tell, the ability to combine motion imitation and task-related demands is a key point, where the task is encoded in the task objective in the reward function. I am curious to understand more about &lt;b&gt;how the authors specify the tasks at hand&lt;/b&gt;, such as by its complexity (simple enough) or commonality (common enough). For example, why we are interested in striking/throwing a target where you could control the characters to pick up, carry and collect the target. &lt;/p&gt;
&lt;p&gt;Another extension is to &lt;b&gt;incorporate pose estimation&lt;/b&gt; to be used in pre-processing to extract kinematic reference motion so that the model can read directly from video sequences in the wild. This can help eliminate the human-authored keyframes or kinematic motion reference. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Building Rome in a Day]]></title><description><![CDATA[Main Contribution The research problem in this paper is image matching and 3D reconstruction. The goal the authors set out was to‚Ä¶]]></description><link>https://azmarie.github.io/Reconstruction1/</link><guid isPermaLink="false">https://azmarie.github.io/Reconstruction1/</guid><pubDate>Sun, 18 Oct 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is image matching and 3D reconstruction. The goal the authors set out was to reconstruct the city of Rome in 3D from the online 2D photo collection in 24h, hence building Rome in a day. This was inspired by the massive image collections available on the internet, as it is an unprecedented opportunity for computer vision algorithms to take advantage of the rich user-generated content to study the hidden 3D information.&lt;/p&gt;
&lt;p&gt;The main contribution of this work is a novel and parallel distributed matching system that matches vast collections of 2D images and solves large non-linear least squares problems in the 3D reconstruction stage effectively and efficiently.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The pipeline the authors proposed has several distinct stages, including feature extraction (pre-processing), imaging matching, track generation, and geometric estimation. This method targets two major problems posed by the massive and unorganized collections of 2D images from different cameras and viewpoints - the correspondence problem and the structure from motion (SFM) problem. For each stage of the pipeline, the authors experiment with different algorithms to explore their performance and scalability. &lt;/p&gt;
&lt;p&gt;First, for the correspondence problem, the author proposed to use SIFT features with ANN, and clean up with RANSAC by imposing the rigid scene constraint. For large scale matching, this problem is better framed as a graph estimation problem - given vertices matches, we want to propose and then verify the set of edges connecting correspondence. This graph is called the match graph. In this stage, the proposals are generated by whole image similarity and query expansion. I find the idea of looking to find the similarity between images close to the concept of word embedding in natural language processing. Applying this idea in computer vision, the authors take the SIFT features and cluster them into ‚Äúvisual words‚Äù, and the images become documents that contain such visual words. With this vocabulary tree in the TF-IDF scheme, we have a sparsely connected match graph as the initial proposal, after which, query expansion is used to increase the density of the component connections for the graph. &lt;/p&gt;
&lt;p&gt;Later, in track generation, tracks can be viewed as the connected components in the match graph described above. The second problem to be solved is the SFM problem - given corresponding points, we look for the 3D coordinates of the points of interest, camera parameters, and focal lengths. &lt;/p&gt;
&lt;p&gt;In this paper, this is done by first constructing a skeletal set and then incrementally improving by bundle adjustment. This design decision is intuitively justified by a large amount of redundancy demonstrated in Internet collections. But it seems unclear &lt;b&gt;how many of the images can be bundled for such improvement&lt;/b&gt;. Lastly, the final reconstruction of the scene geometry within each cluster is done by a multiview stereo algorithm.&lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;Overall, I think the authors came up with a neat pipeline of a distributed implementation for this task. One concern I have regarding this pipeline is that it seems to be &lt;b&gt;relying heavily on the vectors embedding which are connected components&lt;/b&gt; in the match graph. I wonder if this means that the most prominent components will take charge of this process, and the reconstructed 3D model is performant in these areas only, where the less prominent part of the model will be neglected. This leads to my question about the metrics used in evaluating the accuracy of the results. Upon reading the paper, it isn‚Äôt clear to me &lt;b&gt;what metric they used to measure the quality&lt;/b&gt; of the results. It seems that because the goal of this paper is particularly specific and custom-defined (building a 3D model of a city from internet images under 24 hours), it is less obvious to compare this to previous work. However, I wonder if the authors have compared their work with a &lt;b&gt;ground truth or a highly accurate 3D model&lt;/b&gt; of the Colosseum, which would be a helpful addition to make this paper more sound and complete. &lt;/p&gt;
&lt;p&gt;Another limitation I found is the &lt;b&gt;lack of analysis of the problem space&lt;/b&gt;. The authors certainly addressed some problems in the downloaded images from the Internet such as diverse angles and different levels of zooming. I am curious to know &lt;b&gt;if the pipeline proposed can be used in a variety of scenes&lt;/b&gt;, if it performs on the complex scenes and simple scenes equally well, and the role of the skeletal sets in 3D construction. I think these are all interesting things to explore and to extend from this paper. &lt;/p&gt;
&lt;p&gt;I found the &lt;b&gt;critical skeletal sets idea similar to the critical point concept from the PointNet paper&lt;/b&gt;. On this note, I think it would be useful to have an algorithm that identifies the skeletal sets from an extensive unstructured image collection to capture the most significant information to be able to navigate through such an assortment while avoiding redundancy as much as possible. Such an algorithm could help determine the crucial keyframe from a collection, and can also point a direction to what is missing in the collection to &lt;b&gt;foster guided data image collection&lt;/b&gt;. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[KinectFusion: Real-Time Dense Surface Mapping and Tracking]]></title><description><![CDATA[Main Contribution The research problem in this paper is 3D surface reconstruction and interaction. Inspired by the wide availability of the‚Ä¶]]></description><link>https://azmarie.github.io/KinectFusion/</link><guid isPermaLink="false">https://azmarie.github.io/KinectFusion/</guid><pubDate>Sun, 18 Oct 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is 3D surface reconstruction and interaction. Inspired by the wide availability of the low-cost dense cameras, the authors aim to use only the data collected from a Kinect sensor to perform a global 3D dense reconstructions of the scene as they move through space. &lt;/p&gt;
&lt;p&gt;This research setup is meaningful because real-time tracking with infrastructure-free handheld device mimics scenarios in augmented reality (AR) applications, including 3D scanning and gaming. The main contributions include the proposal of KinectFusion, as the first system supporting the real-time 3D surface reconstruction of complex scenes using a depth sensor, and the tracking is always relative to the fully updated fused dense model.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The overall system proposed has several components, including surface measurement, pose estimation, update reconstruction, and surface prediction. In surface measurement, the live depth data is roughly converted to 3D points (referred to as point clouds in the paper) and normals in the camera space. Secondly, the authors use ICP alignment with every 3D points to align overlapping 3D point clouds. An important assumption here is that the points clouds between frames are roughly aligned due to small motion from one frame to the next. To calculate the offset transformation, an energy function is defined to minimize the global point-plane movement. This process is iterative until the points are aligned for each frame. &lt;/p&gt;
&lt;p&gt;Later, in surface reconstruction update, the surface is modelled implicitly by a truncated signed distance function (TSDF) representation. This prepares the continuous estimates as discretized values to be feed into the voxel grid. In this component, one implicit design tradeoff is the choice of using the voxel model as the representation. Compared to a polygon mesh, &lt;b&gt;voxel representation is efficient but it restricts the fixed geometry shape and limits model deformations&lt;/b&gt;. &lt;/p&gt;
&lt;p&gt;Lastly, the surface prediction is produced by raycasting the grid to extract the position of the implicit surface by following zero crossings (a change of sign in the TSDF values). Putting things together, this real-time reconstruction system utilizes highly parallel GPU implementation to make sure its real-time speed. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;I am intrigued by the application of this paper and by the idea of bringing the physical to the digital world. In the section about experiments, I think the authors did a good job in locating ways to optimize the frame-to-frame results from the baseline model, such as using partial/full loops to combat the accumulating errors between frames, increasing viewpoints, and identifying keyframes. While evaluating a surface reconstruction method, it‚Äôs important to &lt;b&gt;consider properties such as scale and speed in addition to quality&lt;/b&gt;. I found the &lt;b&gt;regular grid could be a bottleneck for the scalability&lt;/b&gt; of this model. The authors mentioned each frame from the depth sensor is a surface measurement on a regular grid, which I understand as a fixed resolution model. &lt;/p&gt;
&lt;p&gt;It‚Äôs intuitive to see how a regular grid could make computation easy to manage, but the massive information collected needs to be fit into this &lt;b&gt;fixed grid to be stored in the GPU memory&lt;/b&gt;. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;I think this leads to the limiting capability of this model that it only works properly indoors. If this were to be expanded to work outdoors, they might want to &lt;b&gt;incorporate more data streams&lt;/b&gt; or &lt;b&gt;multiple videos of the scene&lt;/b&gt; to calculate the depth as pre-processing information. From the memory space availability perspective, they could use &lt;b&gt;hierarchical techniques&lt;/b&gt; such as octrees. The hierarchical layout could stay shallow to balance the tradeoff between time and space while keeping the computation relatively simple.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[A Practical Model for Subsurface Light Transport]]></title><description><![CDATA[Main Contribution The research problem in this paper is image rendering, designed to accommodate for subsurface scattering in translucent‚Ä¶]]></description><link>https://azmarie.github.io/Rendering2/</link><guid isPermaLink="false">https://azmarie.github.io/Rendering2/</guid><pubDate>Sun, 11 Oct 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is image rendering, designed to accommodate for subsurface scattering in translucent materials such as liquids and human skin. Subsurface scattering refers to the phenomena where light enters the object, it scatters and propagates inside the material, and then either gets absorbed or leaves the object at a separate location of lower intensity. &lt;/p&gt;
&lt;p&gt;The main contribution includes a practical model of BSSRDF, which efficiently renders a realistic simulation of the object capturing the translucency effect, and its mathematical foundation. This research problem is important because most of the material is translucent to some degree in reality. This is especially useful for the development of realistic CGI and medical physics research.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;Compared to the simplified BRDF model where light enters and leaves at the same surface point, the authors utilize the BSSRDF model accommodating phenomena like subsurface scattering. BSSRDF stands for the bidirectional scattering-surface reflectance distribution function, which describes how light enters an object, scatters around it, and then leaves the surface at a different location. In technical terms, it illustrates the relationship between outgoing radiance and the incident flux in light propagation with reduced intensity. Building on the theory of BSSRDF, the authors constructed this model with two parts - the diffusion approximation term and the single scattering term. &lt;/p&gt;
&lt;p&gt;First, the diffusion approximation is based on the isotropic light distribution in highly scattering media, which in simple terms, means roughly the same light distribution measured in different directions. &lt;/p&gt;
&lt;p&gt;As mentioned earlier, the surface scattering transports light from one point to another. Intuitively, the diffusion term is approximated by the distance of these two points and the Fresnel terms of the incoming and the outgoing light vectors. The dipole is the method of estimating the reflectance from two point sources, which is often a solution in diffusion problems. Here, the authors constructed these two point sources on the opposite side of the surface, with positive and negative contributions individually. In this step, the authors &lt;b&gt;assume a locally flat and semi-infinite homogeneous media&lt;/b&gt;. However, in reality, this could hardly be the case. Hence, I think this could be one of the limitations of this method. &lt;/p&gt;
&lt;p&gt;Secondly, the single scattering term extends from a previous model for BRDF. The outgoing radiance after a single scattering is calculated by integrating the incident radiance along the refracted outgoing ray, which accounts for each reflection on the surface. Taking two parts together, the complete model is a sum of the diffusion approximation and the single scattering term.&lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;This model exceeded the state-of-art method by its great results and efficiency. The authors argued that this outperformed the BRDF models since it produces a soft-looking surface that is more realistic and closer to human eye perception for the translucent materials. It also exceeds the brute force path tracing method in speed and efficiency, though could be less performant than a BRDF due to its computational complexity. In my opinion, this paper is novel in proposing a practical model to formulate the problem of subsurface scattering, which paved the foundation for future work and spike in CGI in the movie industry. One limitation of the paper, as mentioned earlier, is the idealistic assumptions made about the scattering media, in reality, which may not be the case. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;In my research around this paper, I found an extension by the author in 2005 [4] on multipole diffusion approximation for better results. Some other extensions were various efforts made in handcrafting solutions for a specific texture such as hair shading [1], rainbows in nature[2], and fur[3]. The results from the above papers are stunning and many of them are mature enough to be applied in the industry. &lt;/p&gt;
&lt;p&gt;This, on the other hand, makes me think about the &lt;b&gt;trade-off between using a traditional handcrafted method like this one and a deep learning method in computer vision and graphic tasks&lt;/b&gt;. Many deep learning networks that we consider as standard these days, such as CNN or GAN, can often be constructed as a general-purpose or multi-tasking architecture, which achieve generally good results on images from different domains. &lt;/p&gt;
&lt;p&gt;However, the &lt;b&gt;generalizability of neural networks&lt;/b&gt; is not guaranteed on edge or real-world complex cases, for example, in the areas the extensions of this work have addressed - complex texture like fur, hair, rainbows. Another potential problem with neural networks in image rendering is the flexibility to deal with an artifact, as we couldn‚Äôt fiddle with the networks directly case by case.&lt;/p&gt;
&lt;h2 id=&quot;reference&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#reference&quot; aria-label=&quot;reference permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Sadeghi, I., Pritchett, H., Jensen, H. W., &amp;#x26; Tamstorf, R. (2010). An artist friendly hair shading system. ACM Transactions on Graphics (TOG), 29(4), 1-10.&lt;/p&gt;
&lt;p&gt;[2] Sadeghi, I., Munoz, A., Laven, P., Jarosz, W., Seron, F., Gutierrez, D., &amp;#x26; Jensen, H. W. (2012). Physically-based simulation of rainbows. ACM Transactions on Graphics (TOG), 31(1), 1-12.&lt;/p&gt;
&lt;p&gt;[3] Yan, L. Q., Sun, W., Jensen, H. W., &amp;#x26; Ramamoorthi, R. (2017). A BSSRDF model for efficient rendering of fur with global illumination. ACM Transactions on Graphics (TOG), 36(6), 1-13.&lt;/p&gt;
&lt;p&gt;[4] Donner, C., &amp;#x26; Jensen, H. W. (2005). Light diffusion in multi-layered translucent materials. ACM Transactions on Graphics (ToG), 24(3), 1032-1039.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Interactive Reconstruction of Monte Carlo Image Sequences using a Recurrent Denoising Autoencoder]]></title><description><![CDATA[Main Contribution The research problem in this paper is interactive image reconstruction/ rendering. This problem is relevant in the gaming‚Ä¶]]></description><link>https://azmarie.github.io/Rendering1/</link><guid isPermaLink="false">https://azmarie.github.io/Rendering1/</guid><pubDate>Sun, 11 Oct 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is interactive image reconstruction/ rendering. This problem is relevant in the gaming industry and especially in AR/ VR applications enabling real-time experiences. Motivated by the success of image restoration using neural networks in recent years, the authors aimed to target the ray tracing denoising problem as reconstructing image sequences with the help of deep neural networks. &lt;/p&gt;
&lt;p&gt;The main contributions include the proposal of using novel recurrent connections in deep autoencoder networks for temporal stability, which is end-to-end trainable thus can automatically learn relationships based on auxiliary per-pixel input channels. Noteworthy, the network achieved a good execution speed with high-quality results from extremely low sampling frames.   &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The proposed architecture is a recurrent autoencoder with skip connection every second layer, trained end to end in a supervised manner with paired data. In the paper, the authors refer to the autoencoder as denoising autoencoders. Autoencoders are known to be effective in learning to reconstruct the inputs from a minimal latent representation obtained in the bottleneck. The noise is often random and can be averaged out, so it is regarded as non-essential information and won‚Äôt be included in the latent representation. &lt;/p&gt;
&lt;p&gt;Moreover, I think the usage of RNN is also easily justifiable intuitively. The past frames can provide useful information in constructing the current frame through the feedback loop, thus the recurrent connections are a good structure for temporal coherence. The authors made it clear that they are focusing on solving the reconstruction task with an extremely low amount of sampling frames. Thus, using the recurrent structure can obtain illumination information from its sparse sampling point. The temporal features are learnt at multiple scales through recurrent blocks. The architecture is fully convolutional, so it is trainable in small fixed resolution and much faster than training on the full resolution (such as a CNN with skip connection network). &lt;/p&gt;
&lt;p&gt;Further, the network is end-to-end trainable and put auxiliary inputs automatically into use to disambiguate the colour data. The tradeoff of using an RNN is between the &lt;b&gt;temporal coherency and the expensive hierarchy of recurrent blocks&lt;/b&gt;. Since this paper focuses on rendering at an interactive rate with low sample counts, it is not production-quality ready and the performance suffers from low sampling. Besides, neural networks tend to produce what is seen frequently more prominently, thus it may produce poor quality for a complex scene and rare objects. The authors mentioned that they choose to not consider the depth of field and motion blur, which limited the application to gaming and other virtual experiences. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;In the experiments, the authors showcased examples where the inputs were highly noisy, and the network generated good quality outputs. However, it would be good to know &lt;b&gt;if the trained network can fit inputs with multiple intensity of noise&lt;/b&gt;, such as mildly noisy and extremely noisy. I think the authors have yet to explore the spatial property of the objects in the video, adding spatial information to be jointly optimized with the temporal information could be a possible extension while keeping the low sample count. &lt;/p&gt;
&lt;p&gt;Overall, this paper is a successful example of using deep learning techniques in image rendering. It seems that deep learning is not only great for image restoration tasks but also is readily employable for image sequences for light transport reconstruction. It set good examples for future deep learning based ray tracing denoisers, where different types of kernels are learnt to substitute for the hand crafted kernels. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Mesh R-CNN]]></title><description><![CDATA[Main Contribution Drawing inspiration from the state-of-art networks in 2D perception and 3D shape prediction, this paper is tackling the‚Ä¶]]></description><link>https://azmarie.github.io/Mesh-rcnn/</link><guid isPermaLink="false">https://azmarie.github.io/Mesh-rcnn/</guid><pubDate>Sun, 04 Oct 2020 04:15:24 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;Drawing inspiration from the state-of-art networks in 2D perception and 3D shape prediction, this paper is tackling the problem of 3D object reconstruction. Given a real-world input image, the network aims to output 2D instance segmentation results and 3D triangle mesh for each detected object. The main contribution of this paper includes the proposal of Mesh R-CNN, which extends Mask R-CNN with a novel mesh predictor branch designed for 3D shapes and achieves state-of-art results. &lt;/p&gt;
&lt;p&gt;Additionally, Mesh R-CNN is a pioneer in 3D shape prediction in real-world cluttered scenes with a variety of objects. This research problem is important for real-world applications such as autonomous driving, virtual reality, and other emerging domains. &lt;/p&gt;
&lt;p&gt;The main novel part of the method is to find the right way to predict meshes in the neural network, where they use an iterative mesh refinement with hybrid shape representation to overcome the fixed topology in previous mesh deformation works. &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;Mesh R-CNN expands the 2D instance segmentation system (Mask R-CNN) by adding a mesh prediction branch, which is composed of a hybrid approach of voxel prediction followed by mesh refinement. This two-step approach, according to the authors, helps predicting fine-grained 3D structures. Starting from a RGB input image, the network will first perform 2D instance segmentation, same as Mask R-CNN, to get features per region. Then it will predict a coarse 3D structure as a voxelized prediction for each ROI in the voxel branch, which is later cubified to mesh for the mesh branch, including vertex alignment, graph convolution and refinement stages. &lt;/p&gt;
&lt;p&gt;In vertex alignment, the vertices in the current mesh are projected onto the image plane, and then an aligned feature from the image plane is sampled. Graph convolution merges information along neighbouring vertices in the mesh structure. The mesh refinement stage predicts offsets for all the vertices in the mesh to update the positions. &lt;/p&gt;
&lt;p&gt;In defining the loss of mesh, they convert the mesh into point clouds by sampling the points from the surface and calculate the chamfer distance against the ground truth with regularization to enforce smoothness. Chamfer distance seems to be the L2 distance in 3D shapes, and it could be sensitive to outliers. One limitation of this paper I identified is &lt;b&gt;not experimenting with other metrics&lt;/b&gt;, such as F1 score, and/or justify why they have chosen chamfer distance. &lt;/p&gt;
&lt;p&gt;Finally, the voxel and mesh losses alongside the box and mask losses for 2D instance segmentation are used to train the system end-to-end. The results of Mesh R-CNN outperforms its prior work, and it has reasonable completion of the objects even if an object is occluded. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;One thing I found interesting is that compared to the high performing 2D network Mask R-CNN, this network is also trained using fully supervised learning with pairs of images and meshes, however with a much smaller dataset. The Pix3D dataset used for training and testing only contains 10,069 pairs of images and meshes, much smaller than a standard 2D training dataset, such as COCO. &lt;/p&gt;
&lt;p&gt;Imaginable, &lt;b&gt;a 3D dataset is a lot harder to collect and obtain&lt;/b&gt;, since it requires expertise to create and annotate the ground truth. I think this work has chosen an intelligent way to deal with this problem, which is ‚Äústanding on the shoulder of‚Äù high-performing 2D fully supervised network and ripe the benefits from the 2D supervision (using a pre-trained model for instance segmentation on COCO). &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;Relying on 3D supervision doesn‚Äôt seem to be the optimal solution, if &lt;b&gt;unsupervised or semi-supervised&lt;/b&gt; methods could be developed to relax this constraint. Since there are already 2D examples in unsupervised networks, such as GAN or conditional GAN, this seems like a feasible possible extension.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation]]></title><description><![CDATA[Main Contribution The research problem in this paper is 3D Classification and Segmentation. Among different 3D representations, the authors‚Ä¶]]></description><link>https://azmarie.github.io/Pointnet/</link><guid isPermaLink="false">https://azmarie.github.io/Pointnet/</guid><pubDate>Sun, 04 Oct 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is 3D Classification and Segmentation. Among different 3D representations, the authors choose to use point cloud for its proximity to raw sensor data and its canonical form. Previously, the point cloud was often converted to other representation before being processed in deep neural networks, which causes artifacts. To solve this problem, the authors designed an effective feature learning directly on the point clouds. The main contributions include the proposal of PointNet, which performs end-to-end learning in 3D and achieves good results in various 3D tasks.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The proposed method is motivated to overcome the challenges of using point clouds as input. &lt;/p&gt;
&lt;p&gt;Firstly, point clouds are orderless, thus the model consuming N points needs to be invariant to N! permutations. Secondly, a rigid transformation should not alter the output, thus the model needs to preserve invariance under transformation. Thirdly, there are interactions among local points among the set of point clouds, which the model needs to capture. To tackle the first challenge, the authors observe that a permutation invariant neural network corresponds to a symmetric function, because in a symmetric function, for any ordering of the arguments, the function value stays the same. &lt;/p&gt;
&lt;p&gt;In this paper, they construct a family of symmetric functions by neural networks by multi-layer perceptron (MLP) and max pooling. Regarding the second challenge, the authors aligned input point clouds a canonical space, which is achieved by applying an affine transformation to input point coordinates, predicted by a mini PointNet T-net. This can be similarly applied to embedding space alignment in feature space with regularization to avoid bad local minimum. So far the network for 3D classification can be constructed, but the last challenge is important for the 3D segmentation network where class scores for each point need to be predicted. The method used here is to concatenate local embeddings and global feature vectors so that per point feature is aware of both local and global information for segmentation. &lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;One limitation of the paper is, in tackling the third challenge, they use the &lt;b&gt;concatenation of local and global embedding.&lt;/b&gt; However, I think it seems extreme to put together embedding for one local point with the embedding of all points (global). Compared to 3D CNN where hierarchical feature learning is used, this method doesn‚Äôt consider multiple-level local context, and may not generalize well to a complex scene with multiple objects. &lt;/p&gt;
&lt;p&gt;Besides, in tackling the challenge of transformation invariance, they use affirm transformation. &lt;b&gt;Will that affect the coordinates in a way that‚Äôs spatially confusing to the network?&lt;/b&gt; For example, would turning a chair upside down with a rotation transformation be a valid transformation? I think the author could have elaborated on T-net a bit more and what are the valid transformation matrices used to tackle this challenge.&lt;/p&gt;
&lt;p&gt;This paper exceeded the prior work with its direct usage of the point cloud in deep learning networks and great results. PointNet is also robust to data corruption including missing data points or outliers thanks to the ‚Äúcritical points‚Äù which can sparsely summarize the shape. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;Overall I think this is a great paper as the beginning for 3D deep learning with point clouds. One extension could be to exploit the benefit of hierarchical feature learning with PointNet or using a structure such as a skip connection to resample multi-layer local context. In hindsight, PointNet++ has proposed hierarchical feature learning precisely to extract information from the local neighbourhood.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Mask R-CNN]]></title><description><![CDATA[The R-CNN Family 


 Problem Statement Input: Images with objects Output: Correct masks of all objects in the image while also precisely‚Ä¶]]></description><link>https://azmarie.github.io/Mask-rcnn/</link><guid isPermaLink="false">https://azmarie.github.io/Mask-rcnn/</guid><pubDate>Sun, 27 Sep 2020 04:15:24 GMT</pubDate><content:encoded>&lt;h2 id=&quot;the-r-cnn-family&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#the-r-cnn-family&quot; aria-label=&quot;the r cnn family permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The R-CNN Family&lt;/h2&gt;
&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/19af9/b-1.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 33.78378378378378%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAAA2ElEQVQoz61RXQ/CIAzk//83E33Q6NT5rRkMN7exCWM7RxUzNSY+2OQCXI+jLaxtW/yKpmk+ud7eBcMfw5myOEkQxTFUVRFhjIGQZ/COk0lKVTlc8hzTYE68tXeuKCsswhD74xG1tfcKN/sDJrMA5zSF1hp5UWCxWiNYhlhtd8SpssSJCwyGIzi91oYeTi4ZxtMZ6Y039OU6gUPZXc6yDEVnrJQiQ5/rh+0MtL6irmvKVY8OWX+g3phzDiklYmrPvszIa52BEIIQRRGtzpy9/9q3YT/xOH+LG+x9IrukLtoeAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/59245/b-1.webp 148w,
/static/07104a4d1a8b07998ab755f1e76c17c3/4a139/b-1.webp 295w,
/static/07104a4d1a8b07998ab755f1e76c17c3/3b0a6/b-1.webp 590w,
/static/07104a4d1a8b07998ab755f1e76c17c3/fe731/b-1.webp 885w,
/static/07104a4d1a8b07998ab755f1e76c17c3/1d4bd/b-1.webp 1180w,
/static/07104a4d1a8b07998ab755f1e76c17c3/e00f2/b-1.webp 1378w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/f227d/b-1.png 148w,
/static/07104a4d1a8b07998ab755f1e76c17c3/78893/b-1.png 295w,
/static/07104a4d1a8b07998ab755f1e76c17c3/05d41/b-1.png 590w,
/static/07104a4d1a8b07998ab755f1e76c17c3/06dbc/b-1.png 885w,
/static/07104a4d1a8b07998ab755f1e76c17c3/0bd9b/b-1.png 1180w,
/static/07104a4d1a8b07998ab755f1e76c17c3/19af9/b-1.png 1378w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/05d41/b-1.png&quot;
          alt=&quot;RCNN Family&quot;
          title=&quot;RCNN Family&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;RCNN Family&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/c91f3/b-2.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABfklEQVQoz4WSvUvDQBjG8x8J/gUO4qqboIOjIDjYxUEERwu6uNRRcHHTRegkFaSibbVEKoW0IVSr+f5o2jQflzzmTlNL1XrwcLkc/N7nufflBEEAz/Mol8tsr1WraLVa+GslSYJpiyOEgJAIvh/A7fdhmBY03UAQhqB3QRAwhek5iqIRNNNkES77ICSGYRjQVQWKLDNwPy2gaRp0XYdpmvA874fbSTg3/sMPCUqVJpqSwtxQDYdDBqKijuM4huM4sCzr1+dgQBJ/0q8bKlYOSsif3qS3Md5Tp3IqCuj1ehgMBgxIHVO5rsvA1D1NM3IYRjE7XN2pmF89w0VRTIFeGvczKgXatv31NATtdps1TpIkttPGiqLIinHfhgkqlVvUH3m8vgiQlQS2YyPwfeaMKos9bQK4KApwXxMxt7CP5bUcNrbWMTO7g8WlAiz7DaqqodPpsOjdbpc1jjqZbMioKQ9PzyicFJHbLeHw+Bx7+SNsbl+iVm/8O4vj45JBPwDWfaSP6lBdBAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/59245/b-2.webp 148w,
/static/0566b1aa58106dbfbcfe2da01063e141/4a139/b-2.webp 295w,
/static/0566b1aa58106dbfbcfe2da01063e141/3b0a6/b-2.webp 590w,
/static/0566b1aa58106dbfbcfe2da01063e141/fe731/b-2.webp 885w,
/static/0566b1aa58106dbfbcfe2da01063e141/1d4bd/b-2.webp 1180w,
/static/0566b1aa58106dbfbcfe2da01063e141/31901/b-2.webp 1398w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/f227d/b-2.png 148w,
/static/0566b1aa58106dbfbcfe2da01063e141/78893/b-2.png 295w,
/static/0566b1aa58106dbfbcfe2da01063e141/05d41/b-2.png 590w,
/static/0566b1aa58106dbfbcfe2da01063e141/06dbc/b-2.png 885w,
/static/0566b1aa58106dbfbcfe2da01063e141/0bd9b/b-2.png 1180w,
/static/0566b1aa58106dbfbcfe2da01063e141/c91f3/b-2.png 1398w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/05d41/b-2.png&quot;
          alt=&quot;RCNN&quot;
          title=&quot;RCNN&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;RCNN&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/ab1a16397e76571184562efc47123e50/782ef/b-3.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABaElEQVQoz4VSTUvDQBDN3/PqLxDEkyfP3jwJetCT9CCCHz14UChCe/ASEBIVIhFjqZVKKGnSxNi0SdMm6ea5u5iwthUfPDLZzL6ZNxlJ13WoqgrDMKBpGo3vafwKRVHgOA4Y8jznFGORxTmDxILZbIbJNMFgMEA8HiMcjTCkJCRfECwgvouxJCb1PkPIWgvPbQvTnwJjWkC8KJIQsnAmCdn8YTR9uB4TIRjRLqMoQhzH/HIh6vs+LxaGIeYhOf0+TNPkVk17gqv6B+RHB2Y/wnAYIAgCJElS2mLCnU4H3W6Xk83ZdV14nsfzpCzNkJMM770AGzsaVtZUbO49wTADTOKIWo5/dcAELcuCbdtckHWapikd0ZR/Ky2/9SIc3LSwe/mC/Vob57cGLUR4UpZlpeX/UApWGi2sH8rYOn7A6nYVRxc1NjE+Q0Zm56+1WfpTFL2JauMOZ9d1VE5O8eV7pcVlOze/fwW+AcSyq8fCttyWAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/ab1a16397e76571184562efc47123e50/59245/b-3.webp 148w,
/static/ab1a16397e76571184562efc47123e50/4a139/b-3.webp 295w,
/static/ab1a16397e76571184562efc47123e50/3b0a6/b-3.webp 590w,
/static/ab1a16397e76571184562efc47123e50/fe731/b-3.webp 885w,
/static/ab1a16397e76571184562efc47123e50/1d4bd/b-3.webp 1180w,
/static/ab1a16397e76571184562efc47123e50/ef412/b-3.webp 1402w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/ab1a16397e76571184562efc47123e50/f227d/b-3.png 148w,
/static/ab1a16397e76571184562efc47123e50/78893/b-3.png 295w,
/static/ab1a16397e76571184562efc47123e50/05d41/b-3.png 590w,
/static/ab1a16397e76571184562efc47123e50/06dbc/b-3.png 885w,
/static/ab1a16397e76571184562efc47123e50/0bd9b/b-3.png 1180w,
/static/ab1a16397e76571184562efc47123e50/782ef/b-3.png 1402w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/ab1a16397e76571184562efc47123e50/05d41/b-3.png&quot;
          alt=&quot;Fast RCNN&quot;
          title=&quot;Fast RCNN&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Fast RCNN&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/772653a28863423d165820e8de688991/b85b2/b-4.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABbklEQVQoz4VSTUvDQBDNv/NfKPgTBI96FVEv9iSFHgTvKqIgSgWDSmNAW81Xa+ohxlpt03wnm6TP7GokrYU+GHZ3Zvbtm9nhTNOEKIrMZFmGJEngeR7NZhOCIEBVVRQYj8eYBy5NU4RhCMdxYVkWfN/L9zYGw2G+OixWkBWExX7axwjL7JbjQ5ReIHdN+CFBEATMygpnqSz7uOngk/YFRR/SNKbQdV3EcQxaSQHP81jMtm0kSTKpkCQZBm4M24uR5M5rwQB/9wmjH2A0sn/b4CPLsj9CeqaE1MoPUVLuUR+gevqM84aCjaqMhcVbLK7d4+LhHSSOcjX+v9J6vR77LE3ToOs6W6Mo+lEYkRRufun1rY/KUQvrNQHbhyqqZy3mTxKCME+mZRcwDINNBCXqdDpQFIW1YaKH9dYHlndvsFJrYGnnEqtb+0hIzIjo64SQuaPDSi4SlK6B2vEVDk7q2Kzsod3WmJ/2btaIzDKKb/MsqHtpsB6TAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/772653a28863423d165820e8de688991/59245/b-4.webp 148w,
/static/772653a28863423d165820e8de688991/4a139/b-4.webp 295w,
/static/772653a28863423d165820e8de688991/3b0a6/b-4.webp 590w,
/static/772653a28863423d165820e8de688991/fe731/b-4.webp 885w,
/static/772653a28863423d165820e8de688991/1d4bd/b-4.webp 1180w,
/static/772653a28863423d165820e8de688991/ed4c1/b-4.webp 1368w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/772653a28863423d165820e8de688991/f227d/b-4.png 148w,
/static/772653a28863423d165820e8de688991/78893/b-4.png 295w,
/static/772653a28863423d165820e8de688991/05d41/b-4.png 590w,
/static/772653a28863423d165820e8de688991/06dbc/b-4.png 885w,
/static/772653a28863423d165820e8de688991/0bd9b/b-4.png 1180w,
/static/772653a28863423d165820e8de688991/b85b2/b-4.png 1368w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/772653a28863423d165820e8de688991/05d41/b-4.png&quot;
          alt=&quot;FastER RCNN&quot;
          title=&quot;FastER RCNN&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;FastER RCNN&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&quot;problem-statement&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#problem-statement&quot; aria-label=&quot;problem statement permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;: Images with objects&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;: Correct masks of all objects in the image while also precisely segmenting each instance.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Application&lt;/b&gt;: Autonomous driving, medical imaging, human pose estimation, etc.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Goal of this Mask R-CNN&lt;/b&gt;: To create a meta-algorithm to support future research of instance segmentation, that has good speed/accuracy and is intuitive/easy-to-use.&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is instance segmentation. Compared to object detection where a bounding box is determined per instance, instance segmentation predicts pixel-level segmentation for each instance, in addition to the object class. After the authors examine the prior work in object detection and semantic segmentation, they decide to take the best of both worlds and solve the instance segmentation problem by augmenting the Faster R-CNN model with a mask branch that is a small FCN. &lt;/p&gt;
&lt;p&gt;The main contribution of this paper is the proposal of Mask R-CNN, as a fast and easy-to-use network with great accuracy, for it to serve as a meta-algorithm and a solid foundation for future instance-level research. &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/5a42c88461a6855175ed5f830da32812/cb752/1.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABz0lEQVQoz22Sz07bQBDG8xA8Qt+CA4ee+gY9V6pQEe1jwKFHJC5InLlUlVraAyciRQERJQFCnKQhUkpxiP/Gf9axvevk68xaRIAYaWV7duY3n7/dyng8RrVaRb9n4LLdRq1Wg2EY6Ha7qNfr6HQ6aLVaaDabaNO+UgrL5VIvjqfvi8UCFbwSUkoURYFiUegihnAuz/NndY+gx9DAB8vCl+0P+PzpIzY23uLn8a8V1AoFXJHqxizLMJlMMOj10CXVvueVdTQso0FSD1Wo3AwMbL1fx9HOFt6sVbC//xX9/h8Mh0Oksxk820YqC1jTKW4pd9looHl+hinBOWIxh+P6SJIEOQNt9y++HWyi8WMXezvvcHJyCNuekRoTKRUFQQDTszHs9zC4vsbdaASTYAzgmKcpPN9HHMelwjxLcHP1G6PBKRoX36n4duVJrDJEMsWdP0U8F5DULOn32F/2q7QiRxCGEFqhLA9FJDkerNITFUZIfU8rME0TNnmsSKXvuvh3fw8Ri2eHUhCYlfEQpYoSmCRkvusgDAMIIcgXgRn7R8a7BIriCAF9O+Qn7zOIFb6M1bXhE2SvbGpwHEc/LVLmkzcM5BzDefGgKIo0+LV7+B+4L53rXG0qJgAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/5a42c88461a6855175ed5f830da32812/59245/1.webp 148w,
/static/5a42c88461a6855175ed5f830da32812/4a139/1.webp 295w,
/static/5a42c88461a6855175ed5f830da32812/3b0a6/1.webp 590w,
/static/5a42c88461a6855175ed5f830da32812/fe731/1.webp 885w,
/static/5a42c88461a6855175ed5f830da32812/1d4bd/1.webp 1180w,
/static/5a42c88461a6855175ed5f830da32812/39706/1.webp 1434w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/5a42c88461a6855175ed5f830da32812/f227d/1.png 148w,
/static/5a42c88461a6855175ed5f830da32812/78893/1.png 295w,
/static/5a42c88461a6855175ed5f830da32812/05d41/1.png 590w,
/static/5a42c88461a6855175ed5f830da32812/06dbc/1.png 885w,
/static/5a42c88461a6855175ed5f830da32812/0bd9b/1.png 1180w,
/static/5a42c88461a6855175ed5f830da32812/cb752/1.png 1434w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/5a42c88461a6855175ed5f830da32812/05d41/1.png&quot;
          alt=&quot;RoI Align&quot;
          title=&quot;RoI Align&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;RoI Align&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/0f7a111d3dff9960260c847b79234ff3/5771b/2.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABUUlEQVQoz11Ri27CMAzs//8YCIGEBIIx6GOUVnSFtaU0pfRxy3lL6GbJin2xL2fHWa1WmE6n2Gw2WK/XWCwWILbb7TCfz7FcLgVnPJlMsN1uMZvNcDgcQBuGAcYYO8/nE/f7HbfbDapWEhdFgaqqUNe1bej7Xpx513UWZ0wOkwshGx/1Q5MoISIxsTEhTxKOjfn1ehUR5s7p+05eoapS5X+ajJqyLJGmqTwwJifuuq7glrCqfsY9J2fEyQnMS1UIEZ2K9/s9LpcLmqaxhJwsCALZ/WeSvHbIV7IsQxzHOIURPN/Du/umGxqrwvd9UaKUso1U5Xm6Vn9OrmuG18ivZcuS2w7pZ4q2bS1GNVEUya4MIb3RNYH+daXX0ZtPMQUkYCPP/zukE+dp77SIVueZHrck4S/uGAVhGIpTyfhnE91A7Hg8yvjGuHdiH7rnK89t/TcRHrSU97Rh5QAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/0f7a111d3dff9960260c847b79234ff3/59245/2.webp 148w,
/static/0f7a111d3dff9960260c847b79234ff3/4a139/2.webp 295w,
/static/0f7a111d3dff9960260c847b79234ff3/3b0a6/2.webp 590w,
/static/0f7a111d3dff9960260c847b79234ff3/fe731/2.webp 885w,
/static/0f7a111d3dff9960260c847b79234ff3/1d4bd/2.webp 1180w,
/static/0f7a111d3dff9960260c847b79234ff3/a078f/2.webp 1356w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/0f7a111d3dff9960260c847b79234ff3/f227d/2.png 148w,
/static/0f7a111d3dff9960260c847b79234ff3/78893/2.png 295w,
/static/0f7a111d3dff9960260c847b79234ff3/05d41/2.png 590w,
/static/0f7a111d3dff9960260c847b79234ff3/06dbc/2.png 885w,
/static/0f7a111d3dff9960260c847b79234ff3/0bd9b/2.png 1180w,
/static/0f7a111d3dff9960260c847b79234ff3/5771b/2.png 1356w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/0f7a111d3dff9960260c847b79234ff3/05d41/2.png&quot;
          alt=&quot;Parallel Head&quot;
          title=&quot;Parallel Head&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Parallel Head&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/bcfdc/3.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 44.5945945945946%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABkUlEQVQoz41SXXObMBD0//9TfegkmdoOoR4zBlxsMCC+JDDfxMJsdUrq9iEPvZmdPQlpuVvdSgiB7XYL27ZxOBxgGAYsy4Lv+zBNU++dz2c4joPdbof9fq/z9XqDrutAsSyLBsVqGAYkSYI8z1EUxSOvqgqcc5RlqfPr9Yq6rhXXaJpWfROQNwnSWe7/CNLhvu/Rtq062OicmNYkxnmhKqmV2BVfx/IAaWrBNE3heR6iKNKtEsIwhPlm4Ol5jVfjjIKXaGqJwBsQ+ZPiDnFwA0+AnEFV+1khVXE6nbRXx+Pxw0sFYtd14Nq/YLzYyHIOkbUwvlkwv9v4qWC9eHDWF7ibEHKa/7bMGNNVxXGsOQgCvUcILxfkKdP+YpmxyAH326B41Dy/94p7MlL7qAWllJjn+YFxHMGUOFlBrReighAc/xMrer2vgn5EEOpREuYrwRLzrcPURpg6hrGNNb8rTBox7rL5EKRZpMvk558xybJMj0jCLji5P1TLHFNfgMcbCGagiLaKX1Glb3pdsg3mMcFv7hal4RMNIw8AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/59245/3.webp 148w,
/static/89edc638680a0e8cefea2fe3e05b7b33/4a139/3.webp 295w,
/static/89edc638680a0e8cefea2fe3e05b7b33/3b0a6/3.webp 590w,
/static/89edc638680a0e8cefea2fe3e05b7b33/fe731/3.webp 885w,
/static/89edc638680a0e8cefea2fe3e05b7b33/1d4bd/3.webp 1180w,
/static/89edc638680a0e8cefea2fe3e05b7b33/667df/3.webp 1390w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/f227d/3.png 148w,
/static/89edc638680a0e8cefea2fe3e05b7b33/78893/3.png 295w,
/static/89edc638680a0e8cefea2fe3e05b7b33/05d41/3.png 590w,
/static/89edc638680a0e8cefea2fe3e05b7b33/06dbc/3.png 885w,
/static/89edc638680a0e8cefea2fe3e05b7b33/0bd9b/3.png 1180w,
/static/89edc638680a0e8cefea2fe3e05b7b33/bcfdc/3.png 1390w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/05d41/3.png&quot;
          alt=&quot;FCN Mask Head&quot;
          title=&quot;FCN Mask Head&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;FCN Mask Head&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/85932b3c8a94651d46f227321be5af34/6f34f/4.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABqklEQVQoz32SS2/TUBCF8wNZ8wMQEltYsQIJIbY8Be2GXSR2RBSkIiEhFthNKDROQkpC6qYhlRO7cePEj9jx4+PeK1zRqjDSyGdGc854Zm4lCAI07TOdb7uYPzrUdzRa7Ta9Xk/kNQzDoC1iXddpNpsqlrjT6dBoNBQ2TRNpRVFQKQTI/DmT8QH97nc8Z8piuSRdJ/jiq3CaKg/DkCzLiKKI9XqtYpmXuTPBNM8pgjnP3+jcfPSaaOFiHjvoxoDDsSPIIZ7nkQni/0yKSatkQpDIYzIa8UuMfNQdMjyecWQ5TE8D4mSNH0bkeSFZiniZnxvZWcSqOEhy3n0YYuwvcGdzvIMmrtklGPfJxQpK0t8CF0UrI9vnaW2Pj18GXL9jcOVanVtP2uwPTwjdKb7rUMShanhxxEv/ME5S3NM5X/sW96u73H25x8Otn7yt/7lc6aI4F+tZrVYkSfLvHZaJjfcDbrzY4Xa1xdV7NTZebSuppbiyfFpSSF7Wtm0sy8J1XWaz2ZnLZkqwVN7WWmzWPvGsusWDx5uc2JNznUssReM4Vg3k85Eu47LuN77LoEBj7jKNAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/85932b3c8a94651d46f227321be5af34/59245/4.webp 148w,
/static/85932b3c8a94651d46f227321be5af34/4a139/4.webp 295w,
/static/85932b3c8a94651d46f227321be5af34/3b0a6/4.webp 590w,
/static/85932b3c8a94651d46f227321be5af34/fe731/4.webp 885w,
/static/85932b3c8a94651d46f227321be5af34/1d4bd/4.webp 1180w,
/static/85932b3c8a94651d46f227321be5af34/94ef7/4.webp 1428w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/85932b3c8a94651d46f227321be5af34/f227d/4.png 148w,
/static/85932b3c8a94651d46f227321be5af34/78893/4.png 295w,
/static/85932b3c8a94651d46f227321be5af34/05d41/4.png 590w,
/static/85932b3c8a94651d46f227321be5af34/06dbc/4.png 885w,
/static/85932b3c8a94651d46f227321be5af34/0bd9b/4.png 1180w,
/static/85932b3c8a94651d46f227321be5af34/6f34f/4.png 1428w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/85932b3c8a94651d46f227321be5af34/05d41/4.png&quot;
          alt=&quot;Putting things together&quot;
          title=&quot;Putting things together&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Putting things together&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;Mask R-CNN is an intuitive extension from Faster R-CNN with a few unique corrections for instance segmentation task, including RoIAlign and a parallel FCN mask head. RoIAlign is proposed to combat quantization from RoIPool to protect the pixel-to-pixel alignment. The authors argued by experiment results that decoupling segmentation from classification and bounding-box regression is preferred than the coupling of multiple tasks. Thus, Mask R-CNN adds a separate mask head to the Faster R-CNN network to predict a binary mask for each class independently. &lt;/p&gt;
&lt;p&gt;However, the authors didn‚Äôt give a formal proof of &lt;b&gt;why decoupling is more desirable than making masks&lt;/b&gt; across classes compete against each other. I consider this a minor limitation of the paper. An implicit trade-off of the Mask R-CNN design is the &lt;b&gt;accuracy vs. speed balance&lt;/b&gt;, since Mask R-CNN uses the Faster R-CNN network, it also inherent the speed limitation from it as a two-stage network. Compared to a single-stage network YOLO, Mask R-CNN is slower in the object detection task, but more accurate thanks to its a localization step that preserves the spatial coherence for the segmentation. &lt;/p&gt;
&lt;p&gt;Overall, this extension is intuitive, and it generates non-trivial improvement on multiple tasks including object detection, instance segmentation, and human pose estimation with the same framework. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;Mask RCNN is an influential paper, which has been cited 9714 times (Jan 2021). In my opinion, this paper not only advanced the state-of-art networks by its accuracy but more importantly, the vision behind it - &lt;b&gt;you don‚Äôt necessarily need a heavily-engineered complex structure to achieve a fundamental improvement&lt;/b&gt;. &lt;/p&gt;
&lt;p&gt;As the authors did in this paper, observing how the achievement from a previous task (object detection and semantic segmentation in this case) can benefit a problem from an unsolved task (instance segmentation) and putting things together in a way that make sense (intuitive extension to include the mask and avoid quantization) can also produce great results. With the idea behind Mask R-CNN, we can extend even faster object detection networks to solve instance segmentation such as YOLO.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/843f9/5.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 31.756756756756754%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAiUlEQVQY06WPywoAIQhF+//fbFFQFL2JHs4oGNN6BFGveTQBr40xoJQCWmtwzlGeUoIY44noqIcQSMs5n36tFTGw9wbBQGxIKcFaS1Dv/Rn4Qhn0XYbaBZxz0naEGWNORI0dr+BruOYcQRdwrUWP8ctKKdrae7+GW2uXs4bxAnLx15gjuPjrzHkAipzUkeCkp/YAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/59245/5.webp 148w,
/static/5a84631efaec934c65cba2cb2f9d3727/4a139/5.webp 295w,
/static/5a84631efaec934c65cba2cb2f9d3727/3b0a6/5.webp 590w,
/static/5a84631efaec934c65cba2cb2f9d3727/fe731/5.webp 885w,
/static/5a84631efaec934c65cba2cb2f9d3727/1d4bd/5.webp 1180w,
/static/5a84631efaec934c65cba2cb2f9d3727/ff8d7/5.webp 1440w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/f227d/5.png 148w,
/static/5a84631efaec934c65cba2cb2f9d3727/78893/5.png 295w,
/static/5a84631efaec934c65cba2cb2f9d3727/05d41/5.png 590w,
/static/5a84631efaec934c65cba2cb2f9d3727/06dbc/5.png 885w,
/static/5a84631efaec934c65cba2cb2f9d3727/0bd9b/5.png 1180w,
/static/5a84631efaec934c65cba2cb2f9d3727/843f9/5.png 1440w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/05d41/5.png&quot;
          alt=&quot;Future Work&quot;
          title=&quot;Future Work&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Future Work&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Multi-Scale Context Aggregation by Dilated Convolutions]]></title><description><![CDATA[Main Contribution The research problem in this paper is semantic segmentation. The authors observed that the existing semantic segmentation‚Ä¶]]></description><link>https://azmarie.github.io/Dilated-convolutions/</link><guid isPermaLink="false">https://azmarie.github.io/Dilated-convolutions/</guid><pubDate>Sun, 20 Sep 2020 04:15:24 GMT</pubDate><content:encoded>&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is semantic segmentation. The authors observed that the existing semantic segmentation methods were mostly repurposed convolutional networks from image classification tasks, and they want to challenge this adaptation by proposing a model specifically designed for dense prediction. The main contribution includes, firstly, proposing a module using dilated convolutions to aggregate multi-scale contextual information while preserving resolution; secondly, challenge the necessity of the vestigial components that had been developed for image classification in semantic segmentation networks.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The proposed method is motivated by the structure of the full-resolution dense prediction problem and the nice exponentially expanding property of dilated convolution that makes it a natural potential solution. The authors constructed the context module with multiple dilated convolution layers of an increasing dilation rate to effectively learn from a large receptive field without losing resolution. This paper also challenged the existing approaches in dealing with the conflicting demands of reducing resolution in imaging classification and the need for full-resolution output in dense prediction. The authors proposed to remove the last two pooling and striding layers entirely to increase accuracy, rather than applying post hoc measures.&lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;Overall, I found this paper valuable as it proposed an alternative view on the adaptation of image classification networks on semantic segmentation tasks. The repurposed networks outperformed the status quo, however, are all parts of the network necessary for the task at hand? It might not be the case, as proven in this paper. &lt;/p&gt;
&lt;p&gt;One thing I find not abundantly clear in this paper is that it seems to assume that loss of resolution in the operation throughout the network is inherently bad, and &lt;b&gt;preserving the full resolution is inherently desirable&lt;/b&gt; for dense prediction problems. I don‚Äôt think the authors drive the point home by justifying this assumption. Besides, the authors didn‚Äôt mention &lt;b&gt;the benefits of transfer learning from pre-training&lt;/b&gt; on the image classification task, which I find very relevant in arguing the structural difference between these two tasks. &lt;/p&gt;
&lt;p&gt;From my point of view, this paper would be more convincing if the author explained the assumption, and tried to justify both points of view by presenting a formal comparison between the rectangular prisms this paper proposed to the standard pyramid-shaped architectures carried from image classification tasks.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[ImageNet: a Large-Scale Hierarchical Image Database]]></title><description><![CDATA[What‚Äôs ImageNet? Inspired by the explosion of data, ImageNet proposed to target an ambitious research problem - how to harness the power of‚Ä¶]]></description><link>https://azmarie.github.io/ImageNet/</link><guid isPermaLink="false">https://azmarie.github.io/ImageNet/</guid><pubDate>Sun, 20 Sep 2020 00:00:00 GMT</pubDate><content:encoded>&lt;h2 id=&quot;whats-imagenet&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-imagenet&quot; aria-label=&quot;whats imagenet permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What‚Äôs ImageNet?&lt;/h2&gt;
&lt;p&gt;Inspired by the explosion of data, ImageNet proposed to target an ambitious research problem - how to harness the power of vast quantities of image data and organize them in such a way that‚Äôs beneficial to a variety of research problems. The main contribution in the paper is the introduction of a large-scale, highly-diverse, and highly-accurate database built on the hierarchical structure of WordNet called &lt;code class=&quot;language-text&quot;&gt;ImageNet&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;Constructing an accurate large-scale database is no easy task. In ImageNet, the data was collected by querying several image search engines per synset and then verified by global users leveraging the services of &lt;a href=&quot;https://www.mturk.com/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Amazon Mechanical Turk&lt;/a&gt; to reach a predetermined confidence score threshold. The paper advanced the state-of-art dataset by its large scale, high accuracy, and large diversity, and also its semantic hierarchy based on WordNet. One limitation of the ImageNet could be its choice of assigning only a single label to each image, it‚Äôs not optimal when there are more than one clear objects in the image.&lt;/p&gt;
&lt;h2 id=&quot;what-do-i-think&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#what-do-i-think&quot; aria-label=&quot;what do i think permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What do I think?&lt;/h2&gt;
&lt;p&gt;What I found as inherently novel about ImageNet is its &lt;b&gt;focus and belief in data&lt;/b&gt; - a fair representation of the problem space with data is important and can be beneficial to computer vision tasks regardless of the algorithm and models. In hindsight, ImageNet has been proven to be a supreme source of training data and benchmark datasets. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;ImageNet and ImageNet Challenge inspired a stream of work in the neural networks which generated groundbreaking results, to the point where transfer learning via pre training on ImageNet is widely used as a standard procedure before fine-tuning on another dataset. One possible extension into the computer graphic tasks will be to extend the ImageNet dataset into 3D, including depth information for a 2D scene. Even though a 3D dataset is even more costly to obtain, it could benefit 3D scene understanding and robotic tasks greatly. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Face Morphing: A Step-by-Step Tutorial]]></title><description><![CDATA[üìñ For the step-by-step tutorial with intuitive reasoning, check out my article on Medium üåü Want to play with it yourself? Check out my‚Ä¶]]></description><link>https://azmarie.github.io/2020-08-08-face-morphing/</link><guid isPermaLink="false">https://azmarie.github.io/2020-08-08-face-morphing/</guid><pubDate>Sat, 08 Aug 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ For the step-by-step tutorial with intuitive reasoning, check out &lt;a href=&quot;https://azmariewang.medium.com/face-morphing-a-step-by-step-tutorial-with-code-75a663cdc666&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;my article on Medium&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üåü Want to play with it yourself? Check out my &lt;a href=&quot;https://github.com/Azmarie/Face-Morphing&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;github repo&lt;/a&gt; for implementation and demo.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Hairstyle Transfer ‚Äî Semantic Editing GAN Latent Code]]></title><description><![CDATA[üìñ For report and analysis, check out my article on Medium üåü Want to play with it yourself? Check out my github repo for implementation and‚Ä¶]]></description><link>https://azmarie.github.io/2020-03-24-Hairstyle-Transfer/</link><guid isPermaLink="false">https://azmarie.github.io/2020-03-24-Hairstyle-Transfer/</guid><pubDate>Tue, 24 Mar 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;üìñ For report and analysis, check out &lt;a href=&quot;https://medium.com/swlh/hairstyle-transfer-semantic-editing-gan-latent-code-b3a6ccf91e82&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;my article on Medium&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üåü Want to play with it yourself? Check out my &lt;a href=&quot;https://github.com/Azmarie/Hairstyle-Transfer&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;github repo&lt;/a&gt; for implementation and demo.&lt;/p&gt;</content:encoded></item></channel></rss>