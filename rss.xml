<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Azmarie Wang]]></title><description><![CDATA[Azmarie Wang's personal blog]]></description><link>https://azmarie.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Sun, 17 Jan 2021 22:53:23 GMT</lastBuildDate><item><title><![CDATA[Geometry-Aware Style Transfer: Implementation and Analysis]]></title><description><![CDATA[ðŸ“– For report and analysis, check out my article on Medium ðŸŒŸ Want to play with it yourself? Check out my github repo for implementation andâ€¦]]></description><link>https://azmarie.github.io/2020-12-27-geometry-aware-style-transfer/</link><guid isPermaLink="false">https://azmarie.github.io/2020-12-27-geometry-aware-style-transfer/</guid><pubDate>Sun, 27 Dec 2020 20:29:57 GMT</pubDate><content:encoded>&lt;!-- 
**tl;dr** This is to document my proje

--- --&gt;
&lt;p&gt;ðŸ“– For report and analysis, check out &lt;a href=&quot;https://azmariewang.medium.com/geometry-aware-style-transfer-implementation-and-analysis-3a9034dfca2d&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;my article on Medium&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ðŸŒŸ Want to play with it yourself? Check out my &lt;a href=&quot;https://github.com/Azmarie/Caricature-Your-Face&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;github repo&lt;/a&gt; for implementation and demo.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Reivew: Mask R-CNN]]></title><description><![CDATA[ðŸ“– Link to the Paper: Mask R-CNN The R-CNN Family 


 Problem Statement Input: Images with objects Output: Correct masks of all objects inâ€¦]]></description><link>https://azmarie.github.io/2020-09-27-paper-review-mask-rcnn/</link><guid isPermaLink="false">https://azmarie.github.io/2020-09-27-paper-review-mask-rcnn/</guid><pubDate>Sun, 27 Sep 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;ðŸ“– Link to the Paper: &lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Mask R-CNN&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;the-r-cnn-family&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#the-r-cnn-family&quot; aria-label=&quot;the r cnn family permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The R-CNN Family&lt;/h2&gt;
&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/19af9/b-1.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 33.78378378378378%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAAA2ElEQVQoz61RXQ/CIAzk//83E33Q6NT5rRkMN7exCWM7RxUzNSY+2OQCXI+jLaxtW/yKpmk+ud7eBcMfw5myOEkQxTFUVRFhjIGQZ/COk0lKVTlc8hzTYE68tXeuKCsswhD74xG1tfcKN/sDJrMA5zSF1hp5UWCxWiNYhlhtd8SpssSJCwyGIzi91oYeTi4ZxtMZ6Y039OU6gUPZXc6yDEVnrJQiQ5/rh+0MtL6irmvKVY8OWX+g3phzDiklYmrPvszIa52BEIIQRRGtzpy9/9q3YT/xOH+LG+x9IrukLtoeAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/59245/b-1.webp 148w,
/static/07104a4d1a8b07998ab755f1e76c17c3/4a139/b-1.webp 295w,
/static/07104a4d1a8b07998ab755f1e76c17c3/3b0a6/b-1.webp 590w,
/static/07104a4d1a8b07998ab755f1e76c17c3/fe731/b-1.webp 885w,
/static/07104a4d1a8b07998ab755f1e76c17c3/1d4bd/b-1.webp 1180w,
/static/07104a4d1a8b07998ab755f1e76c17c3/e00f2/b-1.webp 1378w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/f227d/b-1.png 148w,
/static/07104a4d1a8b07998ab755f1e76c17c3/78893/b-1.png 295w,
/static/07104a4d1a8b07998ab755f1e76c17c3/05d41/b-1.png 590w,
/static/07104a4d1a8b07998ab755f1e76c17c3/06dbc/b-1.png 885w,
/static/07104a4d1a8b07998ab755f1e76c17c3/0bd9b/b-1.png 1180w,
/static/07104a4d1a8b07998ab755f1e76c17c3/19af9/b-1.png 1378w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/07104a4d1a8b07998ab755f1e76c17c3/05d41/b-1.png&quot;
          alt=&quot;RCNN Family&quot;
          title=&quot;RCNN Family&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;RCNN Family&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/c91f3/b-2.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABfklEQVQoz4WSvUvDQBjG8x8J/gUO4qqboIOjIDjYxUEERwu6uNRRcHHTRegkFaSibbVEKoW0IVSr+f5o2jQflzzmTlNL1XrwcLkc/N7nufflBEEAz/Mol8tsr1WraLVa+GslSYJpiyOEgJAIvh/A7fdhmBY03UAQhqB3QRAwhek5iqIRNNNkES77ICSGYRjQVQWKLDNwPy2gaRp0XYdpmvA874fbSTg3/sMPCUqVJpqSwtxQDYdDBqKijuM4huM4sCzr1+dgQBJ/0q8bKlYOSsif3qS3Md5Tp3IqCuj1ehgMBgxIHVO5rsvA1D1NM3IYRjE7XN2pmF89w0VRTIFeGvczKgXatv31NATtdps1TpIkttPGiqLIinHfhgkqlVvUH3m8vgiQlQS2YyPwfeaMKos9bQK4KApwXxMxt7CP5bUcNrbWMTO7g8WlAiz7DaqqodPpsOjdbpc1jjqZbMioKQ9PzyicFJHbLeHw+Bx7+SNsbl+iVm/8O4vj45JBPwDWfaSP6lBdBAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/59245/b-2.webp 148w,
/static/0566b1aa58106dbfbcfe2da01063e141/4a139/b-2.webp 295w,
/static/0566b1aa58106dbfbcfe2da01063e141/3b0a6/b-2.webp 590w,
/static/0566b1aa58106dbfbcfe2da01063e141/fe731/b-2.webp 885w,
/static/0566b1aa58106dbfbcfe2da01063e141/1d4bd/b-2.webp 1180w,
/static/0566b1aa58106dbfbcfe2da01063e141/31901/b-2.webp 1398w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/f227d/b-2.png 148w,
/static/0566b1aa58106dbfbcfe2da01063e141/78893/b-2.png 295w,
/static/0566b1aa58106dbfbcfe2da01063e141/05d41/b-2.png 590w,
/static/0566b1aa58106dbfbcfe2da01063e141/06dbc/b-2.png 885w,
/static/0566b1aa58106dbfbcfe2da01063e141/0bd9b/b-2.png 1180w,
/static/0566b1aa58106dbfbcfe2da01063e141/c91f3/b-2.png 1398w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/0566b1aa58106dbfbcfe2da01063e141/05d41/b-2.png&quot;
          alt=&quot;RCNN&quot;
          title=&quot;RCNN&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;RCNN&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/ab1a16397e76571184562efc47123e50/782ef/b-3.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABaElEQVQoz4VSTUvDQBDN3/PqLxDEkyfP3jwJetCT9CCCHz14UChCe/ASEBIVIhFjqZVKKGnSxNi0SdMm6ea5u5iwthUfPDLZzL6ZNxlJ13WoqgrDMKBpGo3vafwKRVHgOA4Y8jznFGORxTmDxILZbIbJNMFgMEA8HiMcjTCkJCRfECwgvouxJCb1PkPIWgvPbQvTnwJjWkC8KJIQsnAmCdn8YTR9uB4TIRjRLqMoQhzH/HIh6vs+LxaGIeYhOf0+TNPkVk17gqv6B+RHB2Y/wnAYIAgCJElS2mLCnU4H3W6Xk83ZdV14nsfzpCzNkJMM770AGzsaVtZUbO49wTADTOKIWo5/dcAELcuCbdtckHWapikd0ZR/Ky2/9SIc3LSwe/mC/Vob57cGLUR4UpZlpeX/UApWGi2sH8rYOn7A6nYVRxc1NjE+Q0Zm56+1WfpTFL2JauMOZ9d1VE5O8eV7pcVlOze/fwW+AcSyq8fCttyWAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/ab1a16397e76571184562efc47123e50/59245/b-3.webp 148w,
/static/ab1a16397e76571184562efc47123e50/4a139/b-3.webp 295w,
/static/ab1a16397e76571184562efc47123e50/3b0a6/b-3.webp 590w,
/static/ab1a16397e76571184562efc47123e50/fe731/b-3.webp 885w,
/static/ab1a16397e76571184562efc47123e50/1d4bd/b-3.webp 1180w,
/static/ab1a16397e76571184562efc47123e50/ef412/b-3.webp 1402w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/ab1a16397e76571184562efc47123e50/f227d/b-3.png 148w,
/static/ab1a16397e76571184562efc47123e50/78893/b-3.png 295w,
/static/ab1a16397e76571184562efc47123e50/05d41/b-3.png 590w,
/static/ab1a16397e76571184562efc47123e50/06dbc/b-3.png 885w,
/static/ab1a16397e76571184562efc47123e50/0bd9b/b-3.png 1180w,
/static/ab1a16397e76571184562efc47123e50/782ef/b-3.png 1402w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/ab1a16397e76571184562efc47123e50/05d41/b-3.png&quot;
          alt=&quot;Fast RCNN&quot;
          title=&quot;Fast RCNN&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Fast RCNN&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/772653a28863423d165820e8de688991/b85b2/b-4.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABbklEQVQoz4VSTUvDQBDNv/NfKPgTBI96FVEv9iSFHgTvKqIgSgWDSmNAW81Xa+ohxlpt03wnm6TP7GokrYU+GHZ3Zvbtm9nhTNOEKIrMZFmGJEngeR7NZhOCIEBVVRQYj8eYBy5NU4RhCMdxYVkWfN/L9zYGw2G+OixWkBWExX7axwjL7JbjQ5ReIHdN+CFBEATMygpnqSz7uOngk/YFRR/SNKbQdV3EcQxaSQHP81jMtm0kSTKpkCQZBm4M24uR5M5rwQB/9wmjH2A0sn/b4CPLsj9CeqaE1MoPUVLuUR+gevqM84aCjaqMhcVbLK7d4+LhHSSOcjX+v9J6vR77LE3ToOs6W6Mo+lEYkRRufun1rY/KUQvrNQHbhyqqZy3mTxKCME+mZRcwDINNBCXqdDpQFIW1YaKH9dYHlndvsFJrYGnnEqtb+0hIzIjo64SQuaPDSi4SlK6B2vEVDk7q2Kzsod3WmJ/2btaIzDKKb/MsqHtpsB6TAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/772653a28863423d165820e8de688991/59245/b-4.webp 148w,
/static/772653a28863423d165820e8de688991/4a139/b-4.webp 295w,
/static/772653a28863423d165820e8de688991/3b0a6/b-4.webp 590w,
/static/772653a28863423d165820e8de688991/fe731/b-4.webp 885w,
/static/772653a28863423d165820e8de688991/1d4bd/b-4.webp 1180w,
/static/772653a28863423d165820e8de688991/ed4c1/b-4.webp 1368w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/772653a28863423d165820e8de688991/f227d/b-4.png 148w,
/static/772653a28863423d165820e8de688991/78893/b-4.png 295w,
/static/772653a28863423d165820e8de688991/05d41/b-4.png 590w,
/static/772653a28863423d165820e8de688991/06dbc/b-4.png 885w,
/static/772653a28863423d165820e8de688991/0bd9b/b-4.png 1180w,
/static/772653a28863423d165820e8de688991/b85b2/b-4.png 1368w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/772653a28863423d165820e8de688991/05d41/b-4.png&quot;
          alt=&quot;FastER RCNN&quot;
          title=&quot;FastER RCNN&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;FastER RCNN&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&quot;problem-statement&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#problem-statement&quot; aria-label=&quot;problem statement permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;: Images with objects&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;: Correct masks of all objects in the image while also precisely segmenting each instance.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Application&lt;/b&gt;: Autonomous driving, medical imaging, human pose estimation, etc.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Goal of this Mask R-CNN&lt;/b&gt;: To create a meta-algorithm to support future research of instance segmentation, that has good speed/accuracy and is intuitive/easy-to-use.&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is instance segmentation. Compared to object detection where a bounding box is determined per instance, instance segmentation predicts pixel-level segmentation for each instance, in addition to the object class. After the authors examine the prior work in object detection and semantic segmentation, they decide to take the best of both worlds and solve the instance segmentation problem by augmenting the Faster R-CNN model with a mask branch that is a small FCN. &lt;/p&gt;
&lt;p&gt;The main contribution of this paper is the proposal of Mask R-CNN, as a fast and easy-to-use network with great accuracy, for it to serve as a meta-algorithm and a solid foundation for future instance-level research. &lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/5a42c88461a6855175ed5f830da32812/cb752/1.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABz0lEQVQoz22Sz07bQBDG8xA8Qt+CA4ee+gY9V6pQEe1jwKFHJC5InLlUlVraAyciRQERJQFCnKQhUkpxiP/Gf9axvevk68xaRIAYaWV7duY3n7/dyng8RrVaRb9n4LLdRq1Wg2EY6Ha7qNfr6HQ6aLVaaDabaNO+UgrL5VIvjqfvi8UCFbwSUkoURYFiUegihnAuz/NndY+gx9DAB8vCl+0P+PzpIzY23uLn8a8V1AoFXJHqxizLMJlMMOj10CXVvueVdTQso0FSD1Wo3AwMbL1fx9HOFt6sVbC//xX9/h8Mh0Oksxk820YqC1jTKW4pd9looHl+hinBOWIxh+P6SJIEOQNt9y++HWyi8WMXezvvcHJyCNuekRoTKRUFQQDTszHs9zC4vsbdaASTYAzgmKcpPN9HHMelwjxLcHP1G6PBKRoX36n4duVJrDJEMsWdP0U8F5DULOn32F/2q7QiRxCGEFqhLA9FJDkerNITFUZIfU8rME0TNnmsSKXvuvh3fw8Ri2eHUhCYlfEQpYoSmCRkvusgDAMIIcgXgRn7R8a7BIriCAF9O+Qn7zOIFb6M1bXhE2SvbGpwHEc/LVLmkzcM5BzDefGgKIo0+LV7+B+4L53rXG0qJgAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/5a42c88461a6855175ed5f830da32812/59245/1.webp 148w,
/static/5a42c88461a6855175ed5f830da32812/4a139/1.webp 295w,
/static/5a42c88461a6855175ed5f830da32812/3b0a6/1.webp 590w,
/static/5a42c88461a6855175ed5f830da32812/fe731/1.webp 885w,
/static/5a42c88461a6855175ed5f830da32812/1d4bd/1.webp 1180w,
/static/5a42c88461a6855175ed5f830da32812/39706/1.webp 1434w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/5a42c88461a6855175ed5f830da32812/f227d/1.png 148w,
/static/5a42c88461a6855175ed5f830da32812/78893/1.png 295w,
/static/5a42c88461a6855175ed5f830da32812/05d41/1.png 590w,
/static/5a42c88461a6855175ed5f830da32812/06dbc/1.png 885w,
/static/5a42c88461a6855175ed5f830da32812/0bd9b/1.png 1180w,
/static/5a42c88461a6855175ed5f830da32812/cb752/1.png 1434w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/5a42c88461a6855175ed5f830da32812/05d41/1.png&quot;
          alt=&quot;RoI Align&quot;
          title=&quot;RoI Align&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;RoI Align&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/0f7a111d3dff9960260c847b79234ff3/5771b/2.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 46.621621621621614%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABUUlEQVQoz11Ri27CMAzs//8YCIGEBIIx6GOUVnSFtaU0pfRxy3lL6GbJin2xL2fHWa1WmE6n2Gw2WK/XWCwWILbb7TCfz7FcLgVnPJlMsN1uMZvNcDgcQBuGAcYYO8/nE/f7HbfbDapWEhdFgaqqUNe1bej7Xpx513UWZ0wOkwshGx/1Q5MoISIxsTEhTxKOjfn1ehUR5s7p+05eoapS5X+ajJqyLJGmqTwwJifuuq7glrCqfsY9J2fEyQnMS1UIEZ2K9/s9LpcLmqaxhJwsCALZ/WeSvHbIV7IsQxzHOIURPN/Du/umGxqrwvd9UaKUso1U5Xm6Vn9OrmuG18ivZcuS2w7pZ4q2bS1GNVEUya4MIb3RNYH+daXX0ZtPMQUkYCPP/zukE+dp77SIVueZHrck4S/uGAVhGIpTyfhnE91A7Hg8yvjGuHdiH7rnK89t/TcRHrSU97Rh5QAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/0f7a111d3dff9960260c847b79234ff3/59245/2.webp 148w,
/static/0f7a111d3dff9960260c847b79234ff3/4a139/2.webp 295w,
/static/0f7a111d3dff9960260c847b79234ff3/3b0a6/2.webp 590w,
/static/0f7a111d3dff9960260c847b79234ff3/fe731/2.webp 885w,
/static/0f7a111d3dff9960260c847b79234ff3/1d4bd/2.webp 1180w,
/static/0f7a111d3dff9960260c847b79234ff3/a078f/2.webp 1356w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/0f7a111d3dff9960260c847b79234ff3/f227d/2.png 148w,
/static/0f7a111d3dff9960260c847b79234ff3/78893/2.png 295w,
/static/0f7a111d3dff9960260c847b79234ff3/05d41/2.png 590w,
/static/0f7a111d3dff9960260c847b79234ff3/06dbc/2.png 885w,
/static/0f7a111d3dff9960260c847b79234ff3/0bd9b/2.png 1180w,
/static/0f7a111d3dff9960260c847b79234ff3/5771b/2.png 1356w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/0f7a111d3dff9960260c847b79234ff3/05d41/2.png&quot;
          alt=&quot;Parallel Head&quot;
          title=&quot;Parallel Head&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Parallel Head&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/bcfdc/3.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 44.5945945945946%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABkUlEQVQoz41SXXObMBD0//9TfegkmdoOoR4zBlxsMCC+JDDfxMJsdUrq9iEPvZmdPQlpuVvdSgiB7XYL27ZxOBxgGAYsy4Lv+zBNU++dz2c4joPdbof9fq/z9XqDrutAsSyLBsVqGAYkSYI8z1EUxSOvqgqcc5RlqfPr9Yq6rhXXaJpWfROQNwnSWe7/CNLhvu/Rtq062OicmNYkxnmhKqmV2BVfx/IAaWrBNE3heR6iKNKtEsIwhPlm4Ol5jVfjjIKXaGqJwBsQ+ZPiDnFwA0+AnEFV+1khVXE6nbRXx+Pxw0sFYtd14Nq/YLzYyHIOkbUwvlkwv9v4qWC9eHDWF7ibEHKa/7bMGNNVxXGsOQgCvUcILxfkKdP+YpmxyAH326B41Dy/94p7MlL7qAWllJjn+YFxHMGUOFlBrReighAc/xMrer2vgn5EEOpREuYrwRLzrcPURpg6hrGNNb8rTBox7rL5EKRZpMvk558xybJMj0jCLji5P1TLHFNfgMcbCGagiLaKX1Glb3pdsg3mMcFv7hal4RMNIw8AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/59245/3.webp 148w,
/static/89edc638680a0e8cefea2fe3e05b7b33/4a139/3.webp 295w,
/static/89edc638680a0e8cefea2fe3e05b7b33/3b0a6/3.webp 590w,
/static/89edc638680a0e8cefea2fe3e05b7b33/fe731/3.webp 885w,
/static/89edc638680a0e8cefea2fe3e05b7b33/1d4bd/3.webp 1180w,
/static/89edc638680a0e8cefea2fe3e05b7b33/667df/3.webp 1390w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/f227d/3.png 148w,
/static/89edc638680a0e8cefea2fe3e05b7b33/78893/3.png 295w,
/static/89edc638680a0e8cefea2fe3e05b7b33/05d41/3.png 590w,
/static/89edc638680a0e8cefea2fe3e05b7b33/06dbc/3.png 885w,
/static/89edc638680a0e8cefea2fe3e05b7b33/0bd9b/3.png 1180w,
/static/89edc638680a0e8cefea2fe3e05b7b33/bcfdc/3.png 1390w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/89edc638680a0e8cefea2fe3e05b7b33/05d41/3.png&quot;
          alt=&quot;FCN Mask Head&quot;
          title=&quot;FCN Mask Head&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;FCN Mask Head&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/85932b3c8a94651d46f227321be5af34/6f34f/4.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 45.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABqklEQVQoz32SS2/TUBCF8wNZ8wMQEltYsQIJIbY8Be2GXSR2RBSkIiEhFthNKDROQkpC6qYhlRO7cePEj9jx4+PeK1zRqjDSyGdGc854Zm4lCAI07TOdb7uYPzrUdzRa7Ta9Xk/kNQzDoC1iXddpNpsqlrjT6dBoNBQ2TRNpRVFQKQTI/DmT8QH97nc8Z8piuSRdJ/jiq3CaKg/DkCzLiKKI9XqtYpmXuTPBNM8pgjnP3+jcfPSaaOFiHjvoxoDDsSPIIZ7nkQni/0yKSatkQpDIYzIa8UuMfNQdMjyecWQ5TE8D4mSNH0bkeSFZiniZnxvZWcSqOEhy3n0YYuwvcGdzvIMmrtklGPfJxQpK0t8CF0UrI9vnaW2Pj18GXL9jcOVanVtP2uwPTwjdKb7rUMShanhxxEv/ME5S3NM5X/sW96u73H25x8Otn7yt/7lc6aI4F+tZrVYkSfLvHZaJjfcDbrzY4Xa1xdV7NTZebSuppbiyfFpSSF7Wtm0sy8J1XWaz2ZnLZkqwVN7WWmzWPvGsusWDx5uc2JNznUssReM4Vg3k85Eu47LuN77LoEBj7jKNAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/85932b3c8a94651d46f227321be5af34/59245/4.webp 148w,
/static/85932b3c8a94651d46f227321be5af34/4a139/4.webp 295w,
/static/85932b3c8a94651d46f227321be5af34/3b0a6/4.webp 590w,
/static/85932b3c8a94651d46f227321be5af34/fe731/4.webp 885w,
/static/85932b3c8a94651d46f227321be5af34/1d4bd/4.webp 1180w,
/static/85932b3c8a94651d46f227321be5af34/94ef7/4.webp 1428w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/85932b3c8a94651d46f227321be5af34/f227d/4.png 148w,
/static/85932b3c8a94651d46f227321be5af34/78893/4.png 295w,
/static/85932b3c8a94651d46f227321be5af34/05d41/4.png 590w,
/static/85932b3c8a94651d46f227321be5af34/06dbc/4.png 885w,
/static/85932b3c8a94651d46f227321be5af34/0bd9b/4.png 1180w,
/static/85932b3c8a94651d46f227321be5af34/6f34f/4.png 1428w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/85932b3c8a94651d46f227321be5af34/05d41/4.png&quot;
          alt=&quot;Putting things together&quot;
          title=&quot;Putting things together&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Putting things together&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&quot;whats-good-and-whats-not-so-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good-and-whats-not-so-good&quot; aria-label=&quot;whats good and whats not so good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Whatâ€™s good and whatâ€™s not so good?&lt;/h2&gt;
&lt;p&gt;Mask R-CNN is an intuitive extension from Faster R-CNN with a few unique corrections for instance segmentation task, including RoIAlign and a parallel FCN mask head. RoIAlign is proposed to combat quantization from RoIPool to protect the pixel-to-pixel alignment. The authors argued by experiment results that decoupling segmentation from classification and bounding-box regression is preferred than the coupling of multiple tasks. Thus, Mask R-CNN adds a separate mask head to the Faster R-CNN network to predict a binary mask for each class independently. &lt;/p&gt;
&lt;p&gt;However, the authors didnâ€™t give a formal proof of &lt;b&gt;why decoupling is more desirable than making masks&lt;/b&gt; across classes compete against each other. I consider this a minor limitation of the paper. An implicit trade-off of the Mask R-CNN design is the &lt;b&gt;accuracy vs. speed balance&lt;/b&gt;, since Mask R-CNN uses the Faster R-CNN network, it also inherent the speed limitation from it as a two-stage network. Compared to a single-stage network YOLO, Mask R-CNN is slower in the object detection task, but more accurate thanks to its a localization step that preserves the spatial coherence for the segmentation. &lt;/p&gt;
&lt;p&gt;Overall, this extension is intuitive, and it generates non-trivial improvement on multiple tasks including object detection, instance segmentation, and human pose estimation with the same framework. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;Mask RCNN is an influential paper, which has been cited 9714 times (Jan 2021). In my opinion, this paper not only advanced the state-of-art networks by its accuracy but more importantly, the vision behind it - &lt;b&gt;you donâ€™t necessarily need a heavily-engineered complex structure to achieve a fundamental improvement&lt;/b&gt;. &lt;/p&gt;
&lt;p&gt;As the authors did in this paper, observing how the achievement from a previous task (object detection and semantic segmentation in this case) can benefit a problem from an unsolved task (instance segmentation) and putting things together in a way that make sense (intuitive extension to include the mask and avoid quantization) can also produce great results. With the idea behind Mask R-CNN, we can extend even faster object detection networks to solve instance segmentation such as YOLO.&lt;/p&gt;
&lt;p&gt;&lt;figure class=&quot;gatsby-resp-image-figure&quot; style=&quot;&quot;&gt;
    &lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/843f9/5.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 31.756756756756754%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAiUlEQVQY06WPywoAIQhF+//fbFFQFL2JHs4oGNN6BFGveTQBr40xoJQCWmtwzlGeUoIY44noqIcQSMs5n36tFTGw9wbBQGxIKcFaS1Dv/Rn4Qhn0XYbaBZxz0naEGWNORI0dr+BruOYcQRdwrUWP8ctKKdrae7+GW2uXs4bxAnLx15gjuPjrzHkAipzUkeCkp/YAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
        &lt;source
          srcset=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/59245/5.webp 148w,
/static/5a84631efaec934c65cba2cb2f9d3727/4a139/5.webp 295w,
/static/5a84631efaec934c65cba2cb2f9d3727/3b0a6/5.webp 590w,
/static/5a84631efaec934c65cba2cb2f9d3727/fe731/5.webp 885w,
/static/5a84631efaec934c65cba2cb2f9d3727/1d4bd/5.webp 1180w,
/static/5a84631efaec934c65cba2cb2f9d3727/ff8d7/5.webp 1440w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/webp&quot;
        /&gt;
        &lt;source
          srcset=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/f227d/5.png 148w,
/static/5a84631efaec934c65cba2cb2f9d3727/78893/5.png 295w,
/static/5a84631efaec934c65cba2cb2f9d3727/05d41/5.png 590w,
/static/5a84631efaec934c65cba2cb2f9d3727/06dbc/5.png 885w,
/static/5a84631efaec934c65cba2cb2f9d3727/0bd9b/5.png 1180w,
/static/5a84631efaec934c65cba2cb2f9d3727/843f9/5.png 1440w&quot;
          sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
          type=&quot;image/png&quot;
        /&gt;
        &lt;img
          class=&quot;gatsby-resp-image-image&quot;
          src=&quot;/static/5a84631efaec934c65cba2cb2f9d3727/05d41/5.png&quot;
          alt=&quot;Future Work&quot;
          title=&quot;Future Work&quot;
          loading=&quot;lazy&quot;
          style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        /&gt;
      &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;figcaption class=&quot;gatsby-resp-image-figcaption&quot;&gt;Future Work&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Reivew: ImageNet: a Large-Scale Hierarchical Image Database]]></title><description><![CDATA[ðŸ“– Link to the Paper: ImageNet: a Large-Scale Hierarchical Image Database ðŸ’¡ Link to Project Webstie: ImageNet Whatâ€™s ImageNet? Inspired byâ€¦]]></description><link>https://azmarie.github.io/2020-09-20-paper-review-imageNet/</link><guid isPermaLink="false">https://azmarie.github.io/2020-09-20-paper-review-imageNet/</guid><pubDate>Sun, 20 Sep 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;ðŸ“– Link to the Paper: &lt;a href=&quot;https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;ImageNet: a Large-Scale Hierarchical Image Database&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ðŸ’¡ Link to Project Webstie: &lt;a href=&quot;http://www.image-net.org/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;ImageNet&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;whats-imagenet&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-imagenet&quot; aria-label=&quot;whats imagenet permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Whatâ€™s ImageNet?&lt;/h2&gt;
&lt;p&gt;Inspired by the explosion of data, ImageNet proposed to target an ambitious research problem - how to harness the power of vast quantities of image data and organize them in such a way thatâ€™s beneficial to a variety of research problems. The main contribution in the paper is the introduction of a large-scale, highly-diverse, and highly-accurate database built on the hierarchical structure of WordNet called &lt;code class=&quot;language-text&quot;&gt;ImageNet&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;Constructing an accurate large-scale database is no easy task. In ImageNet, the data was collected by querying several image search engines per synset and then verified by global users leveraging the services of &lt;a href=&quot;https://www.mturk.com/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Amazon Mechanical Turk&lt;/a&gt; to reach a predetermined confidence score threshold. The paper advanced the state-of-art dataset by its large scale, high accuracy, and large diversity, and also its semantic hierarchy based on WordNet. One limitation of the ImageNet could be its choice of assigning only a single label to each image, itâ€™s not optimal when there are more than one clear objects in the image.&lt;/p&gt;
&lt;h2 id=&quot;whats-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good&quot; aria-label=&quot;whats good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Whatâ€™s good?&lt;/h2&gt;
&lt;p&gt;What I found as inherently novel about ImageNet is its &lt;b&gt;focus and belief in data&lt;/b&gt; - a fair representation of the problem space with data is important and can be beneficial to computer vision tasks regardless of the algorithm and models. In hindsight, ImageNet has been proven to be a supreme source of training data and benchmark datasets. &lt;/p&gt;
&lt;h2 id=&quot;future-work&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#future-work&quot; aria-label=&quot;future work permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future Work&lt;/h2&gt;
&lt;p&gt;ImageNet and ImageNet Challenge inspired a stream of work in the neural networks which generated groundbreaking results, to the point where transfer learning via pre training on ImageNet is widely used as a standard procedure before fine-tuning on another dataset. One possible extension into the computer graphic tasks will be to extend the ImageNet dataset into 3D, including depth information for a 2D scene. Even though a 3D dataset is even more costly to obtain, it could benefit 3D scene understanding and robotic tasks greatly. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Paper Reivew: Multi-Scale Context Aggregation by Dilated Convolutions]]></title><description><![CDATA[ðŸ“– Link to the Paper: Multi-Scale Context Aggregation by Dilated Convolutions Main Contribution The research problem in this paper isâ€¦]]></description><link>https://azmarie.github.io/2020-09-20-paper-review-dilated-convolutions/</link><guid isPermaLink="false">https://azmarie.github.io/2020-09-20-paper-review-dilated-convolutions/</guid><pubDate>Sun, 20 Sep 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;ðŸ“– Link to the Paper: &lt;a href=&quot;http://vladlen.info/papers/dilated-convolutions.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Multi-Scale Context Aggregation by Dilated Convolutions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;main-contribution&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#main-contribution&quot; aria-label=&quot;main contribution permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contribution&lt;/h2&gt;
&lt;p&gt;The research problem in this paper is semantic segmentation. The authors observed that the existing semantic segmentation methods were mostly repurposed convolutional networks from image classification tasks, and they want to challenge this adaptation by proposing a model specifically designed for dense prediction. The main contribution includes, firstly, proposing a module using dilated convolutions to aggregate multi-scale contextual information while preserving resolution; secondly, challenge the necessity of the vestigial components that had been developed for image classification in semantic segmentation networks.&lt;/p&gt;
&lt;h2 id=&quot;method&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#method&quot; aria-label=&quot;method permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Method&lt;/h2&gt;
&lt;p&gt;The proposed method is motivated by the structure of the full-resolution dense prediction problem and the nice exponentially expanding property of dilated convolution that makes it a natural potential solution. The authors constructed the context module with multiple dilated convolution layers of an increasing dilation rate to effectively learn from a large receptive field without losing resolution. This paper also challenged the existing approaches in dealing with the conflicting demands of reducing resolution in imaging classification and the need for full-resolution output in dense prediction. The authors proposed to remove the last two pooling and striding layers entirely to increase accuracy, rather than applying post hoc measures.&lt;/p&gt;
&lt;h2 id=&quot;whats-good-and-whats-not-so-good&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#whats-good-and-whats-not-so-good&quot; aria-label=&quot;whats good and whats not so good permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Whatâ€™s good and whatâ€™s not so good?&lt;/h2&gt;
&lt;p&gt;Overall, I found this paper valuable as it proposed an alternative view on the adaptation of image classification networks on semantic segmentation tasks. The repurposed networks outperformed the status quo, however, are all parts of the network necessary for the task at hand? It might not be the case, as proven in this paper. &lt;/p&gt;
&lt;p&gt;One thing I find not abundantly clear in this paper is that it seems to assume that loss of resolution in the operation throughout the network is inherently bad, and &lt;b&gt;preserving the full resolution is inherently desirable&lt;/b&gt; for dense prediction problems. I donâ€™t think the authors drive the point home by justifying this assumption. Besides, the authors didnâ€™t mention &lt;b&gt;the benefits of transfer learning from pre-training&lt;/b&gt; on the image classification task, which I find very relevant in arguing the structural difference between these two tasks. &lt;/p&gt;
&lt;p&gt;From my point of view, this paper would be more convincing if the author explained the assumption, and tried to justify both points of view by presenting a formal comparison between the rectangular prisms this paper proposed to the standard pyramid-shaped architectures carried from image classification tasks.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Face Morphing: A Step-by-Step Tutorial]]></title><description><![CDATA[ðŸ“– For the step-by-step tutorial with intuitive reasoning, check out my article on Medium ðŸŒŸ Want to play with it yourself? Check out myâ€¦]]></description><link>https://azmarie.github.io/2020-08-08-face-morphing/</link><guid isPermaLink="false">https://azmarie.github.io/2020-08-08-face-morphing/</guid><pubDate>Sat, 08 Aug 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;ðŸ“– For the step-by-step tutorial with intuitive reasoning, check out &lt;a href=&quot;https://azmariewang.medium.com/face-morphing-a-step-by-step-tutorial-with-code-75a663cdc666&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;my article on Medium&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ðŸŒŸ Want to play with it yourself? Check out my &lt;a href=&quot;https://github.com/Azmarie/Face-Morphing&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;github repo&lt;/a&gt; for implementation and demo.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Hairstyle Transfer â€” Semantic Editing GAN Latent Code]]></title><description><![CDATA[ðŸ“– For report and analysis, check out my article on Medium ðŸŒŸ Want to play with it yourself? Check out my github repo for implementation andâ€¦]]></description><link>https://azmarie.github.io/2020-03-24-Hairstyle-Transfer/</link><guid isPermaLink="false">https://azmarie.github.io/2020-03-24-Hairstyle-Transfer/</guid><pubDate>Tue, 24 Mar 2020 04:15:24 GMT</pubDate><content:encoded>&lt;p&gt;ðŸ“– For report and analysis, check out &lt;a href=&quot;https://medium.com/swlh/hairstyle-transfer-semantic-editing-gan-latent-code-b3a6ccf91e82&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;my article on Medium&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ðŸŒŸ Want to play with it yourself? Check out my &lt;a href=&quot;https://github.com/Azmarie/Hairstyle-Transfer&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;github repo&lt;/a&gt; for implementation and demo.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Just Use Any: js.la Edition]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2020/js-la/</link><guid isPermaLink="false">https://azmarie.github.io/2020/js-la/</guid><pubDate>Thu, 30 Jan 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Ambitious UIs for Pitch to Play Workflows]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2019/netflix/</link><guid isPermaLink="false">https://azmarie.github.io/2019/netflix/</guid><pubDate>Wed, 13 Nov 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Just Use Any: How to Win Colleagues & Influence Your Boss]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2019/tsconf/</link><guid isPermaLink="false">https://azmarie.github.io/2019/tsconf/</guid><pubDate>Fri, 11 Oct 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Learning to Love Type Systems]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2018/dotjs/</link><guid isPermaLink="false">https://azmarie.github.io/2018/dotjs/</guid><pubDate>Wed, 28 Nov 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Learning to Love Type Systems]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2018/reactathon/</link><guid isPermaLink="false">https://azmarie.github.io/2018/reactathon/</guid><pubDate>Sat, 08 Sep 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Learning to Love Type Systems]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2018/react-rally/</link><guid isPermaLink="false">https://azmarie.github.io/2018/react-rally/</guid><pubDate>Fri, 17 Aug 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Building the World's Largest Studio at Netflix]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2018/rubyconf/</link><guid isPermaLink="false">https://azmarie.github.io/2018/rubyconf/</guid><pubDate>Thu, 08 Mar 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Monoliths to Services with Elixir & Phoenix]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2017/code-europe/</link><guid isPermaLink="false">https://azmarie.github.io/2017/code-europe/</guid><pubDate>Thu, 25 May 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Confessions of an Ember Addon Author]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2017/emberconf/</link><guid isPermaLink="false">https://azmarie.github.io/2017/emberconf/</guid><pubDate>Wed, 29 Mar 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[From Front End to Full Stack]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2017/erlang-and-elixir-factory/</link><guid isPermaLink="false">https://azmarie.github.io/2017/erlang-and-elixir-factory/</guid><pubDate>Thu, 23 Mar 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[From Front End to Full Stack]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2016/elixirconf/</link><guid isPermaLink="false">https://azmarie.github.io/2016/elixirconf/</guid><pubDate>Fri, 02 Sep 2016 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[I Can Write My App with no Handlebars]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2016/embercamp/</link><guid isPermaLink="false">https://azmarie.github.io/2016/embercamp/</guid><pubDate>Tue, 12 Jul 2016 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Idiomatic Ember: Finding the Sweet Spot of Performance & Productivity]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2016/emberconf/</link><guid isPermaLink="false">https://azmarie.github.io/2016/emberconf/</guid><pubDate>Wed, 30 Mar 2016 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Declarative Templating in Ember.js]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2016/oredev-1/</link><guid isPermaLink="false">https://azmarie.github.io/2016/oredev-1/</guid><pubDate>Fri, 04 Mar 2016 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Idiomatic Ember: Finding the Sweet Spot of Performance & Productivity]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2016/oredev-2/</link><guid isPermaLink="false">https://azmarie.github.io/2016/oredev-2/</guid><pubDate>Thu, 03 Mar 2016 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Ambitious UX for Ambitious Apps]]></title><description><![CDATA[Notes coming soon.]]></description><link>https://azmarie.github.io/2015/emberconf/</link><guid isPermaLink="false">https://azmarie.github.io/2015/emberconf/</guid><pubDate>Tue, 03 Mar 2015 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Notes coming soon.&lt;/p&gt;</content:encoded></item></channel></rss>